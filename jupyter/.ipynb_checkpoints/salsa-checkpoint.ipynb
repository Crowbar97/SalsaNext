{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crowbar/.conda/envs/salsanext/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/crowbar/.conda/envs/salsanext/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/crowbar/.conda/envs/salsanext/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/crowbar/.conda/envs/salsanext/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/crowbar/.conda/envs/salsanext/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/crowbar/.conda/envs/salsanext/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/crowbar/.conda/envs/salsanext/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/crowbar/.conda/envs/salsanext/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/crowbar/.conda/envs/salsanext/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/crowbar/.conda/envs/salsanext/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/crowbar/.conda/envs/salsanext/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/crowbar/.conda/envs/salsanext/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "import time\n",
    "import imp\n",
    "from io import BytesIO\n",
    "import collections\n",
    "import queue\n",
    "import threading\n",
    "import functools\n",
    "\n",
    "try:\n",
    "    from itertools import ifilterfalse\n",
    "except ImportError:\n",
    "    from itertools import filterfalse as ifilterfalse\n",
    "\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.batchnorm import _BatchNorm\n",
    "from torch.nn.parallel._functions import ReduceAddCoalesced, Broadcast\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.optim.lr_scheduler as toptim\n",
    "from torch.nn.parallel.data_parallel import DataParallel\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### avgmeter.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logger.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "\n",
    "    def __init__(self, log_dir,model):\n",
    "        \"\"\"Create a summary writer logging to log_dir.\"\"\"\n",
    "        #self.writer = SummaryWriter(log_dir)\n",
    "        #self.writer.add_graph(model,torch.zeros(1,5,64,2048))\n",
    "        self.writer = tf.summary.FileWriter(log_dir)\n",
    "\n",
    "    def scalar_summary(self, tag, value, step):\n",
    "        \"\"\"Log a scalar variable.\"\"\"\n",
    "        summary = tf.Summary(\n",
    "            value=[tf.Summary.Value(tag=tag, simple_value=value)])\n",
    "        self.writer.add_summary(summary, step)\n",
    "        self.writer.flush()\n",
    "\n",
    "    def image_summary(self, tag, images, step):\n",
    "        \"\"\"Log a list of images.\"\"\"\n",
    "\n",
    "        img_summaries = []\n",
    "        for i, img in enumerate(images):\n",
    "            # Write the image to a string\n",
    "            try:\n",
    "                s = StringIO()\n",
    "            except:\n",
    "                s = BytesIO()\n",
    "            scipy.misc.toimage(img).save(s, format=\"png\")\n",
    "\n",
    "            # Create an Image object\n",
    "            img_sum = tf.Summary.Image(encoded_image_string=s.getvalue(),\n",
    "                                       height=img.shape[0],\n",
    "                                       width=img.shape[1])\n",
    "            # Create a Summary value\n",
    "            img_summaries.append(tf.Summary.Value(\n",
    "                tag='%s/%d' % (tag, i), image=img_sum))\n",
    "\n",
    "        # Create and write Summary\n",
    "        summary = tf.Summary(value=img_summaries)\n",
    "        self.writer.add_summary(summary, step)\n",
    "        self.writer.flush()\n",
    "\n",
    "    def histo_summary(self, tag, values, step, bins=1000):\n",
    "        \"\"\"Log a histogram of the tensor of values.\"\"\"\n",
    "\n",
    "        # Create a histogram using numpy\n",
    "        counts, bin_edges = np.histogram(values, bins=bins)\n",
    "\n",
    "        # Fill the fields of the histogram proto\n",
    "        hist = tf.HistogramProto()\n",
    "        hist.min = float(np.min(values))\n",
    "        hist.max = float(np.max(values))\n",
    "        hist.num = int(np.prod(values.shape))\n",
    "        hist.sum = float(np.sum(values))\n",
    "        hist.sum_squares = float(np.sum(values ** 2))\n",
    "\n",
    "        # Drop the start of the first bin\n",
    "        bin_edges = bin_edges[1:]\n",
    "\n",
    "        # Add bin edges and counts\n",
    "        for edge in bin_edges:\n",
    "            hist.bucket_limit.append(edge)\n",
    "        for c in counts:\n",
    "            hist.bucket.append(c)\n",
    "\n",
    "        # Create and write Summary\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, histo=hist)])\n",
    "        self.writer.add_summary(summary, step)\n",
    "        self.writer.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### comm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = ['FutureResult', 'SlavePipe', 'SyncMaster']\n",
    "\n",
    "\n",
    "class FutureResult(object):\n",
    "    \"\"\"A thread-safe future implementation. Used only as one-to-one pipe.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._result = None\n",
    "        self._lock = threading.Lock()\n",
    "        self._cond = threading.Condition(self._lock)\n",
    "\n",
    "    def put(self, result):\n",
    "        with self._lock:\n",
    "            assert self._result is None, 'Previous result has\\'t been fetched.'\n",
    "            self._result = result\n",
    "            self._cond.notify()\n",
    "\n",
    "    def get(self):\n",
    "        with self._lock:\n",
    "            if self._result is None:\n",
    "                self._cond.wait()\n",
    "\n",
    "            res = self._result\n",
    "            self._result = None\n",
    "            return res\n",
    "\n",
    "\n",
    "_MasterRegistry = collections.namedtuple('MasterRegistry', ['result'])\n",
    "_SlavePipeBase = collections.namedtuple(\n",
    "    '_SlavePipeBase', ['identifier', 'queue', 'result'])\n",
    "\n",
    "\n",
    "class SlavePipe(_SlavePipeBase):\n",
    "    \"\"\"Pipe for master-slave communication.\"\"\"\n",
    "\n",
    "    def run_slave(self, msg):\n",
    "        self.queue.put((self.identifier, msg))\n",
    "        ret = self.result.get()\n",
    "        self.queue.put(True)\n",
    "        return ret\n",
    "\n",
    "\n",
    "class SyncMaster(object):\n",
    "    \"\"\"An abstract `SyncMaster` object.\n",
    "\n",
    "    - During the replication, as the data parallel will trigger an callback of each module, all slave devices should\n",
    "    call `register(id)` and obtain an `SlavePipe` to communicate with the master.\n",
    "    - During the forward pass, master device invokes `run_master`, all messages from slave devices will be collected,\n",
    "    and passed to a registered callback.\n",
    "    - After receiving the messages, the master device should gather the information and determine to message passed\n",
    "    back to each slave devices.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, master_callback):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            master_callback: a callback to be invoked after having collected messages from slave devices.\n",
    "        \"\"\"\n",
    "        self._master_callback = master_callback\n",
    "        self._queue = queue.Queue()\n",
    "        self._registry = collections.OrderedDict()\n",
    "        self._activated = False\n",
    "\n",
    "    def __getstate__(self):\n",
    "        return {'master_callback': self._master_callback}\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        self.__init__(state['master_callback'])\n",
    "\n",
    "    def register_slave(self, identifier):\n",
    "        \"\"\"\n",
    "        Register an slave device.\n",
    "\n",
    "        Args:\n",
    "            identifier: an identifier, usually is the device id.\n",
    "\n",
    "        Returns: a `SlavePipe` object which can be used to communicate with the master device.\n",
    "\n",
    "        \"\"\"\n",
    "        if self._activated:\n",
    "            assert self._queue.empty(), 'Queue is not clean before next initialization.'\n",
    "            self._activated = False\n",
    "            self._registry.clear()\n",
    "        future = FutureResult()\n",
    "        self._registry[identifier] = _MasterRegistry(future)\n",
    "        return SlavePipe(identifier, self._queue, future)\n",
    "\n",
    "    def run_master(self, master_msg):\n",
    "        \"\"\"\n",
    "        Main entry for the master device in each forward pass.\n",
    "        The messages were first collected from each devices (including the master device), and then\n",
    "        an callback will be invoked to compute the message to be sent back to each devices\n",
    "        (including the master device).\n",
    "\n",
    "        Args:\n",
    "            master_msg: the message that the master want to send to itself. This will be placed as the first\n",
    "            message when calling `master_callback`. For detailed usage, see `_SynchronizedBatchNorm` for an example.\n",
    "\n",
    "        Returns: the message to be sent back to the master device.\n",
    "\n",
    "        \"\"\"\n",
    "        self._activated = True\n",
    "\n",
    "        intermediates = [(0, master_msg)]\n",
    "        for i in range(self.nr_slaves):\n",
    "            intermediates.append(self._queue.get())\n",
    "\n",
    "        results = self._master_callback(intermediates)\n",
    "        assert results[0][0] == 0, 'The first result should belongs to the master.'\n",
    "\n",
    "        for i, res in results:\n",
    "            if i == 0:\n",
    "                continue\n",
    "            self._registry[i].result.put(res)\n",
    "\n",
    "        for i in range(self.nr_slaves):\n",
    "            assert self._queue.get() is True\n",
    "\n",
    "        return results[0][1]\n",
    "\n",
    "    @property\n",
    "    def nr_slaves(self):\n",
    "        return len(self._registry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### replicate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = [\n",
    "    'CallbackContext',\n",
    "    'execute_replication_callbacks',\n",
    "    'DataParallelWithCallback',\n",
    "    'patch_replication_callback'\n",
    "]\n",
    "\n",
    "\n",
    "class CallbackContext(object):\n",
    "    pass\n",
    "\n",
    "\n",
    "def execute_replication_callbacks(modules):\n",
    "    \"\"\"\n",
    "    Execute an replication callback `__data_parallel_replicate__` on each module created by original replication.\n",
    "\n",
    "    The callback will be invoked with arguments `__data_parallel_replicate__(ctx, copy_id)`\n",
    "\n",
    "    Note that, as all modules are isomorphism, we assign each sub-module with a context\n",
    "    (shared among multiple copies of this module on different devices).\n",
    "    Through this context, different copies can share some information.\n",
    "\n",
    "    We guarantee that the callback on the master copy (the first copy) will be called ahead of calling the callback\n",
    "    of any slave copies.\n",
    "    \"\"\"\n",
    "    master_copy = modules[0]\n",
    "    nr_modules = len(list(master_copy.modules()))\n",
    "    ctxs = [CallbackContext() for _ in range(nr_modules)]\n",
    "\n",
    "    for i, module in enumerate(modules):\n",
    "        for j, m in enumerate(module.modules()):\n",
    "            if hasattr(m, '__data_parallel_replicate__'):\n",
    "                m.__data_parallel_replicate__(ctxs[j], i)\n",
    "\n",
    "\n",
    "class DataParallelWithCallback(DataParallel):\n",
    "    \"\"\"\n",
    "    Data Parallel with a replication callback.\n",
    "\n",
    "    An replication callback `__data_parallel_replicate__` of each module will be invoked after being created by\n",
    "    original `replicate` function.\n",
    "    The callback will be invoked with arguments `__data_parallel_replicate__(ctx, copy_id)`\n",
    "\n",
    "    Examples:\n",
    "        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n",
    "        > sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n",
    "        # sync_bn.__data_parallel_replicate__ will be invoked.\n",
    "    \"\"\"\n",
    "\n",
    "    def replicate(self, module, device_ids):\n",
    "        modules = super(DataParallelWithCallback,\n",
    "                        self).replicate(module, device_ids)\n",
    "        execute_replication_callbacks(modules)\n",
    "        return modules\n",
    "\n",
    "\n",
    "def patch_replication_callback(data_parallel):\n",
    "    \"\"\"\n",
    "    Monkey-patch an existing `DataParallel` object. Add the replication callback.\n",
    "    Useful when you have customized `DataParallel` implementation.\n",
    "\n",
    "    Examples:\n",
    "        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n",
    "        > sync_bn = DataParallel(sync_bn, device_ids=[0, 1])\n",
    "        > patch_replication_callback(sync_bn)\n",
    "        # this is equivalent to\n",
    "        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n",
    "        > sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n",
    "    \"\"\"\n",
    "\n",
    "    assert isinstance(data_parallel, DataParallel)\n",
    "\n",
    "    old_replicate = data_parallel.replicate\n",
    "\n",
    "    @functools.wraps(old_replicate)\n",
    "    def new_replicate(module, device_ids):\n",
    "        modules = old_replicate(module, device_ids)\n",
    "        execute_replication_callbacks(modules)\n",
    "        return modules\n",
    "\n",
    "    data_parallel.replicate = new_replicate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batchnorm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = ['SynchronizedBatchNorm1d', 'SynchronizedBatchNorm2d',\n",
    "           'SynchronizedBatchNorm3d', 'convert_model']\n",
    "\n",
    "\n",
    "def _sum_ft(tensor):\n",
    "    \"\"\"sum over the first and last dimention\"\"\"\n",
    "    return tensor.sum(dim=0).sum(dim=-1)\n",
    "\n",
    "\n",
    "def _unsqueeze_ft(tensor):\n",
    "    \"\"\"add new dementions at the front and the tail\"\"\"\n",
    "    return tensor.unsqueeze(0).unsqueeze(-1)\n",
    "\n",
    "\n",
    "_ChildMessage = collections.namedtuple(\n",
    "    '_ChildMessage', ['sum', 'ssum', 'sum_size'])\n",
    "_MasterMessage = collections.namedtuple('_MasterMessage', ['sum', 'inv_std'])\n",
    "\n",
    "\n",
    "class _SynchronizedBatchNorm(_BatchNorm):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True):\n",
    "        super(_SynchronizedBatchNorm, self).__init__(\n",
    "            num_features, eps=eps, momentum=momentum, affine=affine)\n",
    "\n",
    "        self._sync_master = SyncMaster(self._data_parallel_master)\n",
    "\n",
    "        self._is_parallel = False\n",
    "        self._parallel_id = None\n",
    "        self._slave_pipe = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        # If it is not parallel computation or is in evaluation mode, use PyTorch's implementation.\n",
    "        if not (self._is_parallel and self.training):\n",
    "            return F.batch_norm(\n",
    "                input, self.running_mean, self.running_var, self.weight, self.bias,\n",
    "                self.training, self.momentum, self.eps)\n",
    "\n",
    "        # Resize the input to (B, C, -1).\n",
    "        input_shape = input.size()\n",
    "        input = input.view(input.size(0), self.num_features, -1)\n",
    "\n",
    "        # Compute the sum and square-sum.\n",
    "        sum_size = input.size(0) * input.size(2)\n",
    "        input_sum = _sum_ft(input)\n",
    "        input_ssum = _sum_ft(input ** 2)\n",
    "\n",
    "        # Reduce-and-broadcast the statistics.\n",
    "        if self._parallel_id == 0:\n",
    "            mean, inv_std = self._sync_master.run_master(\n",
    "                _ChildMessage(input_sum, input_ssum, sum_size))\n",
    "        else:\n",
    "            mean, inv_std = self._slave_pipe.run_slave(\n",
    "                _ChildMessage(input_sum, input_ssum, sum_size))\n",
    "\n",
    "        # Compute the output.\n",
    "        if self.affine:\n",
    "            # MJY:: Fuse the multiplication for speed.\n",
    "            output = (input - _unsqueeze_ft(mean)) * \\\n",
    "                     _unsqueeze_ft(inv_std * self.weight) + _unsqueeze_ft(self.bias)\n",
    "        else:\n",
    "            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std)\n",
    "\n",
    "        # Reshape it.\n",
    "        return output.view(input_shape)\n",
    "\n",
    "    def __data_parallel_replicate__(self, ctx, copy_id):\n",
    "        self._is_parallel = True\n",
    "        self._parallel_id = copy_id\n",
    "\n",
    "        # parallel_id == 0 means master device.\n",
    "        if self._parallel_id == 0:\n",
    "            ctx.sync_master = self._sync_master\n",
    "        else:\n",
    "            self._slave_pipe = ctx.sync_master.register_slave(copy_id)\n",
    "\n",
    "    def _data_parallel_master(self, intermediates):\n",
    "        \"\"\"Reduce the sum and square-sum, compute the statistics, and broadcast it.\"\"\"\n",
    "\n",
    "        # Always using same \"device order\" makes the ReduceAdd operation faster.\n",
    "        # Thanks to:: Tete Xiao (http://tetexiao.com/)\n",
    "        intermediates = sorted(intermediates, key=lambda i: i[1].sum.get_device())\n",
    "\n",
    "        to_reduce = [i[1][:2] for i in intermediates]\n",
    "        to_reduce = [j for i in to_reduce for j in i]  # flatten\n",
    "        target_gpus = [i[1].sum.get_device() for i in intermediates]\n",
    "\n",
    "        sum_size = sum([i[1].sum_size for i in intermediates])\n",
    "        sum_, ssum = ReduceAddCoalesced.apply(target_gpus[0], 2, *to_reduce)\n",
    "        mean, inv_std = self._compute_mean_std(sum_, ssum, sum_size)\n",
    "\n",
    "        broadcasted = Broadcast.apply(target_gpus, mean, inv_std)\n",
    "\n",
    "        outputs = []\n",
    "        for i, rec in enumerate(intermediates):\n",
    "            outputs.append((rec[0], _MasterMessage(*broadcasted[i * 2:i * 2 + 2])))\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def _compute_mean_std(self, sum_, ssum, size):\n",
    "        \"\"\"Compute the mean and standard-deviation with sum and square-sum. This method\n",
    "        also maintains the moving average on the master device.\"\"\"\n",
    "        assert size > 1, 'BatchNorm computes unbiased standard-deviation, which requires size > 1.'\n",
    "        mean = sum_ / size\n",
    "        sumvar = ssum - sum_ * mean\n",
    "        unbias_var = sumvar / (size - 1)\n",
    "        bias_var = sumvar / size\n",
    "\n",
    "        self.running_mean = (1 - self.momentum) * \\\n",
    "                            self.running_mean + self.momentum * mean.data\n",
    "        self.running_var = (1 - self.momentum) * \\\n",
    "                           self.running_var + self.momentum * unbias_var.data\n",
    "\n",
    "        return mean, bias_var.clamp(self.eps) ** -0.5\n",
    "\n",
    "\n",
    "class SynchronizedBatchNorm1d(_SynchronizedBatchNorm):\n",
    "    r\"\"\"Applies Synchronized Batch Normalization over a 2d or 3d input that is seen as a\n",
    "    mini-batch.\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n",
    "\n",
    "    This module differs from the built-in PyTorch BatchNorm1d as the mean and\n",
    "    standard-deviation are reduced across all devices during training.\n",
    "\n",
    "    For example, when one uses `nn.DataParallel` to wrap the network during\n",
    "    training, PyTorch's implementation normalize the tensor on each device using\n",
    "    the statistics only on that device, which accelerated the computation and\n",
    "    is also easy to implement, but the statistics might be inaccurate.\n",
    "    Instead, in this synchronized version, the statistics will be computed\n",
    "    over all training samples distributed on multiple devices.\n",
    "\n",
    "    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n",
    "    as the built-in PyTorch implementation.\n",
    "\n",
    "    The mean and standard-deviation are calculated per-dimension over\n",
    "    the mini-batches and gamma and beta are learnable parameter vectors\n",
    "    of size C (where C is the input size).\n",
    "\n",
    "    During training, this layer keeps a running estimate of its computed mean\n",
    "    and variance. The running sum is kept with a default momentum of 0.1.\n",
    "\n",
    "    During evaluation, this running mean/variance is used for normalization.\n",
    "\n",
    "    Because the BatchNorm is done over the `C` dimension, computing statistics\n",
    "    on `(N, L)` slices, it's common terminology to call this Temporal BatchNorm\n",
    "\n",
    "    Args:\n",
    "        num_features: num_features from an expected input of size\n",
    "            `batch_size x num_features [x width]`\n",
    "        eps: a value added to the denominator for numerical stability.\n",
    "            Default: 1e-5\n",
    "        momentum: the value used for the running_mean and running_var\n",
    "            computation. Default: 0.1\n",
    "        affine: a boolean value that when set to ``True``, gives the layer learnable\n",
    "            affine parameters. Default: ``True``\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(N, C)` or :math:`(N, C, L)`\n",
    "        - Output: :math:`(N, C)` or :math:`(N, C, L)` (same shape as input)\n",
    "\n",
    "    Examples:\n",
    "        >>> # With Learnable Parameters\n",
    "        >>> m = SynchronizedBatchNorm1d(100)\n",
    "        >>> # Without Learnable Parameters\n",
    "        >>> m = SynchronizedBatchNorm1d(100, affine=False)\n",
    "        >>> input = torch.autograd.Variable(torch.randn(20, 100))\n",
    "        >>> output = m(input)\n",
    "    \"\"\"\n",
    "\n",
    "    def _check_input_dim(self, input):\n",
    "        if input.dim() != 2 and input.dim() != 3:\n",
    "            raise ValueError('expected 2D or 3D input (got {}D input)'\n",
    "                             .format(input.dim()))\n",
    "        super(SynchronizedBatchNorm1d, self)._check_input_dim(input)\n",
    "\n",
    "\n",
    "class SynchronizedBatchNorm2d(_SynchronizedBatchNorm):\n",
    "    r\"\"\"Applies Batch Normalization over a 4d input that is seen as a mini-batch\n",
    "    of 3d inputs\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n",
    "\n",
    "    This module differs from the built-in PyTorch BatchNorm2d as the mean and\n",
    "    standard-deviation are reduced across all devices during training.\n",
    "\n",
    "    For example, when one uses `nn.DataParallel` to wrap the network during\n",
    "    training, PyTorch's implementation normalize the tensor on each device using\n",
    "    the statistics only on that device, which accelerated the computation and\n",
    "    is also easy to implement, but the statistics might be inaccurate.\n",
    "    Instead, in this synchronized version, the statistics will be computed\n",
    "    over all training samples distributed on multiple devices.\n",
    "\n",
    "    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n",
    "    as the built-in PyTorch implementation.\n",
    "\n",
    "    The mean and standard-deviation are calculated per-dimension over\n",
    "    the mini-batches and gamma and beta are learnable parameter vectors\n",
    "    of size C (where C is the input size).\n",
    "\n",
    "    During training, this layer keeps a running estimate of its computed mean\n",
    "    and variance. The running sum is kept with a default momentum of 0.1.\n",
    "\n",
    "    During evaluation, this running mean/variance is used for normalization.\n",
    "\n",
    "    Because the BatchNorm is done over the `C` dimension, computing statistics\n",
    "    on `(N, H, W)` slices, it's common terminology to call this Spatial BatchNorm\n",
    "\n",
    "    Args:\n",
    "        num_features: num_features from an expected input of\n",
    "            size batch_size x num_features x height x width\n",
    "        eps: a value added to the denominator for numerical stability.\n",
    "            Default: 1e-5\n",
    "        momentum: the value used for the running_mean and running_var\n",
    "            computation. Default: 0.1\n",
    "        affine: a boolean value that when set to ``True``, gives the layer learnable\n",
    "            affine parameters. Default: ``True``\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(N, C, H, W)`\n",
    "        - Output: :math:`(N, C, H, W)` (same shape as input)\n",
    "\n",
    "    Examples:\n",
    "        >>> # With Learnable Parameters\n",
    "        >>> m = SynchronizedBatchNorm2d(100)\n",
    "        >>> # Without Learnable Parameters\n",
    "        >>> m = SynchronizedBatchNorm2d(100, affine=False)\n",
    "        >>> input = torch.autograd.Variable(torch.randn(20, 100, 35, 45))\n",
    "        >>> output = m(input)\n",
    "    \"\"\"\n",
    "\n",
    "    def _check_input_dim(self, input):\n",
    "        if input.dim() != 4:\n",
    "            raise ValueError('expected 4D input (got {}D input)'\n",
    "                             .format(input.dim()))\n",
    "        super(SynchronizedBatchNorm2d, self)._check_input_dim(input)\n",
    "\n",
    "\n",
    "class SynchronizedBatchNorm3d(_SynchronizedBatchNorm):\n",
    "    r\"\"\"Applies Batch Normalization over a 5d input that is seen as a mini-batch\n",
    "    of 4d inputs\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n",
    "\n",
    "    This module differs from the built-in PyTorch BatchNorm3d as the mean and\n",
    "    standard-deviation are reduced across all devices during training.\n",
    "\n",
    "    For example, when one uses `nn.DataParallel` to wrap the network during\n",
    "    training, PyTorch's implementation normalize the tensor on each device using\n",
    "    the statistics only on that device, which accelerated the computation and\n",
    "    is also easy to implement, but the statistics might be inaccurate.\n",
    "    Instead, in this synchronized version, the statistics will be computed\n",
    "    over all training samples distributed on multiple devices.\n",
    "\n",
    "    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n",
    "    as the built-in PyTorch implementation.\n",
    "\n",
    "    The mean and standard-deviation are calculated per-dimension over\n",
    "    the mini-batches and gamma and beta are learnable parameter vectors\n",
    "    of size C (where C is the input size).\n",
    "\n",
    "    During training, this layer keeps a running estimate of its computed mean\n",
    "    and variance. The running sum is kept with a default momentum of 0.1.\n",
    "\n",
    "    During evaluation, this running mean/variance is used for normalization.\n",
    "\n",
    "    Because the BatchNorm is done over the `C` dimension, computing statistics\n",
    "    on `(N, D, H, W)` slices, it's common terminology to call this Volumetric BatchNorm\n",
    "    or Spatio-temporal BatchNorm\n",
    "\n",
    "    Args:\n",
    "        num_features: num_features from an expected input of\n",
    "            size batch_size x num_features x depth x height x width\n",
    "        eps: a value added to the denominator for numerical stability.\n",
    "            Default: 1e-5\n",
    "        momentum: the value used for the running_mean and running_var\n",
    "            computation. Default: 0.1\n",
    "        affine: a boolean value that when set to ``True``, gives the layer learnable\n",
    "            affine parameters. Default: ``True``\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(N, C, D, H, W)`\n",
    "        - Output: :math:`(N, C, D, H, W)` (same shape as input)\n",
    "\n",
    "    Examples:\n",
    "        >>> # With Learnable Parameters\n",
    "        >>> m = SynchronizedBatchNorm3d(100)\n",
    "        >>> # Without Learnable Parameters\n",
    "        >>> m = SynchronizedBatchNorm3d(100, affine=False)\n",
    "        >>> input = torch.autograd.Variable(torch.randn(20, 100, 35, 45, 10))\n",
    "        >>> output = m(input)\n",
    "    \"\"\"\n",
    "\n",
    "    def _check_input_dim(self, input):\n",
    "        if input.dim() != 5:\n",
    "            raise ValueError('expected 5D input (got {}D input)'\n",
    "                             .format(input.dim()))\n",
    "        super(SynchronizedBatchNorm3d, self)._check_input_dim(input)\n",
    "\n",
    "\n",
    "def convert_model(module):\n",
    "    \"\"\"Traverse the input module and its child recursively\n",
    "       and replace all instance of torch.nn.modules.batchnorm.BatchNorm*N*d\n",
    "       to SynchronizedBatchNorm*N*d\n",
    "\n",
    "    Args:\n",
    "        module: the input module needs to be convert to SyncBN model\n",
    "\n",
    "    Examples:\n",
    "        >>> import torch.nn as nn\n",
    "        >>> import torchvision\n",
    "        >>> # m is a standard pytorch model\n",
    "        >>> m = torchvision.models.resnet18(True)\n",
    "        >>> m = nn.DataParallel(m)\n",
    "        >>> # after convert, m is using SyncBN\n",
    "        >>> m = convert_model(m)\n",
    "    \"\"\"\n",
    "    if isinstance(module, torch.nn.DataParallel):\n",
    "        mod = module.module\n",
    "        mod = convert_model(mod)\n",
    "        mod = DataParallelWithCallback(mod)\n",
    "        return mod\n",
    "\n",
    "    mod = module\n",
    "    for pth_module, sync_module in zip([torch.nn.modules.batchnorm.BatchNorm1d,\n",
    "                                        torch.nn.modules.batchnorm.BatchNorm2d,\n",
    "                                        torch.nn.modules.batchnorm.BatchNorm3d],\n",
    "                                       [SynchronizedBatchNorm1d,\n",
    "                                        SynchronizedBatchNorm2d,\n",
    "                                        SynchronizedBatchNorm3d]):\n",
    "        if isinstance(module, pth_module):\n",
    "            mod = sync_module(module.num_features, module.eps,\n",
    "                              module.momentum, module.affine)\n",
    "            mod.running_mean = module.running_mean\n",
    "            mod.running_var = module.running_var\n",
    "            if module.affine:\n",
    "                mod.weight.data = module.weight.data.clone().detach()\n",
    "                mod.bias.data = module.bias.data.clone().detach()\n",
    "\n",
    "    for name, child in module.named_children():\n",
    "        mod.add_module(name, convert_model(child))\n",
    "\n",
    "    return mod\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### warmupLR.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class warmupLR(toptim._LRScheduler):\n",
    "    \"\"\" Warmup learning rate scheduler.\n",
    "        Initially, increases the learning rate from 0 to the final value, in a\n",
    "        certain number of steps. After this number of steps, each step decreases\n",
    "        LR exponentially.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, lr, warmup_steps, momentum, decay):\n",
    "        # cyclic params\n",
    "        self.optimizer = optimizer\n",
    "        self.lr = lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.momentum = momentum\n",
    "        self.decay = decay\n",
    "\n",
    "        # cap to one\n",
    "        if self.warmup_steps < 1:\n",
    "            self.warmup_steps = 1\n",
    "\n",
    "        # cyclic lr\n",
    "        self.initial_scheduler = toptim.CyclicLR(self.optimizer,\n",
    "                                                 base_lr=0,\n",
    "                                                 max_lr=self.lr,\n",
    "                                                 step_size_up=self.warmup_steps,\n",
    "                                                 step_size_down=self.warmup_steps,\n",
    "                                                 cycle_momentum=False,\n",
    "                                                 base_momentum=self.momentum,\n",
    "                                                 max_momentum=self.momentum)\n",
    "\n",
    "        # our params\n",
    "        self.last_epoch = -1  # fix for pytorch 1.1 and below\n",
    "        self.finished = False  # am i done\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [self.lr * (self.decay ** self.last_epoch) for lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if self.finished or self.initial_scheduler.last_epoch >= self.warmup_steps:\n",
    "            if not self.finished:\n",
    "                self.base_lrs = [self.lr for lr in self.base_lrs]\n",
    "                self.finished = True\n",
    "            return super(warmupLR, self).step(epoch)\n",
    "        else:\n",
    "            return self.initial_scheduler.step(epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### onehot.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "Num classes 1d = 6\n",
      "********************************************************************************\n",
      "Tensor 1d spat dim, unbatched\n",
      "in: tensor([0, 1, 2, 3, 4, 5], device='cuda:0')\n",
      "out: tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.]], device='cuda:0')\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Tensor 1d spat dim, batched\n",
      "in: tensor([[0, 1, 2, 3, 4, 5],\n",
      "        [0, 1, 2, 3, 4, 5]], device='cuda:0')\n",
      "out: tensor([[[1., 0., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0., 0.],\n",
      "         [0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 0., 0., 1.]],\n",
      "\n",
      "        [[1., 0., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0., 0.],\n",
      "         [0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 0., 0., 1.]]], device='cuda:0')\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Num classes 2d = 3\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Tensor 2d spat dim, unbatched\n",
      "in: tensor([[0, 1, 2],\n",
      "        [0, 1, 2],\n",
      "        [0, 1, 2],\n",
      "        [0, 1, 2]], device='cuda:0')\n",
      "out: tensor([[[1., 0., 0.],\n",
      "         [1., 0., 0.],\n",
      "         [1., 0., 0.],\n",
      "         [1., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0.],\n",
      "         [0., 1., 0.],\n",
      "         [0., 1., 0.],\n",
      "         [0., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 1.],\n",
      "         [0., 0., 1.],\n",
      "         [0., 0., 1.],\n",
      "         [0., 0., 1.]]], device='cuda:0')\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Tensor 2d spat dim, unbatched\n",
      "in: tensor([[[0, 1, 2],\n",
      "         [0, 1, 2],\n",
      "         [0, 1, 2],\n",
      "         [0, 1, 2]],\n",
      "\n",
      "        [[0, 1, 2],\n",
      "         [0, 1, 2],\n",
      "         [0, 1, 2],\n",
      "         [0, 1, 2]]], device='cuda:0')\n",
      "out: tensor([[[[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]],\n",
      "\n",
      "         [[0., 1., 0.],\n",
      "          [0., 1., 0.],\n",
      "          [0., 1., 0.],\n",
      "          [0., 1., 0.]],\n",
      "\n",
      "         [[0., 0., 1.],\n",
      "          [0., 0., 1.],\n",
      "          [0., 0., 1.],\n",
      "          [0., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]],\n",
      "\n",
      "         [[0., 1., 0.],\n",
      "          [0., 1., 0.],\n",
      "          [0., 1., 0.],\n",
      "          [0., 1., 0.]],\n",
      "\n",
      "         [[0., 0., 1.],\n",
      "          [0., 0., 1.],\n",
      "          [0., 0., 1.],\n",
      "          [0., 0., 1.]]]], device='cuda:0')\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "class oneHot(nn.Module):\n",
    "    def __init__(self, device, nclasses, spatial_dim=2):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.nclasses = nclasses\n",
    "        self.spatial_dim = spatial_dim\n",
    "\n",
    "    def onehot1dspatial(self, x):\n",
    "        # we only do tensors that 1d tensors that are batched or not, so check\n",
    "        assert (len(x.shape) == 1 or len(x.shape) == 2)\n",
    "        # if not batched, batch\n",
    "        remove_dim = False  # flag to unbatch\n",
    "        if len(x.shape) == 1:\n",
    "            # add batch dimension\n",
    "            x = x[None, ...]\n",
    "            remove_dim = True\n",
    "\n",
    "        # get tensor shape\n",
    "        n, b = x.shape\n",
    "\n",
    "        # scatter to onehot\n",
    "        one_hot = torch.zeros((n, self.nclasses, b),\n",
    "                              device=self.device).scatter_(1, x.unsqueeze(1), 1)\n",
    "\n",
    "        # x is now [n,classes,b]\n",
    "\n",
    "        # if it used to be unbatched, then unbatch it\n",
    "        if remove_dim:\n",
    "            one_hot = one_hot[0]\n",
    "\n",
    "        return one_hot\n",
    "\n",
    "    def onehot2dspatial(self, x):\n",
    "        # we only do tensors that 2d tensors that are batched or not, so check\n",
    "        assert (len(x.shape) == 2 or len(x.shape) == 3)\n",
    "        # if not batched, batch\n",
    "        remove_dim = False  # flag to unbatch\n",
    "        if len(x.shape) == 2:\n",
    "            # add batch dimension\n",
    "            x = x[None, ...]\n",
    "            remove_dim = True\n",
    "\n",
    "        # get tensor shape\n",
    "        n, h, w = x.shape\n",
    "\n",
    "        # scatter to onehot\n",
    "        one_hot = torch.zeros((n, self.nclasses, h, w),\n",
    "                              device=self.device).scatter_(1, x.unsqueeze(1), 1)\n",
    "\n",
    "        # x is now [n,classes,b]\n",
    "\n",
    "        # if it used to be unbatched, then unbatch it\n",
    "        if remove_dim:\n",
    "            one_hot = one_hot[0]\n",
    "\n",
    "        return one_hot\n",
    "\n",
    "    def forward(self, x):\n",
    "        # do onehot here\n",
    "        if self.spatial_dim == 1:\n",
    "            return self.onehot1dspatial(x)\n",
    "        elif self.spatial_dim == 2:\n",
    "            return self.onehot2dspatial(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bordermask.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Border Mask for 2D labeled range images.\n",
    "\n",
    "Simple module to obtain the border mask of a given range image.\n",
    "\n",
    "The border mask is defined as the zone where are intersections between\n",
    "differrent classes for the given range image.\n",
    "\n",
    "In this case we will violate a little bit the definition and will augment it. We\n",
    "define the border mask as the zone where are intersections between differnet\n",
    "classes for the given range image in determined neighborhood. To obtain this\n",
    "border mask we will need to apply de binary erosion algorithm multiple times to\n",
    "the same range image.\n",
    "\n",
    "Example:\n",
    "  Suppose we have 3 classes and this given range image(labeled):\n",
    "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0],\n",
    "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0],\n",
    "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0],\n",
    "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0],\n",
    "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0],\n",
    "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "  The output of the bordermask would like:\n",
    "  # 1 erode iteration with a connectivity kernel of 4:\n",
    "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "  [1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1],\n",
    "  [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1],\n",
    "  [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1],\n",
    "  [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1],\n",
    "  [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1],\n",
    "  [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1],\n",
    "  [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1],\n",
    "  [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1],\n",
    "  [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1],\n",
    "  [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1],\n",
    "  [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1],\n",
    "  [1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1],\n",
    "  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1],\n",
    "  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1],\n",
    "  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1],\n",
    "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "  # 2 erode iterations with a connectivity kernel of 8:\n",
    "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1],\n",
    "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1],\n",
    "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1],\n",
    "  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1],\n",
    "  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1],\n",
    "  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1],\n",
    "  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1],\n",
    "  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1],\n",
    "  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1],\n",
    "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1],\n",
    "  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
    "\"\"\"\n",
    "\n",
    "class borderMask(nn.Module):\n",
    "    def __init__(self, nclasses, device, border_size, kern_conn=4, background_class=None):\n",
    "        \"\"\"Get the binary border mask of a labeled 2d range image.\n",
    "\n",
    "      Args:\n",
    "          nclasses(int)         : The number of classes labeled in the input image\n",
    "          device(torch.device)  : Process in host or cuda?\n",
    "          border_size(int)      : How many erode iterations to perform for the mask\n",
    "          kern_conn(int)        : The connectivity kernel number (4 or 8)\n",
    "          background_class(int) : \"unlabeled\" class in dataset (to avoid double borders)\n",
    "\n",
    "      Returns:\n",
    "          eroded_output(tensor) : The 2d binary border mask, 1 where a intersection\n",
    "                                  between classes occurs, 0 everywhere else\n",
    "\n",
    "      \"\"\"\n",
    "        super().__init__()\n",
    "        self.nclasses = nclasses\n",
    "        self.device = device\n",
    "        self.border_size = border_size\n",
    "        self.kern_conn = kern_conn\n",
    "        self.background_class = background_class\n",
    "        if self.background_class is not None:\n",
    "            self.include_idx = list(range(self.nclasses))\n",
    "            self.exclude_idx = self.include_idx.pop(self.background_class)\n",
    "\n",
    "        # check connectivity\n",
    "        # For obtaining the border mask we will be eroding the input image, for this\n",
    "        # reason we only support erode_kernels with connectivity 4 or 8\n",
    "        assert self.kern_conn in (4, 8), (\"The specified kernel connectivity(kern_conn= %r) is \"\n",
    "                                          \"not supported\" % self.kern_conn)\n",
    "\n",
    "        # make the onehot inferer\n",
    "        self.onehot = oneHot(self.device,\n",
    "                             self.nclasses,\n",
    "                             spatial_dim=2)  # range labels\n",
    "\n",
    "    def forward(self, range_label):\n",
    "        # length of shape of range_label must be 3 (N, H, W)\n",
    "        must_unbatch = False  # remove batch dimension after operation?\n",
    "        if len(range_label.shape) != 3:\n",
    "            range_label = range_label[None, ...]\n",
    "            must_unbatch = True\n",
    "\n",
    "        # The range_label comes labeled, we need to create one tensor per class, thus:\n",
    "        input_tensor = self.onehot(range_label)  # (N, C, H, W)\n",
    "\n",
    "        # Because we are using GT range_labels, there is a lot of pixels that end up\n",
    "        # unlabeled(thus, in the background). If we feed the erosion algorithm with\n",
    "        # this \"raw\" gt_labels we will detect intersection between the other classes\n",
    "        # and the backgorund, and we will end with the incorrect border mask. To solve\n",
    "        # this issue we need to pre process the input gt_label. The artifact in this\n",
    "        # case will be to sum the background channel(mostly the channel 0) to\n",
    "        # all the rest channels expect for the background channel itself.\n",
    "        # This will allow us to avoid detecting intersections between a class and the\n",
    "        # background. This also force us to change the logical AND we were doing to\n",
    "        # obtain the border mask when we were working with predicted labels.\n",
    "        # With predicted labels won't see this problem because all the pixels belongs\n",
    "        # to at least one class\n",
    "        if self.background_class is not None:\n",
    "            input_tensor[:, self.include_idx] = input_tensor[:, self.include_idx] + \\\n",
    "                                                input_tensor[:, self.exclude_idx]\n",
    "\n",
    "        # C denotes a number of channels, N, H and W are dismissed\n",
    "        C = input_tensor.shape[1]\n",
    "\n",
    "        # Create an empty erode kernel and send it to 'device'\n",
    "        erode_kernel = torch.zeros((C, 1, 3, 3), device=self.device)\n",
    "        if self.kern_conn == 4:\n",
    "            erode_kernel[:] = torch.tensor([[0, 1, 0],\n",
    "                                            [1, 1, 1],\n",
    "                                            [0, 1, 0]], device=self.device)\n",
    "        else:\n",
    "            erode_kernel[:] = torch.tensor([[1, 1, 1],\n",
    "                                            [1, 1, 1],\n",
    "                                            [1, 1, 1]], device=self.device)\n",
    "\n",
    "        # to check connectivity\n",
    "        kernel_sum = erode_kernel[0][0].sum()  # should be kern_conn + 1\n",
    "\n",
    "        # erode the input image border_size times\n",
    "        erode_input = input_tensor\n",
    "        for _ in range(self.border_size):\n",
    "            eroded_output = F.conv2d(erode_input, erode_kernel, groups=C, padding=1)\n",
    "            # Pick the elements that match the kernel_sum to obtain the eroded\n",
    "            # output and convert to dtype=float32\n",
    "            eroded_output = (eroded_output == kernel_sum).float()\n",
    "            erode_input = eroded_output\n",
    "\n",
    "        # We want to sum up all the channels into 1 unique border mask\n",
    "        # Even when we added the background to all the rest of the channels, there\n",
    "        # might be \"bodies\" in the background channel, thus, the erosion process can\n",
    "        # output \"false positives\" were this \"bodies\" are present in the background.\n",
    "        # We need to obtain the background mask and add it to the eroded bodies to\n",
    "        # obtain a consisent output once we calculate the border mask\n",
    "        if self.background_class is not None:\n",
    "            background_mask = (eroded_output[:, self.exclude_idx] == 1)\n",
    "\n",
    "        # The eroded_bodies mask will consist in all the pixels were the convolution\n",
    "        # returned 1 for all the channels, therefore we need to sum up all the\n",
    "        # channels into one unique tensor and add the background mask to avoid having\n",
    "        # the background in the border mask output\n",
    "        eroded_bodies = (eroded_output.sum(1, keepdim=True) == 1)\n",
    "        if self.background_class is not None:\n",
    "            eroded_bodies = eroded_bodies + background_mask\n",
    "\n",
    "        # we want the opposite\n",
    "        borders = 1 - eroded_bodies\n",
    "\n",
    "        # unbatch?\n",
    "        if must_unbatch:\n",
    "            borders = borders[0]\n",
    "            # import cv2\n",
    "            # import numpy as np\n",
    "            # bordersprint = (borders * 255).squeeze().cpu().numpy().astype(np.uint8)\n",
    "            # cv2.imshow(\"border\", bordersprint)\n",
    "            # cv2.waitKey(0)\n",
    "\n",
    "        return borders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ioueval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IOU EVAL] IGNORE:  tensor([], dtype=torch.int64)\n",
      "[IOU EVAL] INCLUDE:  tensor([0, 1])\n",
      "********************************************************************************\n",
      "Small iou mock problem\n",
      "IoU:  tensor(0.5089, dtype=torch.float64)\n",
      "IoU class:  tensor([0.8750, 0.1429], dtype=torch.float64)\n",
      "Acc:  tensor(0.8776, dtype=torch.float64)\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "class iouEval:\n",
    "    def __init__(self, n_classes, device, ignore=None):\n",
    "        self.n_classes = n_classes\n",
    "        self.device = device\n",
    "        # if ignore is larger than n_classes, consider no ignoreIndex\n",
    "        self.ignore = torch.tensor(ignore).long()\n",
    "        self.include = torch.tensor(\n",
    "            [n for n in range(self.n_classes) if n not in self.ignore]).long()\n",
    "        print(\"[IOU EVAL] IGNORE: \", self.ignore)\n",
    "        print(\"[IOU EVAL] INCLUDE: \", self.include)\n",
    "        self.reset()\n",
    "\n",
    "    def num_classes(self):\n",
    "        return self.n_classes\n",
    "\n",
    "    def reset(self):\n",
    "        self.conf_matrix = torch.zeros(\n",
    "            (self.n_classes, self.n_classes), device=self.device).long()\n",
    "        self.ones = None\n",
    "        self.last_scan_size = None  # for when variable scan size is used\n",
    "\n",
    "    def addBatch(self, x, y):  # x=preds, y=targets\n",
    "        # if numpy, pass to pytorch\n",
    "        # to tensor\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.from_numpy(np.array(x)).long().to(self.device)\n",
    "        if isinstance(y, np.ndarray):\n",
    "            y = torch.from_numpy(np.array(y)).long().to(self.device)\n",
    "\n",
    "        # sizes should be \"batch_size x H x W\"\n",
    "        x_row = x.reshape(-1)  # de-batchify\n",
    "        y_row = y.reshape(-1)  # de-batchify\n",
    "\n",
    "        # idxs are labels and predictions\n",
    "        idxs = torch.stack([x_row, y_row], dim=0)\n",
    "\n",
    "        # ones is what I want to add to conf when I\n",
    "        if self.ones is None or self.last_scan_size != idxs.shape[-1]:\n",
    "            self.ones = torch.ones((idxs.shape[-1]), device=self.device).long()\n",
    "            self.last_scan_size = idxs.shape[-1]\n",
    "\n",
    "        # make confusion matrix (cols = gt, rows = pred)\n",
    "        self.conf_matrix = self.conf_matrix.index_put_(\n",
    "            tuple(idxs), self.ones, accumulate=True)\n",
    "\n",
    "        # print(self.tp.shape)\n",
    "        # print(self.fp.shape)\n",
    "        # print(self.fn.shape)\n",
    "\n",
    "    def getStats(self):\n",
    "        # remove fp and fn from confusion on the ignore classes cols and rows\n",
    "        conf = self.conf_matrix.clone().double()\n",
    "        conf[self.ignore] = 0\n",
    "        conf[:, self.ignore] = 0\n",
    "\n",
    "        # get the clean stats\n",
    "        tp = conf.diag()\n",
    "        fp = conf.sum(dim=1) - tp\n",
    "        fn = conf.sum(dim=0) - tp\n",
    "        return tp, fp, fn\n",
    "\n",
    "    def getIoU(self):\n",
    "        tp, fp, fn = self.getStats()\n",
    "        intersection = tp\n",
    "        union = tp + fp + fn + 1e-15\n",
    "        iou = intersection / union\n",
    "        iou_mean = (intersection[self.include] / union[self.include]).mean()\n",
    "        return iou_mean, iou  # returns \"iou mean\", \"iou per class\" ALL CLASSES\n",
    "\n",
    "    def getacc(self):\n",
    "        tp, fp, fn = self.getStats()\n",
    "        total_tp = tp.sum()\n",
    "        total = tp[self.include].sum() + fp[self.include].sum() + 1e-15\n",
    "        acc_mean = total_tp / total\n",
    "        return acc_mean  # returns \"acc mean\"\n",
    "\n",
    "\n",
    "class biouEval(iouEval):\n",
    "    def __init__(self, n_classes, device, ignore=None, border_size=1, kern_conn=4):\n",
    "        super().__init__(n_classes, device, ignore)\n",
    "        self.border_size = border_size\n",
    "        self.kern_conn = kern_conn\n",
    "\n",
    "        # check that I am only ignoring one class\n",
    "        if len(ignore) > 1:\n",
    "            raise ValueError(\"Length of ignored class list should be 1 or 0\")\n",
    "        elif len(ignore) == 0:\n",
    "            ignore = None\n",
    "        else:\n",
    "            ignore = ignore[0]\n",
    "\n",
    "        self.borderer = borderMask(self.n_classes, self.device,\n",
    "                                   self.border_size, self.kern_conn,\n",
    "                                   background_class=ignore)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        return\n",
    "\n",
    "    def addBorderBatch1d(self, range_y, x, y, px, py):\n",
    "        '''range_y=target as img, x=preds, y=targets, px,py=idxs of points of\n",
    "           pointcloud in range img\n",
    "           WARNING: Only batch size 1 works for now\n",
    "        '''\n",
    "        # if numpy, pass to pytorch\n",
    "        # to tensor\n",
    "        if isinstance(range_y, np.ndarray):\n",
    "            range_y = torch.from_numpy(np.array(range_y)).long().to(self.device)\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.from_numpy(np.array(x)).long().to(self.device)\n",
    "        if isinstance(y, np.ndarray):\n",
    "            y = torch.from_numpy(np.array(y)).long().to(self.device)\n",
    "        if isinstance(px, np.ndarray):\n",
    "            px = torch.from_numpy(np.array(px)).long().to(self.device)\n",
    "        if isinstance(py, np.ndarray):\n",
    "            py = torch.from_numpy(np.array(py)).long().to(self.device)\n",
    "\n",
    "        # get border mask of range_y\n",
    "        border_mask_2d = self.borderer(range_y)\n",
    "\n",
    "        # filter px, py according to if they are on border mask or not\n",
    "        border_mask_1d = border_mask_2d[0, py, px].byte()\n",
    "\n",
    "        # get proper points from filtered x and y\n",
    "        x_in_mask = torch.masked_select(x, border_mask_1d)\n",
    "        y_in_mask = torch.masked_select(y, border_mask_1d)\n",
    "\n",
    "        # add batch\n",
    "        self.addBatch(x_in_mask, y_in_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### segmentator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/usr/bin/env python3\n",
    "# This file is covered by the LICENSE file in the root of this project.\n",
    "\n",
    "\n",
    "TRAIN_PATH = \"../\"\n",
    "\n",
    "class Add(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Add, self).__init__()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        return x + y\n",
    "\n",
    "\n",
    "class resBlock_with_add(nn.Module):\n",
    "    def __init__(self, conv, act, bn):\n",
    "        super(resBlock_with_add, self).__init__()\n",
    "\n",
    "        self.conv = conv\n",
    "        self.act = act\n",
    "        self.bn = bn\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        res = self.conv(x)\n",
    "        res = self.act(res)\n",
    "        res = self.bn(res)\n",
    "        return res + y\n",
    "\n",
    "\n",
    "class Trans(nn.Module):\n",
    "    def __init__(self, trans, trans_act, trans_bn):\n",
    "        super(Trans, self).__init__()\n",
    "        self.trans = trans\n",
    "        self.trans_act = trans_act\n",
    "        self.trans_bn = trans_bn\n",
    "\n",
    "    def forward(self, x):\n",
    "        upA = self.trans(x)\n",
    "        upA = self.trans_act(upA)\n",
    "        upA = self.trans_bn(upA)\n",
    "        return upA\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, f_g, f_l, f_int):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.Wg = nn.Sequential(nn.Conv2d(f_g, f_int, kernel_size=1, padding=0, stride=1),\n",
    "                                nn.BatchNorm2d(f_int))\n",
    "\n",
    "        self.Wx = nn.Sequential(nn.Conv2d(f_l, f_int, kernel_size=1, padding=0, stride=1),\n",
    "                                nn.BatchNorm2d(f_int))\n",
    "\n",
    "        self.psi = nn.Sequential(nn.Conv2d(f_int, 1, kernel_size=1, padding=0, stride=1),\n",
    "                                 nn.BatchNorm2d(1),\n",
    "                                 nn.Sigmoid())\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.Wg(g)\n",
    "        x1 = self.Wx(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        return x * psi\n",
    "\n",
    "\n",
    "class ResContextBlock(nn.Module):\n",
    "    def __init__(self, in_filters, out_filters, kernel_size=(3, 3), stride=1):\n",
    "        super(ResContextBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_filters, out_filters, kernel_size=(1, 1), stride=stride)\n",
    "        self.act1 = nn.LeakyReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_filters, out_filters, kernel_size, padding=1)\n",
    "        self.act2 = nn.LeakyReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(out_filters)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(out_filters, out_filters, kernel_size, padding=1)\n",
    "        self.act3 = nn.LeakyReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(out_filters)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = self.conv1(x)\n",
    "        shortcut = self.act1(shortcut)\n",
    "\n",
    "        resA = self.conv2(x)\n",
    "        resA = self.act2(resA)\n",
    "        resA = self.bn1(resA)\n",
    "\n",
    "        resA = self.conv3(resA)\n",
    "        resA = self.act3(resA)\n",
    "        resA = self.bn2(resA)\n",
    "        return resA + shortcut\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_filters, out_filters, dropout_rate, kernel_size=(3, 3), stride=1,\n",
    "                 pooling=True, drop_out=True):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.pooling = pooling\n",
    "        self.drop_out = drop_out\n",
    "        self.conv1 = nn.Conv2d(in_filters, out_filters, kernel_size=(1, 1), stride=stride)\n",
    "        self.act1 = nn.LeakyReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_filters, out_filters, kernel_size=kernel_size, padding=1)\n",
    "        self.act2 = nn.LeakyReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(out_filters)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(out_filters, out_filters, kernel_size=kernel_size, padding=1)\n",
    "        self.act3 = nn.LeakyReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(out_filters)\n",
    "\n",
    "        if pooling:\n",
    "            self.dropout = nn.Dropout2d(p=dropout_rate)\n",
    "            self.pool = nn.AvgPool2d(kernel_size=kernel_size, stride=2, padding=1)\n",
    "        else:\n",
    "            self.dropout = nn.Dropout2d(p=dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = self.conv1(x)\n",
    "        shortcut = self.act1(shortcut)\n",
    "\n",
    "        resA = self.conv2(x)\n",
    "        resA = self.act2(resA)\n",
    "        resA = self.bn1(resA)\n",
    "\n",
    "        resA = self.conv3(resA)\n",
    "        resA = self.act3(resA)\n",
    "        resA = self.bn2(resA)\n",
    "        resA = shortcut + resA\n",
    "\n",
    "        if self.pooling:\n",
    "            if self.drop_out:\n",
    "                resB = self.dropout(resA)\n",
    "            else:\n",
    "                resB = resA\n",
    "            resB = self.pool(resB)\n",
    "\n",
    "            return resB, resA\n",
    "        else:\n",
    "            if self.drop_out:\n",
    "                resB = self.dropout(resA)\n",
    "            else:\n",
    "                resB = resA\n",
    "            return resB\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_filters, out_filters, dropout_rate, kernel_size=(3, 3),drop_out=True):\n",
    "        super(UpBlock, self).__init__()\n",
    "        self.drop_out = drop_out\n",
    "        self.trans = nn.ConvTranspose2d(in_filters, out_filters, kernel_size, stride=(2, 2), padding=1)\n",
    "        self.trans_act = nn.LeakyReLU()\n",
    "        self.trans_bn = nn.BatchNorm2d(out_filters)\n",
    "\n",
    "        self.dropout1 = nn.Dropout2d(p=dropout_rate)\n",
    "        self.dropout2 = nn.Dropout2d(p=dropout_rate)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(out_filters, out_filters, kernel_size, padding=1)\n",
    "        self.act1 = nn.LeakyReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(out_filters)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_filters, out_filters, kernel_size, padding=1)\n",
    "        self.act2 = nn.LeakyReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(out_filters)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(out_filters, out_filters, kernel_size, padding=1)\n",
    "        self.act3 = nn.LeakyReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(out_filters)\n",
    "        self.dropout3 = nn.Dropout2d(p=dropout_rate)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        upA = self.trans(x)\n",
    "        if upA.shape != skip.shape:\n",
    "            upA = F.pad(upA, (0, 1, 0, 1), mode='replicate')\n",
    "        upA = self.trans_act(upA)\n",
    "        upA = self.trans_bn(upA)\n",
    "        if self.drop_out:\n",
    "            upA = self.dropout1(upA)\n",
    "        upB = upA + skip\n",
    "        if self.drop_out:\n",
    "            upB = self.dropout2(upB)\n",
    "\n",
    "        upE = self.conv1(upB)\n",
    "        upE = self.act1(upE)\n",
    "        upE = self.bn1(upE)\n",
    "\n",
    "        upE = self.conv2(upE)\n",
    "        upE = self.act2(upE)\n",
    "        upE = self.bn2(upE)\n",
    "\n",
    "        upE = self.conv3(upE)\n",
    "        upE = self.act3(upE)\n",
    "        upE = self.bn3(upE)\n",
    "        if self.drop_out:\n",
    "            upE = self.dropout3(upE)\n",
    "\n",
    "        return upE\n",
    "\n",
    "\n",
    "class SalsaNet(nn.Module):\n",
    "    def __init__(self, ARCH, nclasses, path=None, path_append=\"\", strict=False):\n",
    "        super(SalsaNet, self).__init__()\n",
    "        self.ARCH = ARCH\n",
    "        self.nclasses = nclasses\n",
    "        self.path = path\n",
    "        self.path_append = path_append\n",
    "        self.strict = False\n",
    "\n",
    "        self.downCntx = ResContextBlock(5, 32)\n",
    "        self.resBlock1 = ResBlock(32, 32, 0.2, pooling=True, drop_out=False)\n",
    "        self.resBlock2 = ResBlock(32, 2 * 32, 0.2, pooling=True)\n",
    "        self.resBlock3 = ResBlock(2 * 32, 4 * 32, 0.2, pooling=True)\n",
    "        self.resBlock4 = ResBlock(4 * 32, 8 * 32, 0.2, pooling=True)\n",
    "        self.resBlock5 = ResBlock(8 * 32, 16 * 32, 0.2, pooling=True)\n",
    "        self.resBlock6 = ResBlock(16 * 32, 16 * 32, 0.2, pooling=False)\n",
    "\n",
    "        self.upBlock1 = UpBlock(16 * 32, 16 * 32, 0.2)\n",
    "        self.upBlock2 = UpBlock(16 * 32, 8 * 32, 0.2)\n",
    "        self.upBlock3 = UpBlock(8 * 32, 4 * 32, 0.2)\n",
    "        self.upBlock4 = UpBlock(4 * 32, 2 * 32, 0.2)\n",
    "        self.upBlock5 = UpBlock(2 * 32, 32, 0.2, drop_out=False)\n",
    "\n",
    "        self.logits = nn.Conv2d(32, nclasses, kernel_size=(1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        downCntx = self.downCntx(x)\n",
    "        down0c, down0b = self.resBlock1(downCntx)\n",
    "        down1c, down1b = self.resBlock2(down0c)\n",
    "        down2c, down2b = self.resBlock3(down1c)\n",
    "        down3c, down3b = self.resBlock4(down2c)\n",
    "        down4c, down4b = self.resBlock5(down3c)\n",
    "        down5b = self.resBlock6(down4c)\n",
    "\n",
    "        up4e = self.upBlock1(down5b, down4b)\n",
    "        up3e = self.upBlock2(up4e, down3b)\n",
    "        up2e = self.upBlock3(up3e, down2b)\n",
    "        up1e = self.upBlock4(up2e, down1b)\n",
    "        up0e = self.upBlock5(up1e, down0b)\n",
    "\n",
    "        logits = self.logits(up0e)\n",
    "        logits = F.softmax(logits, dim=1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lovasz_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tasks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-62a80b2f0e22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msemantic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegmentator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tasks'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2018 Maxim Berman\n",
    "Copyright (c) 2020 Tiago Cortinhal, George Tzelepis and Eren Erdal Aksoy\n",
    "\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def isnan(x):\n",
    "    return x != x\n",
    "\n",
    "\n",
    "def mean(l, ignore_nan=False, empty=0):\n",
    "    \"\"\"\n",
    "    nanmean compatible with generators.\n",
    "    \"\"\"\n",
    "    l = iter(l)\n",
    "    if ignore_nan:\n",
    "        l = ifilterfalse(isnan, l)\n",
    "    try:\n",
    "        n = 1\n",
    "        acc = next(l)\n",
    "    except StopIteration:\n",
    "        if empty == 'raise':\n",
    "            raise ValueError('Empty mean')\n",
    "        return empty\n",
    "    for n, v in enumerate(l, 2):\n",
    "        acc += v\n",
    "    if n == 1:\n",
    "        return acc\n",
    "    return acc / n\n",
    "\n",
    "\n",
    "def lovasz_grad(gt_sorted):\n",
    "    \"\"\"\n",
    "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
    "    See Alg. 1 in paper\n",
    "    \"\"\"\n",
    "    p = len(gt_sorted)\n",
    "    gts = gt_sorted.sum()\n",
    "    intersection = gts - gt_sorted.float().cumsum(0)\n",
    "    union = gts + (1 - gt_sorted).float().cumsum(0)\n",
    "    jaccard = 1. - intersection / union\n",
    "    if p > 1:  # cover 1-pixel case\n",
    "        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n",
    "    return jaccard\n",
    "\n",
    "\n",
    "def lovasz_softmax(probas, labels, classes='present', per_image=False, ignore=None):\n",
    "    \"\"\"\n",
    "    Multi-class Lovasz-Softmax loss\n",
    "      probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1).\n",
    "              Interpreted as binary (sigmoid) output with outputs of size [B, H, W].\n",
    "      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n",
    "      classes: 'all' for all, 'present' for classes present in labels, or a list of classes to average.\n",
    "      per_image: compute the loss per image instead of per batch\n",
    "      ignore: void class labels\n",
    "    \"\"\"\n",
    "    if per_image:\n",
    "        loss = mean(lovasz_softmax_flat(*flatten_probas(prob.unsqueeze(0), lab.unsqueeze(0), ignore), classes=classes)\n",
    "                    for prob, lab in zip(probas, labels))\n",
    "    else:\n",
    "        loss = lovasz_softmax_flat(*flatten_probas(probas, labels, ignore), classes=classes)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lovasz_softmax_flat(probas, labels, classes='present'):\n",
    "    \"\"\"\n",
    "    Multi-class Lovasz-Softmax loss\n",
    "      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n",
    "      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n",
    "      classes: 'all' for all, 'present' for classes present in labels, or a list of classes to average.\n",
    "    \"\"\"\n",
    "    if probas.numel() == 0:\n",
    "        # only void pixels, the gradients should be 0\n",
    "        return probas * 0.\n",
    "    C = probas.size(1)\n",
    "    losses = []\n",
    "    class_to_sum = list(range(C)) if classes in ['all', 'present'] else classes\n",
    "    for c in class_to_sum:\n",
    "        fg = (labels == c).float()  # foreground for class c\n",
    "        if (classes is 'present' and fg.sum() == 0):\n",
    "            continue\n",
    "        if C == 1:\n",
    "            if len(classes) > 1:\n",
    "                raise ValueError('Sigmoid output possible only with 1 class')\n",
    "            class_pred = probas[:, 0]\n",
    "        else:\n",
    "            class_pred = probas[:, c]\n",
    "        errors = (Variable(fg) - class_pred).abs()\n",
    "        errors_sorted, perm = torch.sort(errors, 0, descending=True)\n",
    "        perm = perm.data\n",
    "        fg_sorted = fg[perm]\n",
    "        losses.append(torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted))))\n",
    "    return mean(losses)\n",
    "\n",
    "\n",
    "def flatten_probas(probas, labels, ignore=None):\n",
    "    \"\"\"\n",
    "    Flattens predictions in the batch\n",
    "    \"\"\"\n",
    "    if probas.dim() == 3:\n",
    "        # assumes output of a sigmoid layer\n",
    "        B, H, W = probas.size()\n",
    "        probas = probas.view(B, 1, H, W)\n",
    "    B, C, H, W = probas.size()\n",
    "    probas = probas.permute(0, 2, 3, 1).contiguous().view(-1, C)  # B * H * W, C = P, C\n",
    "    labels = labels.view(-1)\n",
    "    if ignore is None:\n",
    "        return probas, labels\n",
    "    valid = (labels != ignore)\n",
    "    vprobas = probas[valid.nonzero().squeeze()]\n",
    "    vlabels = labels[valid]\n",
    "    return vprobas, vlabels\n",
    "\n",
    "\n",
    "class Lovasz_softmax(nn.Module):\n",
    "    def __init__(self, classes='present', per_image=False, ignore=None):\n",
    "        super(Lovasz_softmax, self).__init__()\n",
    "        self.classes = classes\n",
    "        self.per_image = per_image\n",
    "        self.ignore = ignore\n",
    "\n",
    "    def forward(self, probas, labels):\n",
    "        return lovasz_softmax(probas, labels, self.classes, self.per_image, self.ignore)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heteroscedastic_loss(true, mean, log_var):\n",
    "    precision = torch.exp(-log_var)\n",
    "    sum = torch.sum(precision * (true - mean)**2 + log_var, 1)\n",
    "    mean = torch.mean(sum, 0)\n",
    "    return mean\n",
    "\n",
    "\n",
    "def save_to_log(logdir, logfile, message):\n",
    "    f = open(logdir + '/' + logfile, \"a\")\n",
    "    f.write(message + '\\n')\n",
    "    f.close()\n",
    "    return\n",
    "\n",
    "\n",
    "def save_checkpoint(to_save, logdir, suffix=\"\"):\n",
    "    # Save the weights\n",
    "    torch.save(to_save, logdir +\n",
    "               \"/SalsaNet\" + suffix)\n",
    "\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, ARCH, DATA, datadir, logdir, path=None, model_mode='salsanext'):\n",
    "        # parameters\n",
    "        self.ARCH = ARCH\n",
    "        self.DATA = DATA\n",
    "        self.datadir = datadir\n",
    "        self.log = logdir\n",
    "        self.path = path\n",
    "        self.model_mode = model_mode\n",
    "\n",
    "        self.batch_time_t = AverageMeter()\n",
    "        self.data_time_t = AverageMeter()\n",
    "        self.batch_time_e = AverageMeter()\n",
    "        self.epoch = 0\n",
    "\n",
    "        # put logger where it belongs\n",
    "\n",
    "        self.info = {\"train_update\": 0,\n",
    "                     \"train_loss\": 0,\n",
    "                     \"train_acc\": 0,\n",
    "                     \"train_iou\": 0,\n",
    "                     \"valid_loss\": 0,\n",
    "                     \"valid_acc\": 0,\n",
    "                     \"valid_iou\": 0,\n",
    "                     \"best_train_iou\": 0,\n",
    "                     \"best_val_iou\": 0}\n",
    "\n",
    "        # get the data\n",
    "        parserModule = imp.load_source(\"parserModule\",\n",
    "                                       TRAIN_PATH + '/tasks/semantic/dataset/' +\n",
    "                                       self.DATA[\"name\"] + '/parser.py')\n",
    "        self.parser = parserModule.Parser(root=self.datadir,\n",
    "                                          train_sequences=self.DATA[\"split\"][\"train\"],\n",
    "                                          valid_sequences=self.DATA[\"split\"][\"valid\"],\n",
    "                                          test_sequences=None,\n",
    "                                          labels=self.DATA[\"labels\"],\n",
    "                                          color_map=self.DATA[\"color_map\"],\n",
    "                                          learning_map=self.DATA[\"learning_map\"],\n",
    "                                          learning_map_inv=self.DATA[\"learning_map_inv\"],\n",
    "                                          sensor=self.ARCH[\"dataset\"][\"sensor\"],\n",
    "                                          max_points=self.ARCH[\"dataset\"][\"max_points\"],\n",
    "                                          batch_size=self.ARCH[\"train\"][\"batch_size\"],\n",
    "                                          workers=self.ARCH[\"train\"][\"workers\"],\n",
    "                                          gt=True,\n",
    "                                          shuffle_train=True)\n",
    "\n",
    "        # weights for loss (and bias)\n",
    "        # weights for loss (and bias)\n",
    "        epsilon_w = self.ARCH[\"train\"][\"epsilon_w\"]\n",
    "        content = torch.zeros(self.parser.get_n_classes(), dtype=torch.float)\n",
    "        for cl, freq in DATA[\"content\"].items():\n",
    "            x_cl = self.parser.to_xentropy(cl)  # map actual class to xentropy class\n",
    "            content[x_cl] += freq\n",
    "        self.loss_w = 1 / (content + epsilon_w)  # get weights\n",
    "        for x_cl, w in enumerate(self.loss_w):  # ignore the ones necessary to ignore\n",
    "            if DATA[\"learning_ignore\"][x_cl]:\n",
    "                # don't weigh\n",
    "                self.loss_w[x_cl] = 0\n",
    "        print(\"Loss weights from content: \", self.loss_w.data)\n",
    "        # concatenate the encoder and the head\n",
    "        with torch.no_grad():\n",
    "            self.model = SalsaNet(self.ARCH,\n",
    "                                  self.parser.get_n_classes(),\n",
    "                                  self.path)\n",
    "\n",
    "        self.tb_logger = Logger(self.log + \"/tb\", self.model)\n",
    "\n",
    "        # GPU?\n",
    "        self.gpu = False\n",
    "        self.multi_gpu = False\n",
    "        self.n_gpus = 0\n",
    "        self.model_single = self.model\n",
    "        pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(\"{}: {:,}\".format(name, param.numel()))\n",
    "        print(\"Total of Trainable Parameters: {:,}\".format(pytorch_total_params))\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"Training in device: \", self.device)\n",
    "        if torch.cuda.is_available() and torch.cuda.device_count() > 0:\n",
    "            cudnn.benchmark = True\n",
    "            cudnn.fastest = True\n",
    "            self.gpu = True\n",
    "            self.n_gpus = 1\n",
    "            self.model.cuda()\n",
    "        if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "            print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "            self.model = nn.DataParallel(self.model)  # spread in gpus\n",
    "            self.model = convert_model(self.model).cuda()  # sync batchnorm\n",
    "            self.model_single = self.model.module  # single model to get weight names\n",
    "            self.multi_gpu = True\n",
    "            self.n_gpus = torch.cuda.device_count()\n",
    "\n",
    "\n",
    "        self.criterion = nn.NLLLoss(weight=self.loss_w).to(self.device)\n",
    "        self.ls = Lovasz_softmax(ignore=0).to(self.device)\n",
    "        # loss as dataparallel too (more images in batch)\n",
    "        if self.n_gpus > 1:\n",
    "            self.criterion = nn.DataParallel(self.criterion).cuda()  # spread in gpus\n",
    "            self.ls = nn.DataParallel(self.ls).cuda()\n",
    "        self.optimizer = optim.SGD([{'params': self.model.parameters()}],\n",
    "                                   lr=self.ARCH[\"train\"][\"lr\"],\n",
    "                                   momentum=self.ARCH[\"train\"][\"momentum\"],\n",
    "                                   weight_decay=self.ARCH[\"train\"][\"w_decay\"])\n",
    "\n",
    "        # Use warmup learning rate\n",
    "        # post decay and step sizes come in epochs and we want it in steps\n",
    "        steps_per_epoch = self.parser.get_train_size()\n",
    "        up_steps = int(self.ARCH[\"train\"][\"wup_epochs\"] * steps_per_epoch)\n",
    "        final_decay = self.ARCH[\"train\"][\"lr_decay\"] ** (1 / steps_per_epoch)\n",
    "        self.scheduler = warmupLR(optimizer=self.optimizer,\n",
    "                                  lr=self.ARCH[\"train\"][\"lr\"],\n",
    "                                  warmup_steps=up_steps,\n",
    "                                  momentum=self.ARCH[\"train\"][\"momentum\"],\n",
    "                                  decay=final_decay)\n",
    "\n",
    "        if self.path is not None:\n",
    "            torch.nn.Module.dump_patches = True\n",
    "            w_dict = torch.load(path + \"/SalsaNet\",\n",
    "                                map_location=lambda storage, loc: storage)\n",
    "            self.model.load_state_dict(w_dict['state_dict'], strict=True)\n",
    "            self.optimizer.load_state_dict(w_dict['optimizer'])\n",
    "            self.epoch = w_dict['epoch'] + 1\n",
    "            self.scheduler.load_state_dict(w_dict['scheduler'])\n",
    "            print(\"dict epoch:\", w_dict['epoch'])\n",
    "            self.info = w_dict['info']\n",
    "            print(\"info\", w_dict['info'])\n",
    "\n",
    "    def calculate_estimate(self, epoch, iter):\n",
    "        estimate = int((self.data_time_t.avg + self.batch_time_t.avg) * \\\n",
    "                       (self.parser.get_train_size() * self.ARCH['train']['max_epochs'] - (\n",
    "                               iter + 1 + epoch * self.parser.get_train_size()))) + \\\n",
    "                   int(self.batch_time_e.avg * self.parser.get_valid_size() * (\n",
    "                           self.ARCH['train']['max_epochs'] - (epoch)))\n",
    "        return str(datetime.timedelta(seconds=estimate))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_mpl_colormap(cmap_name):\n",
    "        cmap = plt.get_cmap(cmap_name)\n",
    "        # Initialize the matplotlib color map\n",
    "        sm = plt.cm.ScalarMappable(cmap=cmap)\n",
    "        # Obtain linear color range\n",
    "        color_range = sm.to_rgba(np.linspace(0, 1, 256), bytes=True)[:, 2::-1]\n",
    "        return color_range.reshape(256, 1, 3)\n",
    "\n",
    "    @staticmethod\n",
    "    def make_log_img(depth, mask, pred, gt, color_fn):\n",
    "        # input should be [depth, pred, gt]\n",
    "        # make range image (normalized to 0,1 for saving)\n",
    "        depth = (cv2.normalize(depth, None, alpha=0, beta=1,\n",
    "                               norm_type=cv2.NORM_MINMAX,\n",
    "                               dtype=cv2.CV_32F) * 255.0).astype(np.uint8)\n",
    "        out_img = cv2.applyColorMap(\n",
    "            depth, Trainer.get_mpl_colormap('viridis')) * mask[..., None]\n",
    "        # make label prediction\n",
    "        pred_color = color_fn((pred * mask).astype(np.int32))\n",
    "        out_img = np.concatenate([out_img, pred_color], axis=0)\n",
    "        # make label gt\n",
    "        gt_color = color_fn(gt)\n",
    "        out_img = np.concatenate([out_img, gt_color], axis=0)\n",
    "        return (out_img).astype(np.uint8)\n",
    "\n",
    "    @staticmethod\n",
    "    def save_to_log(logdir, logger, info, epoch, w_summary=False, model=None, img_summary=False, imgs=[]):\n",
    "        # save scalars\n",
    "        for tag, value in info.items():\n",
    "            logger.scalar_summary(tag, value, epoch)\n",
    "\n",
    "        # save summaries of weights and biases\n",
    "        if w_summary and model:\n",
    "            for tag, value in model.named_parameters():\n",
    "                tag = tag.replace('.', '/')\n",
    "                logger.histo_summary(tag, value.data.cpu().numpy(), epoch)\n",
    "                if value.grad is not None:\n",
    "                    logger.histo_summary(\n",
    "                        tag + '/grad', value.grad.data.cpu().numpy(), epoch)\n",
    "\n",
    "        if img_summary and len(imgs) > 0:\n",
    "            directory = os.path.join(logdir, \"predictions\")\n",
    "            if not os.path.isdir(directory):\n",
    "                os.makedirs(directory)\n",
    "            for i, img in enumerate(imgs):\n",
    "                name = os.path.join(directory, str(i) + \".png\")\n",
    "                cv2.imwrite(name, img)\n",
    "\n",
    "    def train_epoch(self, train_loader, model, criterion, optimizer, epoch,\n",
    "                    evaluator, scheduler, color_fn, report=10, show_scans=False):\n",
    "        losses = AverageMeter()\n",
    "        acc = AverageMeter()\n",
    "        iou = AverageMeter()\n",
    "        update_ratio_meter = AverageMeter()\n",
    "\n",
    "        # empty the cache to train now\n",
    "        if self.gpu:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # switch to train mode\n",
    "        model.train()\n",
    "\n",
    "        end = time.time()\n",
    "        for i, (in_vol, proj_mask, proj_labels, _, path_seq, path_name, _, _, _, _, _, _, _, _, _) in enumerate(train_loader):\n",
    "            # measure data loading time\n",
    "            self.data_time_t.update(time.time() - end)\n",
    "            if not self.multi_gpu and self.gpu:\n",
    "                in_vol = in_vol.cuda()\n",
    "                # proj_mask = proj_mask.cuda()\n",
    "            if self.gpu:\n",
    "                proj_labels = proj_labels.cuda().long()\n",
    "\n",
    "            # compute output\n",
    "            output = model(in_vol)\n",
    "            loss = criterion(torch.log(output.clamp(min=1e-8)), proj_labels) + self.ls(output, proj_labels.long())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            if self.n_gpus > 1:\n",
    "                idx = torch.ones(self.n_gpus).cuda()\n",
    "                loss.backward(idx)\n",
    "            else:\n",
    "                loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            loss = loss.mean()\n",
    "            with torch.no_grad():\n",
    "                evaluator.reset()\n",
    "                argmax = output.argmax(dim=1)\n",
    "                evaluator.addBatch(argmax, proj_labels)\n",
    "                accuracy = evaluator.getacc()\n",
    "                jaccard, class_jaccard = evaluator.getIoU()\n",
    "\n",
    "            losses.update(loss.item(), in_vol.size(0))\n",
    "            acc.update(accuracy.item(), in_vol.size(0))\n",
    "            iou.update(jaccard.item(), in_vol.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            self.batch_time_t.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            # get gradient updates and weights, so I can print the relationship of\n",
    "            # their norms\n",
    "            update_ratios = []\n",
    "            for g in self.optimizer.param_groups:\n",
    "                lr = g[\"lr\"]\n",
    "                for value in g[\"params\"]:\n",
    "                    if value.grad is not None:\n",
    "                        w = np.linalg.norm(value.data.cpu().numpy().reshape((-1)))\n",
    "                        update = np.linalg.norm(-max(lr, 1e-10) *\n",
    "                                                value.grad.cpu().numpy().reshape((-1)))\n",
    "                        update_ratios.append(update / max(w, 1e-10))\n",
    "            update_ratios = np.array(update_ratios)\n",
    "            update_mean = update_ratios.mean()\n",
    "            update_std = update_ratios.std()\n",
    "            update_ratio_meter.update(update_mean)  # over the epoch\n",
    "\n",
    "            if show_scans:\n",
    "                # get the first scan in batch and project points\n",
    "                mask_np = proj_mask[0].cpu().numpy()\n",
    "                depth_np = in_vol[0][0].cpu().numpy()\n",
    "                pred_np = argmax[0].cpu().numpy()\n",
    "                gt_np = proj_labels[0].cpu().numpy()\n",
    "                out = Trainer.make_log_img(depth_np, mask_np, pred_np, gt_np, color_fn)\n",
    "\n",
    "                mask_np = proj_mask[1].cpu().numpy()\n",
    "                depth_np = in_vol[1][0].cpu().numpy()\n",
    "                pred_np = argmax[1].cpu().numpy()\n",
    "                gt_np = proj_labels[1].cpu().numpy()\n",
    "                out2 = Trainer.make_log_img(depth_np, mask_np, pred_np, gt_np, color_fn)\n",
    "\n",
    "                out = np.concatenate([out, out2], axis=0)\n",
    "                cv2.imshow(\"sample_training\", out)\n",
    "                cv2.waitKey(1)\n",
    "\n",
    "            if i % self.ARCH[\"train\"][\"report_batch\"] == 0:\n",
    "                print('Lr: {lr:.3e} | '\n",
    "                      'Update: {umean:.3e} mean,{ustd:.3e} std | '\n",
    "                      'Epoch: [{0}][{1}/{2}] | '\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f}) | '\n",
    "                      'Data {data_time.val:.3f} ({data_time.avg:.3f}) | '\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f}) | '\n",
    "                      'acc {acc.val:.3f} ({acc.avg:.3f}) | '\n",
    "                      'IoU {iou.val:.3f} ({iou.avg:.3f}) | [{estim}]'.format(\n",
    "                    epoch, i, len(train_loader), batch_time=self.batch_time_t,\n",
    "                    data_time=self.data_time_t, loss=losses, acc=acc, iou=iou, lr=lr,\n",
    "                    umean=update_mean, ustd=update_std, estim=self.calculate_estimate(epoch, i)))\n",
    "\n",
    "                save_to_log(self.log, 'log.txt', 'Lr: {lr:.3e} | '\n",
    "                                      'Update: {umean:.3e} mean,{ustd:.3e} std | '\n",
    "                                      'Epoch: [{0}][{1}/{2}] | '\n",
    "                                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f}) | '\n",
    "                                      'Data {data_time.val:.3f} ({data_time.avg:.3f}) | '\n",
    "                                      'Loss {loss.val:.4f} ({loss.avg:.4f}) | '\n",
    "                                      'acc {acc.val:.3f} ({acc.avg:.3f}) | '\n",
    "                                      'IoU {iou.val:.3f} ({iou.avg:.3f}) | [{estim}]'.format(\n",
    "                                    epoch, i, len(train_loader), batch_time=self.batch_time_t,\n",
    "                                    data_time=self.data_time_t, loss=losses, acc=acc, iou=iou, lr=lr,\n",
    "                                    umean=update_mean, ustd=update_std, estim=self.calculate_estimate(epoch, i)))\n",
    "\n",
    "            # step scheduler\n",
    "            scheduler.step()\n",
    "\n",
    "        return acc.avg, iou.avg, losses.avg, update_ratio_meter.avg\n",
    "\n",
    "    def validate(self, val_loader, model, criterion, evaluator, class_func, color_fn, save_scans):\n",
    "        losses = AverageMeter()\n",
    "        jaccs = AverageMeter()\n",
    "        wces = AverageMeter()\n",
    "        acc = AverageMeter()\n",
    "        iou = AverageMeter()\n",
    "        rand_imgs = []\n",
    "\n",
    "        # switch to evaluate mode\n",
    "        model.eval()\n",
    "        evaluator.reset()\n",
    "\n",
    "        # empty the cache to infer in high res\n",
    "        if self.gpu:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            end = time.time()\n",
    "            for i, (in_vol, proj_mask, proj_labels, _, path_seq, path_name, _, _, _, _, _, _, _, _, _) in enumerate(val_loader):\n",
    "                if not self.multi_gpu and self.gpu:\n",
    "                    in_vol = in_vol.cuda()\n",
    "                    proj_mask = proj_mask.cuda()\n",
    "                if self.gpu:\n",
    "                    proj_labels = proj_labels.cuda(non_blocking=True).long()\n",
    "\n",
    "                # compute output\n",
    "                output = model(in_vol)\n",
    "                log_out = torch.log(output.clamp(min=1e-8))\n",
    "                jacc = self.ls(output, proj_labels)\n",
    "                wce = criterion(log_out, proj_labels)\n",
    "                loss = wce + jacc\n",
    "\n",
    "                # measure accuracy and record loss\n",
    "                argmax = output.argmax(dim=1)\n",
    "                evaluator.addBatch(argmax, proj_labels)\n",
    "                losses.update(loss.mean().item(), in_vol.size(0))\n",
    "                jaccs.update(jacc.mean().item(),in_vol.size(0))\n",
    "                wces.update(wce.mean().item(),in_vol.size(0))\n",
    "\n",
    "                if save_scans:\n",
    "                    # get the first scan in batch and project points\n",
    "                    mask_np = proj_mask[0].cpu().numpy()\n",
    "                    depth_np = in_vol[0][0].cpu().numpy()\n",
    "                    pred_np = argmax[0].cpu().numpy()\n",
    "                    gt_np = proj_labels[0].cpu().numpy()\n",
    "                    out = Trainer.make_log_img(depth_np,\n",
    "                                               mask_np,\n",
    "                                               pred_np,\n",
    "                                               gt_np,\n",
    "                                               color_fn)\n",
    "                    rand_imgs.append(out)\n",
    "\n",
    "                # measure elapsed time\n",
    "                self.batch_time_e.update(time.time() - end)\n",
    "                end = time.time()\n",
    "\n",
    "            accuracy = evaluator.getacc()\n",
    "            jaccard, class_jaccard = evaluator.getIoU()\n",
    "            acc.update(accuracy.item(), in_vol.size(0))\n",
    "            iou.update(jaccard.item(), in_vol.size(0))\n",
    "\n",
    "            print('Validation set:\\n'\n",
    "                  'Time avg per batch {batch_time.avg:.3f}\\n'\n",
    "                  'Loss avg {loss.avg:.4f}\\n'\n",
    "                  'Jaccard avg {jac.avg:.4f}\\n'\n",
    "                  'WCE avg {wces.avg:.4f}\\n'\n",
    "                  'Acc avg {acc.avg:.3f}\\n'\n",
    "                  'IoU avg {iou.avg:.3f}'.format(batch_time=self.batch_time_e,\n",
    "                                                 loss=losses,\n",
    "                                                 jac=jaccs,\n",
    "                                                 wces=wces,\n",
    "                                                 acc=acc,\n",
    "                                                 iou=iou))\n",
    "\n",
    "            save_to_log(self.log, 'log.txt', 'Validation set:\\n'\n",
    "                                             'Time avg per batch {batch_time.avg:.3f}\\n'\n",
    "                                             'Loss avg {loss.avg:.4f}\\n'\n",
    "                                             'Jaccard avg {jac.avg:.4f}\\n'\n",
    "                                             'WCE avg {wces.avg:.4f}\\n'\n",
    "                                             'Acc avg {acc.avg:.3f}\\n'\n",
    "                                             'IoU avg {iou.avg:.3f}'.format(batch_time=self.batch_time_e,\n",
    "                                                                            loss=losses,\n",
    "                                                                            jac=jaccs,\n",
    "                                                                            wces=wces,\n",
    "                                                                            acc=acc,\n",
    "                                                                            iou=iou))\n",
    "            # print also classwise\n",
    "            for i, jacc in enumerate(class_jaccard):\n",
    "                print('IoU class {i:} [{class_str:}] = {jacc:.3f}'.format(i=i, class_str=class_func(i), jacc=jacc))\n",
    "                save_to_log(self.log, 'log.txt', 'IoU class {i:} [{class_str:}] = {jacc:.3f}'.format(i=i, class_str=class_func(i), jacc=jacc))\n",
    "\n",
    "        return acc.avg, iou.avg, losses.avg, rand_imgs\n",
    "\n",
    "    def train(self):\n",
    "        self.ignore_class = []\n",
    "        for i, w in enumerate(self.loss_w):\n",
    "            if w < 1e-10:\n",
    "                self.ignore_class.append(i)\n",
    "                print(\"Ignoring class \", i, \" in IoU evaluation\")\n",
    "\n",
    "        self.evaluator = iouEval(self.parser.get_n_classes(),\n",
    "                                 self.device, self.ignore_class)\n",
    "\n",
    "        # train for n epochs\n",
    "        for epoch in range(self.epoch, self.ARCH[\"train\"][\"max_epochs\"]):\n",
    "            # get info for learn rate currently\n",
    "            # groups = self.optimizer.param_groups()\n",
    "            # for name, g in zip(self.lr_group_names, groups):\n",
    "            #     self.info[name] = g['lr']\n",
    "\n",
    "            # train for 1 epoch\n",
    "            acc, iou, loss, update_mean = self.train_epoch(train_loader=self.parser.get_train_set(),\n",
    "                                                           model=self.model,\n",
    "                                                           criterion=self.criterion,\n",
    "                                                           optimizer=self.optimizer,\n",
    "                                                           epoch=epoch,\n",
    "                                                           evaluator=self.evaluator,\n",
    "                                                           scheduler=self.scheduler,\n",
    "                                                           color_fn=self.parser.to_color,\n",
    "                                                           report=self.ARCH[\"train\"][\"report_batch\"],\n",
    "                                                           show_scans=self.ARCH[\"train\"][\"show_scans\"])\n",
    "\n",
    "            # update info\n",
    "            self.info[\"train_update\"] = update_mean\n",
    "            self.info[\"train_loss\"] = loss\n",
    "            self.info[\"train_acc\"] = acc\n",
    "            self.info[\"train_iou\"] = iou\n",
    "\n",
    "            # remember best iou and save checkpoint\n",
    "            state = {'epoch': epoch, 'state_dict': self.model.state_dict(),\n",
    "                     'optimizer': self.optimizer.state_dict(),\n",
    "                     'info': self.info,\n",
    "                     'scheduler': self.scheduler.state_dict()\n",
    "                     }\n",
    "            save_checkpoint(state, self.log, suffix=\"\")\n",
    "\n",
    "            if self.info['train_iou'] > self.info['best_train_iou']:\n",
    "                print(\"Best mean iou in training set so far, save model!\")\n",
    "                self.info['best_train_iou'] = self.info['train_iou']\n",
    "                state = {'epoch': epoch, 'state_dict': self.model.state_dict(),\n",
    "                         'optimizer': self.optimizer.state_dict(),\n",
    "                         'info': self.info,\n",
    "                         'scheduler': self.scheduler.state_dict()\n",
    "                         }\n",
    "                save_checkpoint(state, self.log, suffix=\"_train_best\")\n",
    "\n",
    "            if epoch % self.ARCH[\"train\"][\"report_epoch\"] == 0:\n",
    "                # evaluate on validation set\n",
    "                print(\"*\" * 80)\n",
    "                acc, iou, loss, rand_img = self.validate(val_loader=self.parser.get_valid_set(),\n",
    "                                                         model=self.model,\n",
    "                                                         criterion=self.criterion,\n",
    "                                                         evaluator=self.evaluator,\n",
    "                                                         class_func=self.parser.get_xentropy_class_string,\n",
    "                                                         color_fn=self.parser.to_color,\n",
    "                                                         save_scans=self.ARCH[\"train\"][\"save_scans\"])\n",
    "\n",
    "                # update info\n",
    "                self.info[\"valid_loss\"] = loss\n",
    "                self.info[\"valid_acc\"] = acc\n",
    "                self.info[\"valid_iou\"] = iou\n",
    "\n",
    "            # remember best iou and save checkpoint\n",
    "            if self.info['valid_iou'] > self.info['best_val_iou']:\n",
    "                print(\"Best mean iou in validation so far, save model!\")\n",
    "                print(\"*\" * 80)\n",
    "                self.info['best_val_iou'] = self.info['valid_iou']\n",
    "\n",
    "                # save the weights!\n",
    "                state = {'epoch': epoch, 'state_dict': self.model.state_dict(),\n",
    "                         'optimizer': self.optimizer.state_dict(),\n",
    "                         'info': self.info,\n",
    "                         'scheduler': self.scheduler.state_dict()\n",
    "                         }\n",
    "                save_checkpoint(state, self.log, suffix=\"_valid_best\")\n",
    "\n",
    "            print(\"*\" * 80)\n",
    "\n",
    "            # save to log\n",
    "            Trainer.save_to_log(logdir=self.log,\n",
    "                                logger=self.tb_logger,\n",
    "                                info=self.info,\n",
    "                                epoch=epoch,\n",
    "                                w_summary=self.ARCH[\"train\"][\"save_summary\"],\n",
    "                                model=self.model_single,\n",
    "                                img_summary=self.ARCH[\"train\"][\"save_scans\"],\n",
    "                                imgs=rand_img)\n",
    "\n",
    "        print('Finished Training')\n",
    "\n",
    "        return\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cfg = {\n",
    " 'color_map': {0: [0, 0, 0],\n",
    "               1: [0, 0, 255],\n",
    "               10: [245, 150, 100],\n",
    "               11: [245, 230, 100],\n",
    "               13: [250, 80, 100],\n",
    "               15: [150, 60, 30],\n",
    "               16: [255, 0, 0],\n",
    "               18: [180, 30, 80],\n",
    "               20: [255, 0, 0],\n",
    "               30: [30, 30, 255],\n",
    "               31: [200, 40, 255],\n",
    "               32: [90, 30, 150],\n",
    "               40: [255, 0, 255],\n",
    "               44: [255, 150, 255],\n",
    "               48: [75, 0, 75],\n",
    "               49: [75, 0, 175],\n",
    "               50: [0, 200, 255],\n",
    "               51: [50, 120, 255],\n",
    "               52: [0, 150, 255],\n",
    "               60: [170, 255, 150],\n",
    "               70: [0, 175, 0],\n",
    "               71: [0, 60, 135],\n",
    "               72: [80, 240, 150],\n",
    "               80: [150, 240, 255],\n",
    "               81: [0, 0, 255],\n",
    "               99: [255, 255, 50],\n",
    "               252: [245, 150, 100],\n",
    "               253: [200, 40, 255],\n",
    "               254: [30, 30, 255],\n",
    "               255: [90, 30, 150],\n",
    "               256: [255, 0, 0],\n",
    "               257: [250, 80, 100],\n",
    "               258: [180, 30, 80],\n",
    "               259: [255, 0, 0]},\n",
    " 'content': {0: 0.018889854628292943,\n",
    "             1: 0.0002937197336781505,\n",
    "             10: 0.040818519255974316,\n",
    "             11: 0.00016609538710764618,\n",
    "             13: 2.7879693665067774e-05,\n",
    "             15: 0.00039838616015114444,\n",
    "             16: 0.0,\n",
    "             18: 0.0020633612104619787,\n",
    "             20: 0.0016218197275284021,\n",
    "             30: 0.00017698551338515307,\n",
    "             31: 1.1065903904919655e-08,\n",
    "             32: 5.532951952459828e-09,\n",
    "             40: 0.1987493871255525,\n",
    "             44: 0.014717169549888214,\n",
    "             48: 0.14392298360372,\n",
    "             49: 0.0039048553037472045,\n",
    "             50: 0.1326861944777486,\n",
    "             51: 0.0723592229456223,\n",
    "             52: 0.002395131480328884,\n",
    "             60: 4.7084144280367186e-05,\n",
    "             70: 0.26681502148037506,\n",
    "             71: 0.006035012012626033,\n",
    "             72: 0.07814222006271769,\n",
    "             80: 0.002855498193863172,\n",
    "             81: 0.0006155958086189918,\n",
    "             99: 0.009923127583046915,\n",
    "             252: 0.001789309418528068,\n",
    "             253: 0.00012709999297008662,\n",
    "             254: 0.00016059776092534436,\n",
    "             255: 3.745553104802113e-05,\n",
    "             256: 0.0,\n",
    "             257: 0.00011351574470342043,\n",
    "             258: 0.00010157861367183268,\n",
    "             259: 4.3840131989471124e-05},\n",
    " 'labels': {0: 'unlabeled',\n",
    "            1: 'outlier',\n",
    "            10: 'car',\n",
    "            11: 'bicycle',\n",
    "            13: 'bus',\n",
    "            15: 'motorcycle',\n",
    "            16: 'on-rails',\n",
    "            18: 'truck',\n",
    "            20: 'other-vehicle',\n",
    "            30: 'person',\n",
    "            31: 'bicyclist',\n",
    "            32: 'motorcyclist',\n",
    "            40: 'road',\n",
    "            44: 'parking',\n",
    "            48: 'sidewalk',\n",
    "            49: 'other-ground',\n",
    "            50: 'building',\n",
    "            51: 'fence',\n",
    "            52: 'other-structure',\n",
    "            60: 'lane-marking',\n",
    "            70: 'vegetation',\n",
    "            71: 'trunk',\n",
    "            72: 'terrain',\n",
    "            80: 'pole',\n",
    "            81: 'traffic-sign',\n",
    "            99: 'other-object',\n",
    "            252: 'moving-car',\n",
    "            253: 'moving-bicyclist',\n",
    "            254: 'moving-person',\n",
    "            255: 'moving-motorcyclist',\n",
    "            256: 'moving-on-rails',\n",
    "            257: 'moving-bus',\n",
    "            258: 'moving-truck',\n",
    "            259: 'moving-other-vehicle'},\n",
    " 'learning_ignore': {0: True,\n",
    "                     1: False,\n",
    "                     2: False,\n",
    "                     3: False,\n",
    "                     4: False,\n",
    "                     5: False,\n",
    "                     6: False,\n",
    "                     7: False,\n",
    "                     8: False,\n",
    "                     9: False,\n",
    "                     10: False,\n",
    "                     11: False,\n",
    "                     12: False,\n",
    "                     13: False,\n",
    "                     14: False,\n",
    "                     15: False,\n",
    "                     16: False,\n",
    "                     17: False,\n",
    "                     18: False,\n",
    "                     19: False},\n",
    " 'learning_map': {0: 0,\n",
    "                  1: 0,\n",
    "                  10: 1,\n",
    "                  11: 2,\n",
    "                  13: 5,\n",
    "                  15: 3,\n",
    "                  16: 5,\n",
    "                  18: 4,\n",
    "                  20: 5,\n",
    "                  30: 6,\n",
    "                  31: 7,\n",
    "                  32: 8,\n",
    "                  40: 9,\n",
    "                  44: 10,\n",
    "                  48: 11,\n",
    "                  49: 12,\n",
    "                  50: 13,\n",
    "                  51: 14,\n",
    "                  52: 0,\n",
    "                  60: 9,\n",
    "                  70: 15,\n",
    "                  71: 16,\n",
    "                  72: 17,\n",
    "                  80: 18,\n",
    "                  81: 19,\n",
    "                  99: 0,\n",
    "                  252: 1,\n",
    "                  253: 7,\n",
    "                  254: 6,\n",
    "                  255: 8,\n",
    "                  256: 5,\n",
    "                  257: 5,\n",
    "                  258: 4,\n",
    "                  259: 5},\n",
    " 'learning_map_inv': {0: 0,\n",
    "                      1: 10,\n",
    "                      2: 11,\n",
    "                      3: 15,\n",
    "                      4: 18,\n",
    "                      5: 20,\n",
    "                      6: 30,\n",
    "                      7: 31,\n",
    "                      8: 32,\n",
    "                      9: 40,\n",
    "                      10: 44,\n",
    "                      11: 48,\n",
    "                      12: 49,\n",
    "                      13: 50,\n",
    "                      14: 51,\n",
    "                      15: 70,\n",
    "                      16: 71,\n",
    "                      17: 72,\n",
    "                      18: 80,\n",
    "                      19: 81},\n",
    " 'name': 'kitti',\n",
    " 'split': {'test': [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21],\n",
    "           'train': [0, 1, 2, 3, 4, 5, 6, 7, 9, 10],\n",
    "           'valid': [8]}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### arch cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_cfg = {\n",
    " 'dataset': {'labels': 'kitti',\n",
    "             'max_points': 150000,\n",
    "             'scans': 'kitti',\n",
    "             \n",
    "             # KITTI\n",
    "             'sensor': {\n",
    "                 'fov_down': -25,\n",
    "                 'fov_up': 3,\n",
    "                 \n",
    "                 'img_means': [12.12, 10.88, 0.23, -1.04, 0.21], # range, x, y, z, signal\n",
    "                 'img_stds': [12.32, 11.47, 6.91, 0.86, 0.16], # range, x, y, z, signal\n",
    "                 \n",
    "                 'img_prop': {\n",
    "                     'height': 64,\n",
    "                     'width': 2048,\n",
    "                 },\n",
    "                 'name': 'HDL64',\n",
    "                 'type': 'spherical'\n",
    "             },\n",
    "             \n",
    "             # HUSKY\n",
    "#              'sensor': {\n",
    "#                  'fov_down': -30.67,\n",
    "#                  'fov_up': 10.67,\n",
    "                 \n",
    "#                  'img_means': [8.75550024, 0.07549276, -1.13823771, -0.13648431, 0.06386641], # range, x, y, z, signal\n",
    "#                  'img_stds': [10.08941738, 10.40510729, 8.21806914, 1.15425178, 0.07281147], # range, x, y, z, signal\n",
    "                 \n",
    "#                  'img_prop': {\n",
    "#                      'height': 32,\n",
    "#                      'width': 2048,\n",
    "# #                      TODO: scale?\n",
    "# #                      'width': 2169,\n",
    "#                  },\n",
    "#                  'name': 'HDL32',\n",
    "#                  'type': 'spherical'\n",
    "#              },\n",
    "             \n",
    " },\n",
    " 'post': {'CRF': {'params': False, 'train': True, 'use': False},\n",
    "          'KNN': {'params': {'cutoff': 1.0,\n",
    "                             'knn': 5,\n",
    "                             'search': 5,\n",
    "                             'sigma': 1.0},\n",
    "                  'use': True}},\n",
    " 'train': {'batch_size': 30,\n",
    "           'epsilon_w': 0.001,\n",
    "           'loss': 'xentropy',\n",
    "           'lr': 0.05,\n",
    "           'lr_decay': 0.99,\n",
    "           'max_epochs': 40,\n",
    "           'momentum': 0.9,\n",
    "           'report_batch': 10,\n",
    "           'report_epoch': 1,\n",
    "           'save_scans': True,\n",
    "           'save_summary': False,\n",
    "           'show_scans': False,\n",
    "           'w_decay': 0.0001,\n",
    "           'workers': 4,\n",
    "           'wup_epochs': 1}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir_path = '/datasets/KITTI_Odometry/dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'salsanext'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where to create \"logs\" folder to store model weights\n",
    "log_dir_path = '/home/crowbar/2-projects/SalsaNext/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(arch_cfg, data_cfg, dataset_dir_path, log_dir_path, pretrained, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
