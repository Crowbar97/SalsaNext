{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crowbar/.conda/envs/salsanext/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/crowbar/.conda/envs/salsanext/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/crowbar/.conda/envs/salsanext/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/crowbar/.conda/envs/salsanext/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/crowbar/.conda/envs/salsanext/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/crowbar/.conda/envs/salsanext/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/crowbar/.conda/envs/salsanext/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/crowbar/.conda/envs/salsanext/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/crowbar/.conda/envs/salsanext/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/crowbar/.conda/envs/salsanext/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/crowbar/.conda/envs/salsanext/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/crowbar/.conda/envs/salsanext/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "import time\n",
    "import imp\n",
    "from io import BytesIO\n",
    "import collections\n",
    "import queue\n",
    "import threading\n",
    "import functools\n",
    "\n",
    "try:\n",
    "    from itertools import ifilterfalse\n",
    "except ImportError:\n",
    "    from itertools import filterfalse as ifilterfalse\n",
    "\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.batchnorm import _BatchNorm\n",
    "from torch.nn.parallel._functions import ReduceAddCoalesced, Broadcast\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.optim.lr_scheduler as toptim\n",
    "from torch.nn.parallel.data_parallel import DataParallel\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### avgmeter.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logger.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "\n",
    "    def __init__(self, log_dir,model):\n",
    "        \"\"\"Create a summary writer logging to log_dir.\"\"\"\n",
    "        #self.writer = SummaryWriter(log_dir)\n",
    "        #self.writer.add_graph(model,torch.zeros(1,5,64,2048))\n",
    "        self.writer = tf.summary.FileWriter(log_dir)\n",
    "\n",
    "    def scalar_summary(self, tag, value, step):\n",
    "        \"\"\"Log a scalar variable.\"\"\"\n",
    "        summary = tf.Summary(\n",
    "            value=[tf.Summary.Value(tag=tag, simple_value=value)])\n",
    "        self.writer.add_summary(summary, step)\n",
    "        self.writer.flush()\n",
    "\n",
    "    def image_summary(self, tag, images, step):\n",
    "        \"\"\"Log a list of images.\"\"\"\n",
    "\n",
    "        img_summaries = []\n",
    "        for i, img in enumerate(images):\n",
    "            # Write the image to a string\n",
    "            try:\n",
    "                s = StringIO()\n",
    "            except:\n",
    "                s = BytesIO()\n",
    "            scipy.misc.toimage(img).save(s, format=\"png\")\n",
    "\n",
    "            # Create an Image object\n",
    "            img_sum = tf.Summary.Image(encoded_image_string=s.getvalue(),\n",
    "                                       height=img.shape[0],\n",
    "                                       width=img.shape[1])\n",
    "            # Create a Summary value\n",
    "            img_summaries.append(tf.Summary.Value(\n",
    "                tag='%s/%d' % (tag, i), image=img_sum))\n",
    "\n",
    "        # Create and write Summary\n",
    "        summary = tf.Summary(value=img_summaries)\n",
    "        self.writer.add_summary(summary, step)\n",
    "        self.writer.flush()\n",
    "\n",
    "    def histo_summary(self, tag, values, step, bins=1000):\n",
    "        \"\"\"Log a histogram of the tensor of values.\"\"\"\n",
    "\n",
    "        # Create a histogram using numpy\n",
    "        counts, bin_edges = np.histogram(values, bins=bins)\n",
    "\n",
    "        # Fill the fields of the histogram proto\n",
    "        hist = tf.HistogramProto()\n",
    "        hist.min = float(np.min(values))\n",
    "        hist.max = float(np.max(values))\n",
    "        hist.num = int(np.prod(values.shape))\n",
    "        hist.sum = float(np.sum(values))\n",
    "        hist.sum_squares = float(np.sum(values ** 2))\n",
    "\n",
    "        # Drop the start of the first bin\n",
    "        bin_edges = bin_edges[1:]\n",
    "\n",
    "        # Add bin edges and counts\n",
    "        for edge in bin_edges:\n",
    "            hist.bucket_limit.append(edge)\n",
    "        for c in counts:\n",
    "            hist.bucket.append(c)\n",
    "\n",
    "        # Create and write Summary\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, histo=hist)])\n",
    "        self.writer.add_summary(summary, step)\n",
    "        self.writer.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### comm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = ['FutureResult', 'SlavePipe', 'SyncMaster']\n",
    "\n",
    "\n",
    "class FutureResult(object):\n",
    "    \"\"\"A thread-safe future implementation. Used only as one-to-one pipe.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._result = None\n",
    "        self._lock = threading.Lock()\n",
    "        self._cond = threading.Condition(self._lock)\n",
    "\n",
    "    def put(self, result):\n",
    "        with self._lock:\n",
    "            assert self._result is None, 'Previous result has\\'t been fetched.'\n",
    "            self._result = result\n",
    "            self._cond.notify()\n",
    "\n",
    "    def get(self):\n",
    "        with self._lock:\n",
    "            if self._result is None:\n",
    "                self._cond.wait()\n",
    "\n",
    "            res = self._result\n",
    "            self._result = None\n",
    "            return res\n",
    "\n",
    "\n",
    "_MasterRegistry = collections.namedtuple('MasterRegistry', ['result'])\n",
    "_SlavePipeBase = collections.namedtuple(\n",
    "    '_SlavePipeBase', ['identifier', 'queue', 'result'])\n",
    "\n",
    "\n",
    "class SlavePipe(_SlavePipeBase):\n",
    "    \"\"\"Pipe for master-slave communication.\"\"\"\n",
    "\n",
    "    def run_slave(self, msg):\n",
    "        self.queue.put((self.identifier, msg))\n",
    "        ret = self.result.get()\n",
    "        self.queue.put(True)\n",
    "        return ret\n",
    "\n",
    "\n",
    "class SyncMaster(object):\n",
    "    \"\"\"An abstract `SyncMaster` object.\n",
    "\n",
    "    - During the replication, as the data parallel will trigger an callback of each module, all slave devices should\n",
    "    call `register(id)` and obtain an `SlavePipe` to communicate with the master.\n",
    "    - During the forward pass, master device invokes `run_master`, all messages from slave devices will be collected,\n",
    "    and passed to a registered callback.\n",
    "    - After receiving the messages, the master device should gather the information and determine to message passed\n",
    "    back to each slave devices.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, master_callback):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            master_callback: a callback to be invoked after having collected messages from slave devices.\n",
    "        \"\"\"\n",
    "        self._master_callback = master_callback\n",
    "        self._queue = queue.Queue()\n",
    "        self._registry = collections.OrderedDict()\n",
    "        self._activated = False\n",
    "\n",
    "    def __getstate__(self):\n",
    "        return {'master_callback': self._master_callback}\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        self.__init__(state['master_callback'])\n",
    "\n",
    "    def register_slave(self, identifier):\n",
    "        \"\"\"\n",
    "        Register an slave device.\n",
    "\n",
    "        Args:\n",
    "            identifier: an identifier, usually is the device id.\n",
    "\n",
    "        Returns: a `SlavePipe` object which can be used to communicate with the master device.\n",
    "\n",
    "        \"\"\"\n",
    "        if self._activated:\n",
    "            assert self._queue.empty(), 'Queue is not clean before next initialization.'\n",
    "            self._activated = False\n",
    "            self._registry.clear()\n",
    "        future = FutureResult()\n",
    "        self._registry[identifier] = _MasterRegistry(future)\n",
    "        return SlavePipe(identifier, self._queue, future)\n",
    "\n",
    "    def run_master(self, master_msg):\n",
    "        \"\"\"\n",
    "        Main entry for the master device in each forward pass.\n",
    "        The messages were first collected from each devices (including the master device), and then\n",
    "        an callback will be invoked to compute the message to be sent back to each devices\n",
    "        (including the master device).\n",
    "\n",
    "        Args:\n",
    "            master_msg: the message that the master want to send to itself. This will be placed as the first\n",
    "            message when calling `master_callback`. For detailed usage, see `_SynchronizedBatchNorm` for an example.\n",
    "\n",
    "        Returns: the message to be sent back to the master device.\n",
    "\n",
    "        \"\"\"\n",
    "        self._activated = True\n",
    "\n",
    "        intermediates = [(0, master_msg)]\n",
    "        for i in range(self.nr_slaves):\n",
    "            intermediates.append(self._queue.get())\n",
    "\n",
    "        results = self._master_callback(intermediates)\n",
    "        assert results[0][0] == 0, 'The first result should belongs to the master.'\n",
    "\n",
    "        for i, res in results:\n",
    "            if i == 0:\n",
    "                continue\n",
    "            self._registry[i].result.put(res)\n",
    "\n",
    "        for i in range(self.nr_slaves):\n",
    "            assert self._queue.get() is True\n",
    "\n",
    "        return results[0][1]\n",
    "\n",
    "    @property\n",
    "    def nr_slaves(self):\n",
    "        return len(self._registry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### replicate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = [\n",
    "    'CallbackContext',\n",
    "    'execute_replication_callbacks',\n",
    "    'DataParallelWithCallback',\n",
    "    'patch_replication_callback'\n",
    "]\n",
    "\n",
    "\n",
    "class CallbackContext(object):\n",
    "    pass\n",
    "\n",
    "\n",
    "def execute_replication_callbacks(modules):\n",
    "    \"\"\"\n",
    "    Execute an replication callback `__data_parallel_replicate__` on each module created by original replication.\n",
    "\n",
    "    The callback will be invoked with arguments `__data_parallel_replicate__(ctx, copy_id)`\n",
    "\n",
    "    Note that, as all modules are isomorphism, we assign each sub-module with a context\n",
    "    (shared among multiple copies of this module on different devices).\n",
    "    Through this context, different copies can share some information.\n",
    "\n",
    "    We guarantee that the callback on the master copy (the first copy) will be called ahead of calling the callback\n",
    "    of any slave copies.\n",
    "    \"\"\"\n",
    "    master_copy = modules[0]\n",
    "    nr_modules = len(list(master_copy.modules()))\n",
    "    ctxs = [CallbackContext() for _ in range(nr_modules)]\n",
    "\n",
    "    for i, module in enumerate(modules):\n",
    "        for j, m in enumerate(module.modules()):\n",
    "            if hasattr(m, '__data_parallel_replicate__'):\n",
    "                m.__data_parallel_replicate__(ctxs[j], i)\n",
    "\n",
    "\n",
    "class DataParallelWithCallback(DataParallel):\n",
    "    \"\"\"\n",
    "    Data Parallel with a replication callback.\n",
    "\n",
    "    An replication callback `__data_parallel_replicate__` of each module will be invoked after being created by\n",
    "    original `replicate` function.\n",
    "    The callback will be invoked with arguments `__data_parallel_replicate__(ctx, copy_id)`\n",
    "\n",
    "    Examples:\n",
    "        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n",
    "        > sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n",
    "        # sync_bn.__data_parallel_replicate__ will be invoked.\n",
    "    \"\"\"\n",
    "\n",
    "    def replicate(self, module, device_ids):\n",
    "        modules = super(DataParallelWithCallback,\n",
    "                        self).replicate(module, device_ids)\n",
    "        execute_replication_callbacks(modules)\n",
    "        return modules\n",
    "\n",
    "\n",
    "def patch_replication_callback(data_parallel):\n",
    "    \"\"\"\n",
    "    Monkey-patch an existing `DataParallel` object. Add the replication callback.\n",
    "    Useful when you have customized `DataParallel` implementation.\n",
    "\n",
    "    Examples:\n",
    "        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n",
    "        > sync_bn = DataParallel(sync_bn, device_ids=[0, 1])\n",
    "        > patch_replication_callback(sync_bn)\n",
    "        # this is equivalent to\n",
    "        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n",
    "        > sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n",
    "    \"\"\"\n",
    "\n",
    "    assert isinstance(data_parallel, DataParallel)\n",
    "\n",
    "    old_replicate = data_parallel.replicate\n",
    "\n",
    "    @functools.wraps(old_replicate)\n",
    "    def new_replicate(module, device_ids):\n",
    "        modules = old_replicate(module, device_ids)\n",
    "        execute_replication_callbacks(modules)\n",
    "        return modules\n",
    "\n",
    "    data_parallel.replicate = new_replicate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batchnorm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = ['SynchronizedBatchNorm1d', 'SynchronizedBatchNorm2d',\n",
    "           'SynchronizedBatchNorm3d', 'convert_model']\n",
    "\n",
    "\n",
    "def _sum_ft(tensor):\n",
    "    \"\"\"sum over the first and last dimention\"\"\"\n",
    "    return tensor.sum(dim=0).sum(dim=-1)\n",
    "\n",
    "\n",
    "def _unsqueeze_ft(tensor):\n",
    "    \"\"\"add new dementions at the front and the tail\"\"\"\n",
    "    return tensor.unsqueeze(0).unsqueeze(-1)\n",
    "\n",
    "\n",
    "_ChildMessage = collections.namedtuple(\n",
    "    '_ChildMessage', ['sum', 'ssum', 'sum_size'])\n",
    "_MasterMessage = collections.namedtuple('_MasterMessage', ['sum', 'inv_std'])\n",
    "\n",
    "\n",
    "class _SynchronizedBatchNorm(_BatchNorm):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True):\n",
    "        super(_SynchronizedBatchNorm, self).__init__(\n",
    "            num_features, eps=eps, momentum=momentum, affine=affine)\n",
    "\n",
    "        self._sync_master = SyncMaster(self._data_parallel_master)\n",
    "\n",
    "        self._is_parallel = False\n",
    "        self._parallel_id = None\n",
    "        self._slave_pipe = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        # If it is not parallel computation or is in evaluation mode, use PyTorch's implementation.\n",
    "        if not (self._is_parallel and self.training):\n",
    "            return F.batch_norm(\n",
    "                input, self.running_mean, self.running_var, self.weight, self.bias,\n",
    "                self.training, self.momentum, self.eps)\n",
    "\n",
    "        # Resize the input to (B, C, -1).\n",
    "        input_shape = input.size()\n",
    "        input = input.view(input.size(0), self.num_features, -1)\n",
    "\n",
    "        # Compute the sum and square-sum.\n",
    "        sum_size = input.size(0) * input.size(2)\n",
    "        input_sum = _sum_ft(input)\n",
    "        input_ssum = _sum_ft(input ** 2)\n",
    "\n",
    "        # Reduce-and-broadcast the statistics.\n",
    "        if self._parallel_id == 0:\n",
    "            mean, inv_std = self._sync_master.run_master(\n",
    "                _ChildMessage(input_sum, input_ssum, sum_size))\n",
    "        else:\n",
    "            mean, inv_std = self._slave_pipe.run_slave(\n",
    "                _ChildMessage(input_sum, input_ssum, sum_size))\n",
    "\n",
    "        # Compute the output.\n",
    "        if self.affine:\n",
    "            # MJY:: Fuse the multiplication for speed.\n",
    "            output = (input - _unsqueeze_ft(mean)) * \\\n",
    "                     _unsqueeze_ft(inv_std * self.weight) + _unsqueeze_ft(self.bias)\n",
    "        else:\n",
    "            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std)\n",
    "\n",
    "        # Reshape it.\n",
    "        return output.view(input_shape)\n",
    "\n",
    "    def __data_parallel_replicate__(self, ctx, copy_id):\n",
    "        self._is_parallel = True\n",
    "        self._parallel_id = copy_id\n",
    "\n",
    "        # parallel_id == 0 means master device.\n",
    "        if self._parallel_id == 0:\n",
    "            ctx.sync_master = self._sync_master\n",
    "        else:\n",
    "            self._slave_pipe = ctx.sync_master.register_slave(copy_id)\n",
    "\n",
    "    def _data_parallel_master(self, intermediates):\n",
    "        \"\"\"Reduce the sum and square-sum, compute the statistics, and broadcast it.\"\"\"\n",
    "\n",
    "        # Always using same \"device order\" makes the ReduceAdd operation faster.\n",
    "        # Thanks to:: Tete Xiao (http://tetexiao.com/)\n",
    "        intermediates = sorted(intermediates, key=lambda i: i[1].sum.get_device())\n",
    "\n",
    "        to_reduce = [i[1][:2] for i in intermediates]\n",
    "        to_reduce = [j for i in to_reduce for j in i]  # flatten\n",
    "        target_gpus = [i[1].sum.get_device() for i in intermediates]\n",
    "\n",
    "        sum_size = sum([i[1].sum_size for i in intermediates])\n",
    "        sum_, ssum = ReduceAddCoalesced.apply(target_gpus[0], 2, *to_reduce)\n",
    "        mean, inv_std = self._compute_mean_std(sum_, ssum, sum_size)\n",
    "\n",
    "        broadcasted = Broadcast.apply(target_gpus, mean, inv_std)\n",
    "\n",
    "        outputs = []\n",
    "        for i, rec in enumerate(intermediates):\n",
    "            outputs.append((rec[0], _MasterMessage(*broadcasted[i * 2:i * 2 + 2])))\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def _compute_mean_std(self, sum_, ssum, size):\n",
    "        \"\"\"Compute the mean and standard-deviation with sum and square-sum. This method\n",
    "        also maintains the moving average on the master device.\"\"\"\n",
    "        assert size > 1, 'BatchNorm computes unbiased standard-deviation, which requires size > 1.'\n",
    "        mean = sum_ / size\n",
    "        sumvar = ssum - sum_ * mean\n",
    "        unbias_var = sumvar / (size - 1)\n",
    "        bias_var = sumvar / size\n",
    "\n",
    "        self.running_mean = (1 - self.momentum) * \\\n",
    "                            self.running_mean + self.momentum * mean.data\n",
    "        self.running_var = (1 - self.momentum) * \\\n",
    "                           self.running_var + self.momentum * unbias_var.data\n",
    "\n",
    "        return mean, bias_var.clamp(self.eps) ** -0.5\n",
    "\n",
    "\n",
    "class SynchronizedBatchNorm1d(_SynchronizedBatchNorm):\n",
    "    r\"\"\"Applies Synchronized Batch Normalization over a 2d or 3d input that is seen as a\n",
    "    mini-batch.\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n",
    "\n",
    "    This module differs from the built-in PyTorch BatchNorm1d as the mean and\n",
    "    standard-deviation are reduced across all devices during training.\n",
    "\n",
    "    For example, when one uses `nn.DataParallel` to wrap the network during\n",
    "    training, PyTorch's implementation normalize the tensor on each device using\n",
    "    the statistics only on that device, which accelerated the computation and\n",
    "    is also easy to implement, but the statistics might be inaccurate.\n",
    "    Instead, in this synchronized version, the statistics will be computed\n",
    "    over all training samples distributed on multiple devices.\n",
    "\n",
    "    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n",
    "    as the built-in PyTorch implementation.\n",
    "\n",
    "    The mean and standard-deviation are calculated per-dimension over\n",
    "    the mini-batches and gamma and beta are learnable parameter vectors\n",
    "    of size C (where C is the input size).\n",
    "\n",
    "    During training, this layer keeps a running estimate of its computed mean\n",
    "    and variance. The running sum is kept with a default momentum of 0.1.\n",
    "\n",
    "    During evaluation, this running mean/variance is used for normalization.\n",
    "\n",
    "    Because the BatchNorm is done over the `C` dimension, computing statistics\n",
    "    on `(N, L)` slices, it's common terminology to call this Temporal BatchNorm\n",
    "\n",
    "    Args:\n",
    "        num_features: num_features from an expected input of size\n",
    "            `batch_size x num_features [x width]`\n",
    "        eps: a value added to the denominator for numerical stability.\n",
    "            Default: 1e-5\n",
    "        momentum: the value used for the running_mean and running_var\n",
    "            computation. Default: 0.1\n",
    "        affine: a boolean value that when set to ``True``, gives the layer learnable\n",
    "            affine parameters. Default: ``True``\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(N, C)` or :math:`(N, C, L)`\n",
    "        - Output: :math:`(N, C)` or :math:`(N, C, L)` (same shape as input)\n",
    "\n",
    "    Examples:\n",
    "        >>> # With Learnable Parameters\n",
    "        >>> m = SynchronizedBatchNorm1d(100)\n",
    "        >>> # Without Learnable Parameters\n",
    "        >>> m = SynchronizedBatchNorm1d(100, affine=False)\n",
    "        >>> input = torch.autograd.Variable(torch.randn(20, 100))\n",
    "        >>> output = m(input)\n",
    "    \"\"\"\n",
    "\n",
    "    def _check_input_dim(self, input):\n",
    "        if input.dim() != 2 and input.dim() != 3:\n",
    "            raise ValueError('expected 2D or 3D input (got {}D input)'\n",
    "                             .format(input.dim()))\n",
    "        super(SynchronizedBatchNorm1d, self)._check_input_dim(input)\n",
    "\n",
    "\n",
    "class SynchronizedBatchNorm2d(_SynchronizedBatchNorm):\n",
    "    r\"\"\"Applies Batch Normalization over a 4d input that is seen as a mini-batch\n",
    "    of 3d inputs\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n",
    "\n",
    "    This module differs from the built-in PyTorch BatchNorm2d as the mean and\n",
    "    standard-deviation are reduced across all devices during training.\n",
    "\n",
    "    For example, when one uses `nn.DataParallel` to wrap the network during\n",
    "    training, PyTorch's implementation normalize the tensor on each device using\n",
    "    the statistics only on that device, which accelerated the computation and\n",
    "    is also easy to implement, but the statistics might be inaccurate.\n",
    "    Instead, in this synchronized version, the statistics will be computed\n",
    "    over all training samples distributed on multiple devices.\n",
    "\n",
    "    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n",
    "    as the built-in PyTorch implementation.\n",
    "\n",
    "    The mean and standard-deviation are calculated per-dimension over\n",
    "    the mini-batches and gamma and beta are learnable parameter vectors\n",
    "    of size C (where C is the input size).\n",
    "\n",
    "    During training, this layer keeps a running estimate of its computed mean\n",
    "    and variance. The running sum is kept with a default momentum of 0.1.\n",
    "\n",
    "    During evaluation, this running mean/variance is used for normalization.\n",
    "\n",
    "    Because the BatchNorm is done over the `C` dimension, computing statistics\n",
    "    on `(N, H, W)` slices, it's common terminology to call this Spatial BatchNorm\n",
    "\n",
    "    Args:\n",
    "        num_features: num_features from an expected input of\n",
    "            size batch_size x num_features x height x width\n",
    "        eps: a value added to the denominator for numerical stability.\n",
    "            Default: 1e-5\n",
    "        momentum: the value used for the running_mean and running_var\n",
    "            computation. Default: 0.1\n",
    "        affine: a boolean value that when set to ``True``, gives the layer learnable\n",
    "            affine parameters. Default: ``True``\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(N, C, H, W)`\n",
    "        - Output: :math:`(N, C, H, W)` (same shape as input)\n",
    "\n",
    "    Examples:\n",
    "        >>> # With Learnable Parameters\n",
    "        >>> m = SynchronizedBatchNorm2d(100)\n",
    "        >>> # Without Learnable Parameters\n",
    "        >>> m = SynchronizedBatchNorm2d(100, affine=False)\n",
    "        >>> input = torch.autograd.Variable(torch.randn(20, 100, 35, 45))\n",
    "        >>> output = m(input)\n",
    "    \"\"\"\n",
    "\n",
    "    def _check_input_dim(self, input):\n",
    "        if input.dim() != 4:\n",
    "            raise ValueError('expected 4D input (got {}D input)'\n",
    "                             .format(input.dim()))\n",
    "        super(SynchronizedBatchNorm2d, self)._check_input_dim(input)\n",
    "\n",
    "\n",
    "class SynchronizedBatchNorm3d(_SynchronizedBatchNorm):\n",
    "    r\"\"\"Applies Batch Normalization over a 5d input that is seen as a mini-batch\n",
    "    of 4d inputs\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n",
    "\n",
    "    This module differs from the built-in PyTorch BatchNorm3d as the mean and\n",
    "    standard-deviation are reduced across all devices during training.\n",
    "\n",
    "    For example, when one uses `nn.DataParallel` to wrap the network during\n",
    "    training, PyTorch's implementation normalize the tensor on each device using\n",
    "    the statistics only on that device, which accelerated the computation and\n",
    "    is also easy to implement, but the statistics might be inaccurate.\n",
    "    Instead, in this synchronized version, the statistics will be computed\n",
    "    over all training samples distributed on multiple devices.\n",
    "\n",
    "    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n",
    "    as the built-in PyTorch implementation.\n",
    "\n",
    "    The mean and standard-deviation are calculated per-dimension over\n",
    "    the mini-batches and gamma and beta are learnable parameter vectors\n",
    "    of size C (where C is the input size).\n",
    "\n",
    "    During training, this layer keeps a running estimate of its computed mean\n",
    "    and variance. The running sum is kept with a default momentum of 0.1.\n",
    "\n",
    "    During evaluation, this running mean/variance is used for normalization.\n",
    "\n",
    "    Because the BatchNorm is done over the `C` dimension, computing statistics\n",
    "    on `(N, D, H, W)` slices, it's common terminology to call this Volumetric BatchNorm\n",
    "    or Spatio-temporal BatchNorm\n",
    "\n",
    "    Args:\n",
    "        num_features: num_features from an expected input of\n",
    "            size batch_size x num_features x depth x height x width\n",
    "        eps: a value added to the denominator for numerical stability.\n",
    "            Default: 1e-5\n",
    "        momentum: the value used for the running_mean and running_var\n",
    "            computation. Default: 0.1\n",
    "        affine: a boolean value that when set to ``True``, gives the layer learnable\n",
    "            affine parameters. Default: ``True``\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(N, C, D, H, W)`\n",
    "        - Output: :math:`(N, C, D, H, W)` (same shape as input)\n",
    "\n",
    "    Examples:\n",
    "        >>> # With Learnable Parameters\n",
    "        >>> m = SynchronizedBatchNorm3d(100)\n",
    "        >>> # Without Learnable Parameters\n",
    "        >>> m = SynchronizedBatchNorm3d(100, affine=False)\n",
    "        >>> input = torch.autograd.Variable(torch.randn(20, 100, 35, 45, 10))\n",
    "        >>> output = m(input)\n",
    "    \"\"\"\n",
    "\n",
    "    def _check_input_dim(self, input):\n",
    "        if input.dim() != 5:\n",
    "            raise ValueError('expected 5D input (got {}D input)'\n",
    "                             .format(input.dim()))\n",
    "        super(SynchronizedBatchNorm3d, self)._check_input_dim(input)\n",
    "\n",
    "\n",
    "def convert_model(module):\n",
    "    \"\"\"Traverse the input module and its child recursively\n",
    "       and replace all instance of torch.nn.modules.batchnorm.BatchNorm*N*d\n",
    "       to SynchronizedBatchNorm*N*d\n",
    "\n",
    "    Args:\n",
    "        module: the input module needs to be convert to SyncBN model\n",
    "\n",
    "    Examples:\n",
    "        >>> import torch.nn as nn\n",
    "        >>> import torchvision\n",
    "        >>> # m is a standard pytorch model\n",
    "        >>> m = torchvision.models.resnet18(True)\n",
    "        >>> m = nn.DataParallel(m)\n",
    "        >>> # after convert, m is using SyncBN\n",
    "        >>> m = convert_model(m)\n",
    "    \"\"\"\n",
    "    if isinstance(module, torch.nn.DataParallel):\n",
    "        mod = module.module\n",
    "        mod = convert_model(mod)\n",
    "        mod = DataParallelWithCallback(mod)\n",
    "        return mod\n",
    "\n",
    "    mod = module\n",
    "    for pth_module, sync_module in zip([torch.nn.modules.batchnorm.BatchNorm1d,\n",
    "                                        torch.nn.modules.batchnorm.BatchNorm2d,\n",
    "                                        torch.nn.modules.batchnorm.BatchNorm3d],\n",
    "                                       [SynchronizedBatchNorm1d,\n",
    "                                        SynchronizedBatchNorm2d,\n",
    "                                        SynchronizedBatchNorm3d]):\n",
    "        if isinstance(module, pth_module):\n",
    "            mod = sync_module(module.num_features, module.eps,\n",
    "                              module.momentum, module.affine)\n",
    "            mod.running_mean = module.running_mean\n",
    "            mod.running_var = module.running_var\n",
    "            if module.affine:\n",
    "                mod.weight.data = module.weight.data.clone().detach()\n",
    "                mod.bias.data = module.bias.data.clone().detach()\n",
    "\n",
    "    for name, child in module.named_children():\n",
    "        mod.add_module(name, convert_model(child))\n",
    "\n",
    "    return mod\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### warmupLR.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class warmupLR(toptim._LRScheduler):\n",
    "    \"\"\" Warmup learning rate scheduler.\n",
    "        Initially, increases the learning rate from 0 to the final value, in a\n",
    "        certain number of steps. After this number of steps, each step decreases\n",
    "        LR exponentially.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, lr, warmup_steps, momentum, decay):\n",
    "        # cyclic params\n",
    "        self.optimizer = optimizer\n",
    "        self.lr = lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.momentum = momentum\n",
    "        self.decay = decay\n",
    "\n",
    "        # cap to one\n",
    "        if self.warmup_steps < 1:\n",
    "            self.warmup_steps = 1\n",
    "\n",
    "        # cyclic lr\n",
    "        self.initial_scheduler = toptim.CyclicLR(self.optimizer,\n",
    "                                                 base_lr=0,\n",
    "                                                 max_lr=self.lr,\n",
    "                                                 step_size_up=self.warmup_steps,\n",
    "                                                 step_size_down=self.warmup_steps,\n",
    "                                                 cycle_momentum=False,\n",
    "                                                 base_momentum=self.momentum,\n",
    "                                                 max_momentum=self.momentum)\n",
    "\n",
    "        # our params\n",
    "        self.last_epoch = -1  # fix for pytorch 1.1 and below\n",
    "        self.finished = False  # am i done\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [self.lr * (self.decay ** self.last_epoch) for lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if self.finished or self.initial_scheduler.last_epoch >= self.warmup_steps:\n",
    "            if not self.finished:\n",
    "                self.base_lrs = [self.lr for lr in self.base_lrs]\n",
    "                self.finished = True\n",
    "            return super(warmupLR, self).step(epoch)\n",
    "        else:\n",
    "            return self.initial_scheduler.step(epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### onehot.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class oneHot(nn.Module):\n",
    "    def __init__(self, device, nclasses, spatial_dim=2):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.nclasses = nclasses\n",
    "        self.spatial_dim = spatial_dim\n",
    "\n",
    "    def onehot1dspatial(self, x):\n",
    "        # we only do tensors that 1d tensors that are batched or not, so check\n",
    "        assert (len(x.shape) == 1 or len(x.shape) == 2)\n",
    "        # if not batched, batch\n",
    "        remove_dim = False  # flag to unbatch\n",
    "        if len(x.shape) == 1:\n",
    "            # add batch dimension\n",
    "            x = x[None, ...]\n",
    "            remove_dim = True\n",
    "\n",
    "        # get tensor shape\n",
    "        n, b = x.shape\n",
    "\n",
    "        # scatter to onehot\n",
    "        one_hot = torch.zeros((n, self.nclasses, b),\n",
    "                              device=self.device).scatter_(1, x.unsqueeze(1), 1)\n",
    "\n",
    "        # x is now [n,classes,b]\n",
    "\n",
    "        # if it used to be unbatched, then unbatch it\n",
    "        if remove_dim:\n",
    "            one_hot = one_hot[0]\n",
    "\n",
    "        return one_hot\n",
    "\n",
    "    def onehot2dspatial(self, x):\n",
    "        # we only do tensors that 2d tensors that are batched or not, so check\n",
    "        assert (len(x.shape) == 2 or len(x.shape) == 3)\n",
    "        # if not batched, batch\n",
    "        remove_dim = False  # flag to unbatch\n",
    "        if len(x.shape) == 2:\n",
    "            # add batch dimension\n",
    "            x = x[None, ...]\n",
    "            remove_dim = True\n",
    "\n",
    "        # get tensor shape\n",
    "        n, h, w = x.shape\n",
    "\n",
    "        # scatter to onehot\n",
    "        one_hot = torch.zeros((n, self.nclasses, h, w),\n",
    "                              device=self.device).scatter_(1, x.unsqueeze(1), 1)\n",
    "\n",
    "        # x is now [n,classes,b]\n",
    "\n",
    "        # if it used to be unbatched, then unbatch it\n",
    "        if remove_dim:\n",
    "            one_hot = one_hot[0]\n",
    "\n",
    "        return one_hot\n",
    "\n",
    "    def forward(self, x):\n",
    "        # do onehot here\n",
    "        if self.spatial_dim == 1:\n",
    "            return self.onehot1dspatial(x)\n",
    "        elif self.spatial_dim == 2:\n",
    "            return self.onehot2dspatial(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bordermask.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Border Mask for 2D labeled range images.\n",
    "\n",
    "Simple module to obtain the border mask of a given range image.\n",
    "\n",
    "The border mask is defined as the zone where are intersections between\n",
    "differrent classes for the given range image.\n",
    "\n",
    "In this case we will violate a little bit the definition and will augment it. We\n",
    "define the border mask as the zone where are intersections between differnet\n",
    "classes for the given range image in determined neighborhood. To obtain this\n",
    "border mask we will need to apply de binary erosion algorithm multiple times to\n",
    "the same range image.\n",
    "\n",
    "Example:\n",
    "  Suppose we have 3 classes and this given range image(labeled):\n",
    "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0],\n",
    "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0],\n",
    "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0],\n",
    "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0],\n",
    "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0],\n",
    "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "  The output of the bordermask would like:\n",
    "  # 1 erode iteration with a connectivity kernel of 4:\n",
    "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "  [1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1],\n",
    "  [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1],\n",
    "  [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1],\n",
    "  [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1],\n",
    "  [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1],\n",
    "  [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1],\n",
    "  [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1],\n",
    "  [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1],\n",
    "  [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1],\n",
    "  [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1],\n",
    "  [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1],\n",
    "  [1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1],\n",
    "  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1],\n",
    "  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1],\n",
    "  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1],\n",
    "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "  # 2 erode iterations with a connectivity kernel of 8:\n",
    "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1],\n",
    "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1],\n",
    "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1],\n",
    "  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1],\n",
    "  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1],\n",
    "  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1],\n",
    "  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1],\n",
    "  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1],\n",
    "  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1],\n",
    "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1],\n",
    "  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
    "\"\"\"\n",
    "\n",
    "class borderMask(nn.Module):\n",
    "    def __init__(self, nclasses, device, border_size, kern_conn=4, background_class=None):\n",
    "        \"\"\"Get the binary border mask of a labeled 2d range image.\n",
    "\n",
    "      Args:\n",
    "          nclasses(int)         : The number of classes labeled in the input image\n",
    "          device(torch.device)  : Process in host or cuda?\n",
    "          border_size(int)      : How many erode iterations to perform for the mask\n",
    "          kern_conn(int)        : The connectivity kernel number (4 or 8)\n",
    "          background_class(int) : \"unlabeled\" class in dataset (to avoid double borders)\n",
    "\n",
    "      Returns:\n",
    "          eroded_output(tensor) : The 2d binary border mask, 1 where a intersection\n",
    "                                  between classes occurs, 0 everywhere else\n",
    "\n",
    "      \"\"\"\n",
    "        super().__init__()\n",
    "        self.nclasses = nclasses\n",
    "        self.device = device\n",
    "        self.border_size = border_size\n",
    "        self.kern_conn = kern_conn\n",
    "        self.background_class = background_class\n",
    "        if self.background_class is not None:\n",
    "            self.include_idx = list(range(self.nclasses))\n",
    "            self.exclude_idx = self.include_idx.pop(self.background_class)\n",
    "\n",
    "        # check connectivity\n",
    "        # For obtaining the border mask we will be eroding the input image, for this\n",
    "        # reason we only support erode_kernels with connectivity 4 or 8\n",
    "        assert self.kern_conn in (4, 8), (\"The specified kernel connectivity(kern_conn= %r) is \"\n",
    "                                          \"not supported\" % self.kern_conn)\n",
    "\n",
    "        # make the onehot inferer\n",
    "        self.onehot = oneHot(self.device,\n",
    "                             self.nclasses,\n",
    "                             spatial_dim=2)  # range labels\n",
    "\n",
    "    def forward(self, range_label):\n",
    "        # length of shape of range_label must be 3 (N, H, W)\n",
    "        must_unbatch = False  # remove batch dimension after operation?\n",
    "        if len(range_label.shape) != 3:\n",
    "            range_label = range_label[None, ...]\n",
    "            must_unbatch = True\n",
    "\n",
    "        # The range_label comes labeled, we need to create one tensor per class, thus:\n",
    "        input_tensor = self.onehot(range_label)  # (N, C, H, W)\n",
    "\n",
    "        # Because we are using GT range_labels, there is a lot of pixels that end up\n",
    "        # unlabeled(thus, in the background). If we feed the erosion algorithm with\n",
    "        # this \"raw\" gt_labels we will detect intersection between the other classes\n",
    "        # and the backgorund, and we will end with the incorrect border mask. To solve\n",
    "        # this issue we need to pre process the input gt_label. The artifact in this\n",
    "        # case will be to sum the background channel(mostly the channel 0) to\n",
    "        # all the rest channels expect for the background channel itself.\n",
    "        # This will allow us to avoid detecting intersections between a class and the\n",
    "        # background. This also force us to change the logical AND we were doing to\n",
    "        # obtain the border mask when we were working with predicted labels.\n",
    "        # With predicted labels won't see this problem because all the pixels belongs\n",
    "        # to at least one class\n",
    "        if self.background_class is not None:\n",
    "            input_tensor[:, self.include_idx] = input_tensor[:, self.include_idx] + \\\n",
    "                                                input_tensor[:, self.exclude_idx]\n",
    "\n",
    "        # C denotes a number of channels, N, H and W are dismissed\n",
    "        C = input_tensor.shape[1]\n",
    "\n",
    "        # Create an empty erode kernel and send it to 'device'\n",
    "        erode_kernel = torch.zeros((C, 1, 3, 3), device=self.device)\n",
    "        if self.kern_conn == 4:\n",
    "            erode_kernel[:] = torch.tensor([[0, 1, 0],\n",
    "                                            [1, 1, 1],\n",
    "                                            [0, 1, 0]], device=self.device)\n",
    "        else:\n",
    "            erode_kernel[:] = torch.tensor([[1, 1, 1],\n",
    "                                            [1, 1, 1],\n",
    "                                            [1, 1, 1]], device=self.device)\n",
    "\n",
    "        # to check connectivity\n",
    "        kernel_sum = erode_kernel[0][0].sum()  # should be kern_conn + 1\n",
    "\n",
    "        # erode the input image border_size times\n",
    "        erode_input = input_tensor\n",
    "        for _ in range(self.border_size):\n",
    "            eroded_output = F.conv2d(erode_input, erode_kernel, groups=C, padding=1)\n",
    "            # Pick the elements that match the kernel_sum to obtain the eroded\n",
    "            # output and convert to dtype=float32\n",
    "            eroded_output = (eroded_output == kernel_sum).float()\n",
    "            erode_input = eroded_output\n",
    "\n",
    "        # We want to sum up all the channels into 1 unique border mask\n",
    "        # Even when we added the background to all the rest of the channels, there\n",
    "        # might be \"bodies\" in the background channel, thus, the erosion process can\n",
    "        # output \"false positives\" were this \"bodies\" are present in the background.\n",
    "        # We need to obtain the background mask and add it to the eroded bodies to\n",
    "        # obtain a consisent output once we calculate the border mask\n",
    "        if self.background_class is not None:\n",
    "            background_mask = (eroded_output[:, self.exclude_idx] == 1)\n",
    "\n",
    "        # The eroded_bodies mask will consist in all the pixels were the convolution\n",
    "        # returned 1 for all the channels, therefore we need to sum up all the\n",
    "        # channels into one unique tensor and add the background mask to avoid having\n",
    "        # the background in the border mask output\n",
    "        eroded_bodies = (eroded_output.sum(1, keepdim=True) == 1)\n",
    "        if self.background_class is not None:\n",
    "            eroded_bodies = eroded_bodies + background_mask\n",
    "\n",
    "        # we want the opposite\n",
    "        borders = 1 - eroded_bodies\n",
    "\n",
    "        # unbatch?\n",
    "        if must_unbatch:\n",
    "            borders = borders[0]\n",
    "            # import cv2\n",
    "            # import numpy as np\n",
    "            # bordersprint = (borders * 255).squeeze().cpu().numpy().astype(np.uint8)\n",
    "            # cv2.imshow(\"border\", bordersprint)\n",
    "            # cv2.waitKey(0)\n",
    "\n",
    "        return borders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ioueval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iouEval:\n",
    "    def __init__(self, n_classes, device, ignore=None):\n",
    "        self.n_classes = n_classes\n",
    "        self.device = device\n",
    "        # if ignore is larger than n_classes, consider no ignoreIndex\n",
    "        self.ignore = torch.tensor(ignore).long()\n",
    "        self.include = torch.tensor(\n",
    "            [n for n in range(self.n_classes) if n not in self.ignore]).long()\n",
    "        print(\"[IOU EVAL] IGNORE: \", self.ignore)\n",
    "        print(\"[IOU EVAL] INCLUDE: \", self.include)\n",
    "        self.reset()\n",
    "\n",
    "    def num_classes(self):\n",
    "        return self.n_classes\n",
    "\n",
    "    def reset(self):\n",
    "        self.conf_matrix = torch.zeros(\n",
    "            (self.n_classes, self.n_classes), device=self.device).long()\n",
    "        self.ones = None\n",
    "        self.last_scan_size = None  # for when variable scan size is used\n",
    "\n",
    "    def addBatch(self, x, y):  # x=preds, y=targets\n",
    "        # if numpy, pass to pytorch\n",
    "        # to tensor\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.from_numpy(np.array(x)).long().to(self.device)\n",
    "        if isinstance(y, np.ndarray):\n",
    "            y = torch.from_numpy(np.array(y)).long().to(self.device)\n",
    "\n",
    "        # sizes should be \"batch_size x H x W\"\n",
    "        x_row = x.reshape(-1)  # de-batchify\n",
    "        y_row = y.reshape(-1)  # de-batchify\n",
    "\n",
    "        # idxs are labels and predictions\n",
    "        idxs = torch.stack([x_row, y_row], dim=0)\n",
    "\n",
    "        # ones is what I want to add to conf when I\n",
    "        if self.ones is None or self.last_scan_size != idxs.shape[-1]:\n",
    "            self.ones = torch.ones((idxs.shape[-1]), device=self.device).long()\n",
    "            self.last_scan_size = idxs.shape[-1]\n",
    "\n",
    "        # make confusion matrix (cols = gt, rows = pred)\n",
    "        self.conf_matrix = self.conf_matrix.index_put_(\n",
    "            tuple(idxs), self.ones, accumulate=True)\n",
    "\n",
    "        # print(self.tp.shape)\n",
    "        # print(self.fp.shape)\n",
    "        # print(self.fn.shape)\n",
    "\n",
    "    def getStats(self):\n",
    "        # remove fp and fn from confusion on the ignore classes cols and rows\n",
    "        conf = self.conf_matrix.clone().double()\n",
    "        conf[self.ignore] = 0\n",
    "        conf[:, self.ignore] = 0\n",
    "\n",
    "        # get the clean stats\n",
    "        tp = conf.diag()\n",
    "        fp = conf.sum(dim=1) - tp\n",
    "        fn = conf.sum(dim=0) - tp\n",
    "        return tp, fp, fn\n",
    "\n",
    "    def getIoU(self):\n",
    "        tp, fp, fn = self.getStats()\n",
    "        intersection = tp\n",
    "        union = tp + fp + fn + 1e-15\n",
    "        iou = intersection / union\n",
    "        iou_mean = (intersection[self.include] / union[self.include]).mean()\n",
    "        return iou_mean, iou  # returns \"iou mean\", \"iou per class\" ALL CLASSES\n",
    "\n",
    "    def getacc(self):\n",
    "        tp, fp, fn = self.getStats()\n",
    "        total_tp = tp.sum()\n",
    "        total = tp[self.include].sum() + fp[self.include].sum() + 1e-15\n",
    "        acc_mean = total_tp / total\n",
    "        return acc_mean  # returns \"acc mean\"\n",
    "\n",
    "\n",
    "class biouEval(iouEval):\n",
    "    def __init__(self, n_classes, device, ignore=None, border_size=1, kern_conn=4):\n",
    "        super().__init__(n_classes, device, ignore)\n",
    "        self.border_size = border_size\n",
    "        self.kern_conn = kern_conn\n",
    "\n",
    "        # check that I am only ignoring one class\n",
    "        if len(ignore) > 1:\n",
    "            raise ValueError(\"Length of ignored class list should be 1 or 0\")\n",
    "        elif len(ignore) == 0:\n",
    "            ignore = None\n",
    "        else:\n",
    "            ignore = ignore[0]\n",
    "\n",
    "        self.borderer = borderMask(self.n_classes, self.device,\n",
    "                                   self.border_size, self.kern_conn,\n",
    "                                   background_class=ignore)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        return\n",
    "\n",
    "    def addBorderBatch1d(self, range_y, x, y, px, py):\n",
    "        '''range_y=target as img, x=preds, y=targets, px,py=idxs of points of\n",
    "           pointcloud in range img\n",
    "           WARNING: Only batch size 1 works for now\n",
    "        '''\n",
    "        # if numpy, pass to pytorch\n",
    "        # to tensor\n",
    "        if isinstance(range_y, np.ndarray):\n",
    "            range_y = torch.from_numpy(np.array(range_y)).long().to(self.device)\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.from_numpy(np.array(x)).long().to(self.device)\n",
    "        if isinstance(y, np.ndarray):\n",
    "            y = torch.from_numpy(np.array(y)).long().to(self.device)\n",
    "        if isinstance(px, np.ndarray):\n",
    "            px = torch.from_numpy(np.array(px)).long().to(self.device)\n",
    "        if isinstance(py, np.ndarray):\n",
    "            py = torch.from_numpy(np.array(py)).long().to(self.device)\n",
    "\n",
    "        # get border mask of range_y\n",
    "        border_mask_2d = self.borderer(range_y)\n",
    "\n",
    "        # filter px, py according to if they are on border mask or not\n",
    "        border_mask_1d = border_mask_2d[0, py, px].byte()\n",
    "\n",
    "        # get proper points from filtered x and y\n",
    "        x_in_mask = torch.masked_select(x, border_mask_1d)\n",
    "        y_in_mask = torch.masked_select(y, border_mask_1d)\n",
    "\n",
    "        # add batch\n",
    "        self.addBatch(x_in_mask, y_in_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### segmentator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/usr/bin/env python3\n",
    "# This file is covered by the LICENSE file in the root of this project.\n",
    "\n",
    "\n",
    "TRAIN_PATH = \"../\"\n",
    "\n",
    "class Add(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Add, self).__init__()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        return x + y\n",
    "\n",
    "\n",
    "class resBlock_with_add(nn.Module):\n",
    "    def __init__(self, conv, act, bn):\n",
    "        super(resBlock_with_add, self).__init__()\n",
    "\n",
    "        self.conv = conv\n",
    "        self.act = act\n",
    "        self.bn = bn\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        res = self.conv(x)\n",
    "        res = self.act(res)\n",
    "        res = self.bn(res)\n",
    "        return res + y\n",
    "\n",
    "\n",
    "class Trans(nn.Module):\n",
    "    def __init__(self, trans, trans_act, trans_bn):\n",
    "        super(Trans, self).__init__()\n",
    "        self.trans = trans\n",
    "        self.trans_act = trans_act\n",
    "        self.trans_bn = trans_bn\n",
    "\n",
    "    def forward(self, x):\n",
    "        upA = self.trans(x)\n",
    "        upA = self.trans_act(upA)\n",
    "        upA = self.trans_bn(upA)\n",
    "        return upA\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, f_g, f_l, f_int):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.Wg = nn.Sequential(nn.Conv2d(f_g, f_int, kernel_size=1, padding=0, stride=1),\n",
    "                                nn.BatchNorm2d(f_int))\n",
    "\n",
    "        self.Wx = nn.Sequential(nn.Conv2d(f_l, f_int, kernel_size=1, padding=0, stride=1),\n",
    "                                nn.BatchNorm2d(f_int))\n",
    "\n",
    "        self.psi = nn.Sequential(nn.Conv2d(f_int, 1, kernel_size=1, padding=0, stride=1),\n",
    "                                 nn.BatchNorm2d(1),\n",
    "                                 nn.Sigmoid())\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.Wg(g)\n",
    "        x1 = self.Wx(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        return x * psi\n",
    "\n",
    "\n",
    "class ResContextBlock(nn.Module):\n",
    "    def __init__(self, in_filters, out_filters, kernel_size=(3, 3), stride=1):\n",
    "        super(ResContextBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_filters, out_filters, kernel_size=(1, 1), stride=stride)\n",
    "        self.act1 = nn.LeakyReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_filters, out_filters, kernel_size, padding=1)\n",
    "        self.act2 = nn.LeakyReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(out_filters)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(out_filters, out_filters, kernel_size, padding=1)\n",
    "        self.act3 = nn.LeakyReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(out_filters)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = self.conv1(x)\n",
    "        shortcut = self.act1(shortcut)\n",
    "\n",
    "        resA = self.conv2(x)\n",
    "        resA = self.act2(resA)\n",
    "        resA = self.bn1(resA)\n",
    "\n",
    "        resA = self.conv3(resA)\n",
    "        resA = self.act3(resA)\n",
    "        resA = self.bn2(resA)\n",
    "        return resA + shortcut\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_filters, out_filters, dropout_rate, kernel_size=(3, 3), stride=1,\n",
    "                 pooling=True, drop_out=True):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.pooling = pooling\n",
    "        self.drop_out = drop_out\n",
    "        self.conv1 = nn.Conv2d(in_filters, out_filters, kernel_size=(1, 1), stride=stride)\n",
    "        self.act1 = nn.LeakyReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_filters, out_filters, kernel_size=kernel_size, padding=1)\n",
    "        self.act2 = nn.LeakyReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(out_filters)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(out_filters, out_filters, kernel_size=kernel_size, padding=1)\n",
    "        self.act3 = nn.LeakyReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(out_filters)\n",
    "\n",
    "        if pooling:\n",
    "            self.dropout = nn.Dropout2d(p=dropout_rate)\n",
    "            self.pool = nn.AvgPool2d(kernel_size=kernel_size, stride=2, padding=1)\n",
    "        else:\n",
    "            self.dropout = nn.Dropout2d(p=dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = self.conv1(x)\n",
    "        shortcut = self.act1(shortcut)\n",
    "\n",
    "        resA = self.conv2(x)\n",
    "        resA = self.act2(resA)\n",
    "        resA = self.bn1(resA)\n",
    "\n",
    "        resA = self.conv3(resA)\n",
    "        resA = self.act3(resA)\n",
    "        resA = self.bn2(resA)\n",
    "        resA = shortcut + resA\n",
    "\n",
    "        if self.pooling:\n",
    "            if self.drop_out:\n",
    "                resB = self.dropout(resA)\n",
    "            else:\n",
    "                resB = resA\n",
    "            resB = self.pool(resB)\n",
    "\n",
    "            return resB, resA\n",
    "        else:\n",
    "            if self.drop_out:\n",
    "                resB = self.dropout(resA)\n",
    "            else:\n",
    "                resB = resA\n",
    "            return resB\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_filters, out_filters, dropout_rate, kernel_size=(3, 3),drop_out=True):\n",
    "        super(UpBlock, self).__init__()\n",
    "        self.drop_out = drop_out\n",
    "        self.trans = nn.ConvTranspose2d(in_filters, out_filters, kernel_size, stride=(2, 2), padding=1)\n",
    "        self.trans_act = nn.LeakyReLU()\n",
    "        self.trans_bn = nn.BatchNorm2d(out_filters)\n",
    "\n",
    "        self.dropout1 = nn.Dropout2d(p=dropout_rate)\n",
    "        self.dropout2 = nn.Dropout2d(p=dropout_rate)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(out_filters, out_filters, kernel_size, padding=1)\n",
    "        self.act1 = nn.LeakyReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(out_filters)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_filters, out_filters, kernel_size, padding=1)\n",
    "        self.act2 = nn.LeakyReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(out_filters)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(out_filters, out_filters, kernel_size, padding=1)\n",
    "        self.act3 = nn.LeakyReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(out_filters)\n",
    "        self.dropout3 = nn.Dropout2d(p=dropout_rate)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        upA = self.trans(x)\n",
    "        if upA.shape != skip.shape:\n",
    "            upA = F.pad(upA, (0, 1, 0, 1), mode='replicate')\n",
    "        upA = self.trans_act(upA)\n",
    "        upA = self.trans_bn(upA)\n",
    "        if self.drop_out:\n",
    "            upA = self.dropout1(upA)\n",
    "        upB = upA + skip\n",
    "        if self.drop_out:\n",
    "            upB = self.dropout2(upB)\n",
    "\n",
    "        upE = self.conv1(upB)\n",
    "        upE = self.act1(upE)\n",
    "        upE = self.bn1(upE)\n",
    "\n",
    "        upE = self.conv2(upE)\n",
    "        upE = self.act2(upE)\n",
    "        upE = self.bn2(upE)\n",
    "\n",
    "        upE = self.conv3(upE)\n",
    "        upE = self.act3(upE)\n",
    "        upE = self.bn3(upE)\n",
    "        if self.drop_out:\n",
    "            upE = self.dropout3(upE)\n",
    "\n",
    "        return upE\n",
    "\n",
    "\n",
    "class SalsaNet(nn.Module):\n",
    "    def __init__(self, ARCH, nclasses, path=None, path_append=\"\", strict=False):\n",
    "        super(SalsaNet, self).__init__()\n",
    "        self.ARCH = ARCH\n",
    "        self.nclasses = nclasses\n",
    "        self.path = path\n",
    "        self.path_append = path_append\n",
    "        self.strict = False\n",
    "\n",
    "        self.downCntx = ResContextBlock(5, 32)\n",
    "        self.resBlock1 = ResBlock(32, 32, 0.2, pooling=True, drop_out=False)\n",
    "        self.resBlock2 = ResBlock(32, 2 * 32, 0.2, pooling=True)\n",
    "        self.resBlock3 = ResBlock(2 * 32, 4 * 32, 0.2, pooling=True)\n",
    "        self.resBlock4 = ResBlock(4 * 32, 8 * 32, 0.2, pooling=True)\n",
    "        self.resBlock5 = ResBlock(8 * 32, 16 * 32, 0.2, pooling=True)\n",
    "        self.resBlock6 = ResBlock(16 * 32, 16 * 32, 0.2, pooling=False)\n",
    "\n",
    "        self.upBlock1 = UpBlock(16 * 32, 16 * 32, 0.2)\n",
    "        self.upBlock2 = UpBlock(16 * 32, 8 * 32, 0.2)\n",
    "        self.upBlock3 = UpBlock(8 * 32, 4 * 32, 0.2)\n",
    "        self.upBlock4 = UpBlock(4 * 32, 2 * 32, 0.2)\n",
    "        self.upBlock5 = UpBlock(2 * 32, 32, 0.2, drop_out=False)\n",
    "\n",
    "        self.logits = nn.Conv2d(32, nclasses, kernel_size=(1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        downCntx = self.downCntx(x)\n",
    "        down0c, down0b = self.resBlock1(downCntx)\n",
    "        down1c, down1b = self.resBlock2(down0c)\n",
    "        down2c, down2b = self.resBlock3(down1c)\n",
    "        down3c, down3b = self.resBlock4(down2c)\n",
    "        down4c, down4b = self.resBlock5(down3c)\n",
    "        down5b = self.resBlock6(down4c)\n",
    "\n",
    "        up4e = self.upBlock1(down5b, down4b)\n",
    "        up3e = self.upBlock2(up4e, down3b)\n",
    "        up2e = self.upBlock3(up3e, down2b)\n",
    "        up1e = self.upBlock4(up2e, down1b)\n",
    "        up0e = self.upBlock5(up1e, down0b)\n",
    "\n",
    "        logits = self.logits(up0e)\n",
    "        logits = F.softmax(logits, dim=1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lovasz_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2018 Maxim Berman\n",
    "Copyright (c) 2020 Tiago Cortinhal, George Tzelepis and Eren Erdal Aksoy\n",
    "\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def isnan(x):\n",
    "    return x != x\n",
    "\n",
    "\n",
    "def mean(l, ignore_nan=False, empty=0):\n",
    "    \"\"\"\n",
    "    nanmean compatible with generators.\n",
    "    \"\"\"\n",
    "    l = iter(l)\n",
    "    if ignore_nan:\n",
    "        l = ifilterfalse(isnan, l)\n",
    "    try:\n",
    "        n = 1\n",
    "        acc = next(l)\n",
    "    except StopIteration:\n",
    "        if empty == 'raise':\n",
    "            raise ValueError('Empty mean')\n",
    "        return empty\n",
    "    for n, v in enumerate(l, 2):\n",
    "        acc += v\n",
    "    if n == 1:\n",
    "        return acc\n",
    "    return acc / n\n",
    "\n",
    "\n",
    "def lovasz_grad(gt_sorted):\n",
    "    \"\"\"\n",
    "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
    "    See Alg. 1 in paper\n",
    "    \"\"\"\n",
    "    p = len(gt_sorted)\n",
    "    gts = gt_sorted.sum()\n",
    "    intersection = gts - gt_sorted.float().cumsum(0)\n",
    "    union = gts + (1 - gt_sorted).float().cumsum(0)\n",
    "    jaccard = 1. - intersection / union\n",
    "    if p > 1:  # cover 1-pixel case\n",
    "        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n",
    "    return jaccard\n",
    "\n",
    "\n",
    "def lovasz_softmax(probas, labels, classes='present', per_image=False, ignore=None):\n",
    "    \"\"\"\n",
    "    Multi-class Lovasz-Softmax loss\n",
    "      probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1).\n",
    "              Interpreted as binary (sigmoid) output with outputs of size [B, H, W].\n",
    "      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n",
    "      classes: 'all' for all, 'present' for classes present in labels, or a list of classes to average.\n",
    "      per_image: compute the loss per image instead of per batch\n",
    "      ignore: void class labels\n",
    "    \"\"\"\n",
    "    if per_image:\n",
    "        loss = mean(lovasz_softmax_flat(*flatten_probas(prob.unsqueeze(0), lab.unsqueeze(0), ignore), classes=classes)\n",
    "                    for prob, lab in zip(probas, labels))\n",
    "    else:\n",
    "        loss = lovasz_softmax_flat(*flatten_probas(probas, labels, ignore), classes=classes)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lovasz_softmax_flat(probas, labels, classes='present'):\n",
    "    \"\"\"\n",
    "    Multi-class Lovasz-Softmax loss\n",
    "      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n",
    "      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n",
    "      classes: 'all' for all, 'present' for classes present in labels, or a list of classes to average.\n",
    "    \"\"\"\n",
    "    if probas.numel() == 0:\n",
    "        # only void pixels, the gradients should be 0\n",
    "        return probas * 0.\n",
    "    C = probas.size(1)\n",
    "    losses = []\n",
    "    class_to_sum = list(range(C)) if classes in ['all', 'present'] else classes\n",
    "    for c in class_to_sum:\n",
    "        fg = (labels == c).float()  # foreground for class c\n",
    "        if (classes is 'present' and fg.sum() == 0):\n",
    "            continue\n",
    "        if C == 1:\n",
    "            if len(classes) > 1:\n",
    "                raise ValueError('Sigmoid output possible only with 1 class')\n",
    "            class_pred = probas[:, 0]\n",
    "        else:\n",
    "            class_pred = probas[:, c]\n",
    "        errors = (Variable(fg) - class_pred).abs()\n",
    "        errors_sorted, perm = torch.sort(errors, 0, descending=True)\n",
    "        perm = perm.data\n",
    "        fg_sorted = fg[perm]\n",
    "        losses.append(torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted))))\n",
    "    return mean(losses)\n",
    "\n",
    "\n",
    "def flatten_probas(probas, labels, ignore=None):\n",
    "    \"\"\"\n",
    "    Flattens predictions in the batch\n",
    "    \"\"\"\n",
    "    if probas.dim() == 3:\n",
    "        # assumes output of a sigmoid layer\n",
    "        B, H, W = probas.size()\n",
    "        probas = probas.view(B, 1, H, W)\n",
    "    B, C, H, W = probas.size()\n",
    "    probas = probas.permute(0, 2, 3, 1).contiguous().view(-1, C)  # B * H * W, C = P, C\n",
    "    labels = labels.view(-1)\n",
    "    if ignore is None:\n",
    "        return probas, labels\n",
    "    valid = (labels != ignore)\n",
    "    vprobas = probas[valid.nonzero().squeeze()]\n",
    "    vlabels = labels[valid]\n",
    "    return vprobas, vlabels\n",
    "\n",
    "\n",
    "class Lovasz_softmax(nn.Module):\n",
    "    def __init__(self, classes='present', per_image=False, ignore=None):\n",
    "        super(Lovasz_softmax, self).__init__()\n",
    "        self.classes = classes\n",
    "        self.per_image = per_image\n",
    "        self.ignore = ignore\n",
    "\n",
    "    def forward(self, probas, labels):\n",
    "        return lovasz_softmax(probas, labels, self.classes, self.per_image, self.ignore)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### laserscan.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaserScan:\n",
    "    \"\"\"Class that contains LaserScan with x,y,z,r\"\"\"\n",
    "    EXTENSIONS_SCAN = ['.bin']\n",
    "\n",
    "    def __init__(self, project=False, H=64, W=1024, fov_up=3.0, fov_down=-25.0):\n",
    "        self.project = project\n",
    "        self.proj_H = H\n",
    "        self.proj_W = W\n",
    "        self.proj_fov_up = fov_up\n",
    "        self.proj_fov_down = fov_down\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Reset scan members. \"\"\"\n",
    "        self.points = np.zeros((0, 3), dtype=np.float32)  # [m, 3]: x, y, z\n",
    "        self.remissions = np.zeros((0, 1), dtype=np.float32)  # [m ,1]: remission\n",
    "\n",
    "        # projected range image - [H,W] range (-1 is no data)\n",
    "        self.proj_range = np.full((self.proj_H, self.proj_W), -1,\n",
    "                                  dtype=np.float32)\n",
    "\n",
    "        # unprojected range (list of depths for each point)\n",
    "        self.unproj_range = np.zeros((0, 1), dtype=np.float32)\n",
    "\n",
    "        # projected point cloud xyz - [H,W,3] xyz coord (-1 is no data)\n",
    "        self.proj_xyz = np.full((self.proj_H, self.proj_W, 3), -1,\n",
    "                                dtype=np.float32)\n",
    "\n",
    "        # projected remission - [H,W] intensity (-1 is no data)\n",
    "        self.proj_remission = np.full((self.proj_H, self.proj_W), -1,\n",
    "                                      dtype=np.float32)\n",
    "\n",
    "        # projected index (for each pixel, what I am in the pointcloud)\n",
    "        # [H,W] index (-1 is no data)\n",
    "        self.proj_idx = np.full((self.proj_H, self.proj_W), -1,\n",
    "                                dtype=np.int32)\n",
    "\n",
    "        # for each point, where it is in the range image\n",
    "        self.proj_x = np.zeros((0, 1), dtype=np.int32)  # [m, 1]: x\n",
    "        self.proj_y = np.zeros((0, 1), dtype=np.int32)  # [m, 1]: y\n",
    "\n",
    "        # mask containing for each pixel, if it contains a point or not\n",
    "        self.proj_mask = np.zeros((self.proj_H, self.proj_W),\n",
    "                                  dtype=np.int32)  # [H,W] mask\n",
    "\n",
    "    def size(self):\n",
    "        \"\"\" Return the size of the point cloud. \"\"\"\n",
    "        return self.points.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size()\n",
    "\n",
    "    def open_scan(self, filename):\n",
    "        \"\"\" Open raw scan and fill in attributes\n",
    "        \"\"\"\n",
    "        # reset just in case there was an open structure\n",
    "        self.reset()\n",
    "\n",
    "        # check filename is string\n",
    "        if not isinstance(filename, str):\n",
    "            raise TypeError(\"Filename should be string type, \"\n",
    "                            \"but was {type}\".format(type=str(type(filename))))\n",
    "\n",
    "        # check extension is a laserscan\n",
    "        if not any(filename.endswith(ext) for ext in self.EXTENSIONS_SCAN):\n",
    "            raise RuntimeError(\"Filename extension is not valid scan file.\")\n",
    "\n",
    "        # if all goes well, open pointcloud\n",
    "        scan = np.fromfile(filename, dtype=np.float32)\n",
    "        scan = scan.reshape((-1, 4))\n",
    "\n",
    "        # put in attribute\n",
    "        points = scan[:, 0:3]  # get xyz\n",
    "        remissions = scan[:, 3]  # get remission\n",
    "        self.set_points(points, remissions)\n",
    "\n",
    "    def set_points(self, points, remissions=None):\n",
    "        \"\"\" Set scan attributes (instead of opening from file)\n",
    "        \"\"\"\n",
    "        # reset just in case there was an open structure\n",
    "        self.reset()\n",
    "\n",
    "        # check scan makes sense\n",
    "        if not isinstance(points, np.ndarray):\n",
    "            raise TypeError(\"Scan should be numpy array\")\n",
    "\n",
    "        # check remission makes sense\n",
    "        if remissions is not None and not isinstance(remissions, np.ndarray):\n",
    "            raise TypeError(\"Remissions should be numpy array\")\n",
    "\n",
    "        # put in attribute\n",
    "        self.points = points  # get xyz\n",
    "        if remissions is not None:\n",
    "            self.remissions = remissions  # get remission\n",
    "        else:\n",
    "            self.remissions = np.zeros((points.shape[0]), dtype=np.float32)\n",
    "\n",
    "        # if projection is wanted, then do it and fill in the structure\n",
    "        if self.project:\n",
    "            self.do_range_projection()\n",
    "\n",
    "    def do_range_projection(self):\n",
    "        \"\"\" Project a pointcloud into a spherical projection image.projection.\n",
    "            Function takes no arguments because it can be also called externally\n",
    "            if the value of the constructor was not set (in case you change your\n",
    "            mind about wanting the projection)\n",
    "        \"\"\"\n",
    "        # laser parameters\n",
    "        fov_up = self.proj_fov_up / 180.0 * np.pi  # field of view up in rad\n",
    "        fov_down = self.proj_fov_down / 180.0 * np.pi  # field of view down in rad\n",
    "        fov = abs(fov_down) + abs(fov_up)  # get field of view total in rad\n",
    "\n",
    "        # get depth of all points\n",
    "        depth = np.linalg.norm(self.points, 2, axis=1)\n",
    "\n",
    "        # get scan components\n",
    "        scan_x = self.points[:, 0]\n",
    "        scan_y = self.points[:, 1]\n",
    "        scan_z = self.points[:, 2]\n",
    "\n",
    "        # get angles of all points\n",
    "        yaw = -np.arctan2(scan_y, scan_x)\n",
    "        pitch = np.arcsin(scan_z / depth)\n",
    "\n",
    "        # get projections in image coords\n",
    "        proj_x = 0.5 * (yaw / np.pi + 1.0)  # in [0.0, 1.0]\n",
    "        proj_y = 1.0 - (pitch + abs(fov_down)) / fov  # in [0.0, 1.0]\n",
    "\n",
    "        # scale to image size using angular resolution\n",
    "        proj_x *= self.proj_W  # in [0.0, W]\n",
    "        proj_y *= self.proj_H  # in [0.0, H]\n",
    "\n",
    "        # round and clamp for use as index\n",
    "        proj_x = np.floor(proj_x)\n",
    "        proj_x = np.minimum(self.proj_W - 1, proj_x)\n",
    "        proj_x = np.maximum(0, proj_x).astype(np.int32)  # in [0,W-1]\n",
    "        self.proj_x = np.copy(proj_x)  # store a copy in orig order\n",
    "\n",
    "        proj_y = np.floor(proj_y)\n",
    "        proj_y = np.minimum(self.proj_H - 1, proj_y)\n",
    "        proj_y = np.maximum(0, proj_y).astype(np.int32)  # in [0,H-1]\n",
    "        self.proj_y = np.copy(proj_y)  # stope a copy in original order\n",
    "\n",
    "        # copy of depth in original order\n",
    "        self.unproj_range = np.copy(depth)\n",
    "\n",
    "        # order in decreasing depth\n",
    "        indices = np.arange(depth.shape[0])\n",
    "        order = np.argsort(depth)[::-1]\n",
    "        depth = depth[order]\n",
    "        indices = indices[order]\n",
    "        points = self.points[order]\n",
    "        remission = self.remissions[order]\n",
    "        proj_y = proj_y[order]\n",
    "        proj_x = proj_x[order]\n",
    "\n",
    "        # assing to images\n",
    "        self.proj_range[proj_y, proj_x] = depth\n",
    "        self.proj_xyz[proj_y, proj_x] = points\n",
    "        self.proj_remission[proj_y, proj_x] = remission\n",
    "        self.proj_idx[proj_y, proj_x] = indices\n",
    "        self.proj_mask = (self.proj_idx > 0).astype(np.int32)\n",
    "\n",
    "\n",
    "class SemLaserScan(LaserScan):\n",
    "    \"\"\"Class that contains LaserScan with x,y,z,r,sem_label,sem_color_label,inst_label,inst_color_label\"\"\"\n",
    "    EXTENSIONS_LABEL = ['.label']\n",
    "\n",
    "    def __init__(self, sem_color_dict=None, project=False, H=64, W=1024, fov_up=3.0, fov_down=-25.0, max_classes=300):\n",
    "        super(SemLaserScan, self).__init__(project, H, W, fov_up, fov_down)\n",
    "        self.reset()\n",
    "\n",
    "        # make semantic colors\n",
    "        if sem_color_dict:\n",
    "            # if I have a dict, make it\n",
    "            max_sem_key = 0\n",
    "            for key, data in sem_color_dict.items():\n",
    "                if key + 1 > max_sem_key:\n",
    "                    max_sem_key = key + 1\n",
    "            self.sem_color_lut = np.zeros((max_sem_key + 100, 3), dtype=np.float32)\n",
    "            for key, value in sem_color_dict.items():\n",
    "                self.sem_color_lut[key] = np.array(value, np.float32) / 255.0\n",
    "        else:\n",
    "            # otherwise make random\n",
    "            max_sem_key = max_classes\n",
    "            self.sem_color_lut = np.random.uniform(low=0.0,\n",
    "                                                   high=1.0,\n",
    "                                                   size=(max_sem_key, 3))\n",
    "            # force zero to a gray-ish color\n",
    "            self.sem_color_lut[0] = np.full((3), 0.1)\n",
    "\n",
    "        # make instance colors\n",
    "        max_inst_id = 100000\n",
    "        self.inst_color_lut = np.random.uniform(low=0.0,\n",
    "                                                high=1.0,\n",
    "                                                size=(max_inst_id, 3))\n",
    "        # force zero to a gray-ish color\n",
    "        self.inst_color_lut[0] = np.full((3), 0.1)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Reset scan members. \"\"\"\n",
    "        super(SemLaserScan, self).reset()\n",
    "\n",
    "        # semantic labels\n",
    "        self.sem_label = np.zeros((0, 1), dtype=np.int32)  # [m, 1]: label\n",
    "        self.sem_label_color = np.zeros((0, 3), dtype=np.float32)  # [m ,3]: color\n",
    "\n",
    "        # instance labels\n",
    "        self.inst_label = np.zeros((0, 1), dtype=np.int32)  # [m, 1]: label\n",
    "        self.inst_label_color = np.zeros((0, 3), dtype=np.float32)  # [m ,3]: color\n",
    "\n",
    "        # projection color with semantic labels\n",
    "        self.proj_sem_label = np.zeros((self.proj_H, self.proj_W),\n",
    "                                       dtype=np.int32)  # [H,W]  label\n",
    "        self.proj_sem_color = np.zeros((self.proj_H, self.proj_W, 3),\n",
    "                                       dtype=np.float)  # [H,W,3] color\n",
    "\n",
    "        # projection color with instance labels\n",
    "        self.proj_inst_label = np.zeros((self.proj_H, self.proj_W),\n",
    "                                        dtype=np.int32)  # [H,W]  label\n",
    "        self.proj_inst_color = np.zeros((self.proj_H, self.proj_W, 3),\n",
    "                                        dtype=np.float)  # [H,W,3] color\n",
    "\n",
    "    def open_label(self, filename):\n",
    "        \"\"\" Open raw scan and fill in attributes\n",
    "        \"\"\"\n",
    "        # check filename is string\n",
    "        if not isinstance(filename, str):\n",
    "            raise TypeError(\"Filename should be string type, \"\n",
    "                            \"but was {type}\".format(type=str(type(filename))))\n",
    "\n",
    "        # check extension is a laserscan\n",
    "        if not any(filename.endswith(ext) for ext in self.EXTENSIONS_LABEL):\n",
    "            raise RuntimeError(\"Filename extension is not valid label file.\")\n",
    "\n",
    "        # if all goes well, open label\n",
    "        label = np.fromfile(filename, dtype=np.int32)\n",
    "        label = label.reshape((-1))\n",
    "\n",
    "        # set it\n",
    "        self.set_label(label)\n",
    "\n",
    "    def set_label(self, label):\n",
    "        \"\"\" Set points for label not from file but from np\n",
    "        \"\"\"\n",
    "        # check label makes sense\n",
    "        if not isinstance(label, np.ndarray):\n",
    "            raise TypeError(\"Label should be numpy array\")\n",
    "\n",
    "        # only fill in attribute if the right size\n",
    "        if label.shape[0] == self.points.shape[0]:\n",
    "            self.sem_label = label & 0xFFFF  # semantic label in lower half\n",
    "            self.inst_label = label >> 16  # instance id in upper half\n",
    "        else:\n",
    "            print(\"Points shape: \", self.points.shape)\n",
    "            print(\"Label shape: \", label.shape)\n",
    "            raise ValueError(\"Scan and Label don't contain same number of points\")\n",
    "\n",
    "        # sanity check\n",
    "        assert ((self.sem_label + (self.inst_label << 16) == label).all())\n",
    "\n",
    "        if self.project:\n",
    "            self.do_label_projection()\n",
    "\n",
    "    def colorize(self):\n",
    "        \"\"\" Colorize pointcloud with the color of each semantic label\n",
    "        \"\"\"\n",
    "        self.sem_label_color = self.sem_color_lut[self.sem_label]\n",
    "        self.sem_label_color = self.sem_label_color.reshape((-1, 3))\n",
    "\n",
    "        self.inst_label_color = self.inst_color_lut[self.inst_label]\n",
    "        self.inst_label_color = self.inst_label_color.reshape((-1, 3))\n",
    "\n",
    "    def do_label_projection(self):\n",
    "        # only map colors to labels that exist\n",
    "        mask = self.proj_idx >= 0\n",
    "\n",
    "        # semantics\n",
    "        self.proj_sem_label[mask] = self.sem_label[self.proj_idx[mask]]\n",
    "        self.proj_sem_color[mask] = self.sem_color_lut[self.sem_label[self.proj_idx[mask]]]\n",
    "\n",
    "        # instances\n",
    "        self.proj_inst_label[mask] = self.inst_label[self.proj_idx[mask]]\n",
    "        self.proj_inst_color[mask] = self.inst_color_lut[self.inst_label[self.proj_idx[mask]]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parser.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTENSIONS_SCAN = ['.bin']\n",
    "EXTENSIONS_LABEL = ['.label']\n",
    "\n",
    "def get_sync_time():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    return time.perf_counter()\n",
    "\n",
    "\n",
    "def is_scan(filename):\n",
    "  return any(filename.endswith(ext) for ext in EXTENSIONS_SCAN)\n",
    "\n",
    "\n",
    "def is_label(filename):\n",
    "  return any(filename.endswith(ext) for ext in EXTENSIONS_LABEL)\n",
    "\n",
    "\n",
    "class SemanticKitti(Dataset):\n",
    "  def __init__(self, root,          # directory where data is\n",
    "               sequences,           # sequences for this data (e.g. [1,3,4,6])\n",
    "               labels,              # label dict: (e.g 10: \"car\")\n",
    "               color_map,           # colors dict bgr (e.g 10: [255, 0, 0])\n",
    "               learning_map,        # classes to learn (0 to N-1 for xentropy)\n",
    "               learning_map_inv,    # inverse of previous (recover labels)\n",
    "               sensor,              # sensor to parse scans from\n",
    "               max_points=150000,   # max number of points present in dataset\n",
    "               gt=True):            # send ground truth?\n",
    "    # save deats\n",
    "    self.root = os.path.join(root, \"sequences\")\n",
    "    self.sequences = sequences\n",
    "    self.labels = labels\n",
    "    self.color_map = color_map\n",
    "    self.learning_map = learning_map\n",
    "    self.learning_map_inv = learning_map_inv\n",
    "    self.sensor = sensor\n",
    "    self.sensor_img_H = sensor[\"img_prop\"][\"height\"]\n",
    "    self.sensor_img_W = sensor[\"img_prop\"][\"width\"]\n",
    "    self.sensor_img_means = torch.tensor(sensor[\"img_means\"],\n",
    "                                         dtype=torch.float)\n",
    "    self.sensor_img_stds = torch.tensor(sensor[\"img_stds\"],\n",
    "                                        dtype=torch.float)\n",
    "    self.sensor_fov_up = sensor[\"fov_up\"]\n",
    "    self.sensor_fov_down = sensor[\"fov_down\"]\n",
    "    self.max_points = max_points\n",
    "    self.gt = gt\n",
    "\n",
    "    # get number of classes (can't be len(self.learning_map) because there\n",
    "    # are multiple repeated entries, so the number that matters is how many\n",
    "    # there are for the xentropy)\n",
    "    self.nclasses = len(self.learning_map_inv)\n",
    "\n",
    "    # sanity checks\n",
    "\n",
    "    # make sure directory exists\n",
    "    if os.path.isdir(self.root):\n",
    "      print(\"Sequences folder exists! Using sequences from %s\" % self.root)\n",
    "    else:\n",
    "      raise ValueError(\"Sequences folder doesn't exist! Exiting...\")\n",
    "\n",
    "    # make sure labels is a dict\n",
    "    assert(isinstance(self.labels, dict))\n",
    "\n",
    "    # make sure color_map is a dict\n",
    "    assert(isinstance(self.color_map, dict))\n",
    "\n",
    "    # make sure learning_map is a dict\n",
    "    assert(isinstance(self.learning_map, dict))\n",
    "\n",
    "    # make sure sequences is a list\n",
    "    assert(isinstance(self.sequences, list))\n",
    "\n",
    "    # placeholder for filenames\n",
    "    self.scan_files = []\n",
    "    self.label_files = []\n",
    "\n",
    "    # fill in with names, checking that all sequences are complete\n",
    "    for seq in self.sequences:\n",
    "      # to string\n",
    "      seq = '{0:02d}'.format(int(seq))\n",
    "\n",
    "      print(\"parsing seq {}\".format(seq))\n",
    "\n",
    "      # get paths for each\n",
    "      scan_path = os.path.join(self.root, seq, \"velodyne\")\n",
    "      label_path = os.path.join(self.root, seq, \"labels\")\n",
    "\n",
    "      # get files\n",
    "      scan_files = [os.path.join(dp, f) for dp, dn, fn in os.walk(\n",
    "          os.path.expanduser(scan_path)) for f in fn if is_scan(f)]\n",
    "      label_files = [os.path.join(dp, f) for dp, dn, fn in os.walk(\n",
    "          os.path.expanduser(label_path)) for f in fn if is_label(f)]\n",
    "\n",
    "      # check all scans have labels\n",
    "      if self.gt:\n",
    "        assert(len(scan_files) == len(label_files))\n",
    "\n",
    "      # extend list\n",
    "      self.scan_files.extend(scan_files)\n",
    "      self.label_files.extend(label_files)\n",
    "\n",
    "    # sort for correspondance\n",
    "    self.scan_files.sort()\n",
    "    self.label_files.sort()\n",
    "\n",
    "    print(\"Using {} scans from sequences {}\".format(len(self.scan_files),\n",
    "                                                    self.sequences))\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # get item in tensor shape\n",
    "    scan_file = self.scan_files[index]\n",
    "    if self.gt:\n",
    "      label_file = self.label_files[index]\n",
    "\n",
    "    # open a semantic laserscan\n",
    "    if self.gt:\n",
    "      scan = SemLaserScan(self.color_map,\n",
    "                          project=True,\n",
    "                          H=self.sensor_img_H,\n",
    "                          W=self.sensor_img_W,\n",
    "                          fov_up=self.sensor_fov_up,\n",
    "                          fov_down=self.sensor_fov_down)\n",
    "    else:\n",
    "      scan = LaserScan(project=True,\n",
    "                       H=self.sensor_img_H,\n",
    "                       W=self.sensor_img_W,\n",
    "                       fov_up=self.sensor_fov_up,\n",
    "                       fov_down=self.sensor_fov_down)\n",
    "\n",
    "    # open and obtain scan\n",
    "    scan.open_scan(scan_file)\n",
    "    if self.gt:\n",
    "      scan.open_label(label_file)\n",
    "      # map unused classes to used classes (also for projection)\n",
    "      scan.sem_label = self.map(scan.sem_label, self.learning_map)\n",
    "      scan.proj_sem_label = self.map(scan.proj_sem_label, self.learning_map)\n",
    "\n",
    "    # PROJECTION TIME START\n",
    "    proj_time_start = get_sync_time()\n",
    "    \n",
    "    # make a tensor of the uncompressed data (with the max num points)\n",
    "    unproj_n_points = scan.points.shape[0]\n",
    "    unproj_xyz = torch.full((self.max_points, 3), -1.0, dtype=torch.float)\n",
    "    unproj_xyz[:unproj_n_points] = torch.from_numpy(scan.points)\n",
    "    unproj_range = torch.full([self.max_points], -1.0, dtype=torch.float)\n",
    "    unproj_range[:unproj_n_points] = torch.from_numpy(scan.unproj_range)\n",
    "    unproj_remissions = torch.full([self.max_points], -1.0, dtype=torch.float)\n",
    "    unproj_remissions[:unproj_n_points] = torch.from_numpy(scan.remissions)\n",
    "    if self.gt:\n",
    "      unproj_labels = torch.full([self.max_points], -1.0, dtype=torch.int32)\n",
    "      unproj_labels[:unproj_n_points] = torch.from_numpy(scan.sem_label)\n",
    "    else:\n",
    "      unproj_labels = []\n",
    "\n",
    "    # get points and labels\n",
    "    proj_range = torch.from_numpy(scan.proj_range).clone()\n",
    "    proj_xyz = torch.from_numpy(scan.proj_xyz).clone()\n",
    "    proj_remission = torch.from_numpy(scan.proj_remission).clone()\n",
    "    proj_mask = torch.from_numpy(scan.proj_mask)\n",
    "    if self.gt:\n",
    "      proj_labels = torch.from_numpy(scan.proj_sem_label).clone()\n",
    "      proj_labels = proj_labels * proj_mask\n",
    "    else:\n",
    "      proj_labels = []\n",
    "    proj_x = torch.full([self.max_points], -1, dtype=torch.long)\n",
    "    proj_x[:unproj_n_points] = torch.from_numpy(scan.proj_x)\n",
    "    proj_y = torch.full([self.max_points], -1, dtype=torch.long)\n",
    "    proj_y[:unproj_n_points] = torch.from_numpy(scan.proj_y)\n",
    "    proj = torch.cat([proj_range.unsqueeze(0).clone(),\n",
    "                      proj_xyz.clone().permute(2, 0, 1),\n",
    "                      proj_remission.unsqueeze(0).clone()])\n",
    "    proj = (proj - self.sensor_img_means[:, None, None]\n",
    "            ) / self.sensor_img_stds[:, None, None]\n",
    "    proj = proj * proj_mask.float()\n",
    "\n",
    "    # get name and sequence\n",
    "    path_norm = os.path.normpath(scan_file)\n",
    "    path_split = path_norm.split(os.sep)\n",
    "    path_seq = path_split[-3]\n",
    "    path_name = path_split[-1].replace(\".bin\", \".label\")\n",
    "    # print(\"path_norm: \", path_norm)\n",
    "    # print(\"path_seq\", path_seq)\n",
    "    # print(\"path_name\", path_name)\n",
    "\n",
    "    # FIXME: projections are created in the opening function!\n",
    "    # PROJECTION TIME END\n",
    "    proj_time_end = get_sync_time()\n",
    "    proj_time = proj_time_end - proj_time_start\n",
    "\n",
    "    # return\n",
    "    return ( proj,\n",
    "             proj_mask,\n",
    "             proj_labels,\n",
    "             unproj_labels,\n",
    "             path_seq,\n",
    "             path_name,\n",
    "             proj_x,\n",
    "             proj_y,\n",
    "             proj_range,\n",
    "             unproj_range,\n",
    "             proj_xyz,\n",
    "             unproj_xyz,\n",
    "             proj_remission,\n",
    "             unproj_remissions,\n",
    "             unproj_n_points,\n",
    "             proj_time )\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.scan_files)\n",
    "\n",
    "  @staticmethod\n",
    "  def map(label, mapdict):\n",
    "    # put label from original values to xentropy\n",
    "    # or vice-versa, depending on dictionary values\n",
    "    # make learning map a lookup table\n",
    "    maxkey = 0\n",
    "    for key, data in mapdict.items():\n",
    "      if isinstance(data, list):\n",
    "        nel = len(data)\n",
    "      else:\n",
    "        nel = 1\n",
    "      if key > maxkey:\n",
    "        maxkey = key\n",
    "    # +100 hack making lut bigger just in case there are unknown labels\n",
    "    if nel > 1:\n",
    "      lut = np.zeros((maxkey + 100, nel), dtype=np.int32)\n",
    "    else:\n",
    "      lut = np.zeros((maxkey + 100), dtype=np.int32)\n",
    "    for key, data in mapdict.items():\n",
    "      try:\n",
    "        lut[key] = data\n",
    "      except IndexError:\n",
    "        print(\"Wrong key \", key)\n",
    "    # do the mapping\n",
    "    return lut[label]\n",
    "\n",
    "\n",
    "class Parser():\n",
    "  # standard conv, BN, relu\n",
    "  def __init__(self,\n",
    "               root,                 # directory for data\n",
    "               train_sequences,      # sequences to train\n",
    "               valid_sequences,      # sequences to validate.\n",
    "               test_sequences,       # sequences to test (if none, don't get)\n",
    "               labels,               # labels in data\n",
    "               color_map,            # color for each label\n",
    "               learning_map,         # mapping for training labels\n",
    "               learning_map_inv,     # recover labels from xentropy\n",
    "               sensor,               # sensor to use\n",
    "               max_points,           # max points in each scan in entire dataset\n",
    "               batch_size,           # batch size for train and val\n",
    "               workers,              # threads to load data\n",
    "               gt=True,              # get gt?\n",
    "               shuffle_train=True):  # shuffle training set?\n",
    "    super(Parser, self).__init__()\n",
    "\n",
    "    # if I am training, get the dataset\n",
    "    self.root = root\n",
    "    self.train_sequences = train_sequences\n",
    "    self.valid_sequences = valid_sequences\n",
    "    self.test_sequences = test_sequences\n",
    "    self.labels = labels\n",
    "    self.color_map = color_map\n",
    "    self.learning_map = learning_map\n",
    "    self.learning_map_inv = learning_map_inv\n",
    "    self.sensor = sensor\n",
    "    self.max_points = max_points\n",
    "    self.batch_size = batch_size\n",
    "    self.workers = workers\n",
    "    self.gt = gt\n",
    "    self.shuffle_train = shuffle_train\n",
    "\n",
    "    # number of classes that matters is the one for xentropy\n",
    "    self.nclasses = len(self.learning_map_inv)\n",
    "\n",
    "    # Train dataset\n",
    "    self.train_dataset = SemanticKitti(root=self.root,\n",
    "                                       sequences=self.train_sequences,\n",
    "                                       labels=self.labels,\n",
    "                                       color_map=self.color_map,\n",
    "                                       learning_map=self.learning_map,\n",
    "                                       learning_map_inv=self.learning_map_inv,\n",
    "                                       sensor=self.sensor,\n",
    "                                       max_points=max_points,\n",
    "                                       gt=self.gt)\n",
    "\n",
    "    self.trainloader = torch.utils.data.DataLoader(self.train_dataset,\n",
    "                                                   batch_size=self.batch_size,\n",
    "                                                   shuffle=self.shuffle_train,\n",
    "                                                   num_workers=self.workers,\n",
    "                                                   drop_last=True)\n",
    "    assert len(self.trainloader) > 0\n",
    "    self.trainiter = iter(self.trainloader)\n",
    "\n",
    "    # Valid dataset\n",
    "    self.valid_dataset = SemanticKitti(root=self.root,\n",
    "                                       sequences=self.valid_sequences,\n",
    "                                       labels=self.labels,\n",
    "                                       color_map=self.color_map,\n",
    "                                       learning_map=self.learning_map,\n",
    "                                       learning_map_inv=self.learning_map_inv,\n",
    "                                       sensor=self.sensor,\n",
    "                                       max_points=max_points,\n",
    "                                       gt=self.gt)\n",
    "\n",
    "    self.validloader = torch.utils.data.DataLoader(self.valid_dataset,\n",
    "                                                   batch_size=self.batch_size,\n",
    "                                                   shuffle=False,\n",
    "                                                   num_workers=self.workers,\n",
    "                                                   drop_last=True)\n",
    "    assert len(self.validloader) > 0\n",
    "    self.validiter = iter(self.validloader)\n",
    "\n",
    "    # Test dataset\n",
    "    if self.test_sequences:\n",
    "      self.test_dataset = SemanticKitti(root=self.root,\n",
    "                                        sequences=self.test_sequences,\n",
    "                                        labels=self.labels,\n",
    "                                        color_map=self.color_map,\n",
    "                                        learning_map=self.learning_map,\n",
    "                                        learning_map_inv=self.learning_map_inv,\n",
    "                                        sensor=self.sensor,\n",
    "                                        max_points=max_points,\n",
    "                                        gt=False)\n",
    "\n",
    "      self.testloader = torch.utils.data.DataLoader(self.test_dataset,\n",
    "                                                    batch_size=self.batch_size,\n",
    "                                                    shuffle=False,\n",
    "                                                    num_workers=self.workers,\n",
    "                                                    drop_last=True)\n",
    "      assert len(self.testloader) > 0\n",
    "      self.testiter = iter(self.testloader)\n",
    "\n",
    "  def get_train_batch(self):\n",
    "    scans = self.trainiter.next()\n",
    "    return scans\n",
    "\n",
    "  def get_train_set(self):\n",
    "    return self.trainloader\n",
    "\n",
    "  def get_valid_batch(self):\n",
    "    scans = self.validiter.next()\n",
    "    return scans\n",
    "\n",
    "  def get_valid_set(self):\n",
    "    return self.validloader\n",
    "\n",
    "  def get_test_batch(self):\n",
    "    scans = self.testiter.next()\n",
    "    return scans\n",
    "\n",
    "  def get_test_set(self):\n",
    "    return self.testloader\n",
    "\n",
    "  def get_train_size(self):\n",
    "    return len(self.trainloader)\n",
    "\n",
    "  def get_valid_size(self):\n",
    "    return len(self.validloader)\n",
    "\n",
    "  def get_test_size(self):\n",
    "    return len(self.testloader)\n",
    "\n",
    "  def get_n_classes(self):\n",
    "    return self.nclasses\n",
    "\n",
    "  def get_original_class_string(self, idx):\n",
    "    return self.labels[idx]\n",
    "\n",
    "  def get_xentropy_class_string(self, idx):\n",
    "    return self.labels[self.learning_map_inv[idx]]\n",
    "\n",
    "  def to_original(self, label):\n",
    "    # put label in original values\n",
    "    return SemanticKitti.map(label, self.learning_map_inv)\n",
    "\n",
    "  def to_xentropy(self, label):\n",
    "    # put label in xentropy values\n",
    "    return SemanticKitti.map(label, self.learning_map)\n",
    "\n",
    "  def to_color(self, label):\n",
    "    # put label in original values\n",
    "    label = SemanticKitti.map(label, self.learning_map_inv)\n",
    "    # put label in color\n",
    "    return SemanticKitti.map(label, self.color_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heteroscedastic_loss(true, mean, log_var):\n",
    "    precision = torch.exp(-log_var)\n",
    "    sum = torch.sum(precision * (true - mean)**2 + log_var, 1)\n",
    "    mean = torch.mean(sum, 0)\n",
    "    return mean\n",
    "\n",
    "\n",
    "def save_to_log(logdir, logfile, message):\n",
    "    f = open(logdir + '/' + logfile, \"a\")\n",
    "    f.write(message + '\\n')\n",
    "    f.close()\n",
    "    return\n",
    "\n",
    "\n",
    "def save_checkpoint(to_save, logdir, suffix=\"\"):\n",
    "    # Save the weights\n",
    "    torch.save(to_save, logdir +\n",
    "               \"/SalsaNet\" + suffix)\n",
    "\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, ARCH, DATA, datadir, logdir, path=None, model_mode='salsanext'):\n",
    "        # parameters\n",
    "        self.ARCH = ARCH\n",
    "        self.DATA = DATA\n",
    "        self.datadir = datadir\n",
    "        self.log = logdir\n",
    "        self.path = path\n",
    "        self.model_mode = model_mode\n",
    "\n",
    "        self.batch_time_t = AverageMeter()\n",
    "        self.data_time_t = AverageMeter()\n",
    "        self.batch_time_e = AverageMeter()\n",
    "        self.epoch = 0\n",
    "\n",
    "        # put logger where it belongs\n",
    "\n",
    "        self.info = {\"train_update\": 0,\n",
    "                     \"train_loss\": 0,\n",
    "                     \"train_acc\": 0,\n",
    "                     \"train_iou\": 0,\n",
    "                     \"valid_loss\": 0,\n",
    "                     \"valid_acc\": 0,\n",
    "                     \"valid_iou\": 0,\n",
    "                     \"best_train_iou\": 0,\n",
    "                     \"best_val_iou\": 0}\n",
    "\n",
    "        # get the data\n",
    "        self.parser = Parser(root=self.datadir,\n",
    "                             train_sequences=self.DATA[\"split\"][\"train\"],\n",
    "                             valid_sequences=self.DATA[\"split\"][\"valid\"],\n",
    "                             test_sequences=None,\n",
    "                             labels=self.DATA[\"labels\"],\n",
    "                             color_map=self.DATA[\"color_map\"],\n",
    "                             learning_map=self.DATA[\"learning_map\"],\n",
    "                             learning_map_inv=self.DATA[\"learning_map_inv\"],\n",
    "                             sensor=self.ARCH[\"dataset\"][\"sensor\"],\n",
    "                             max_points=self.ARCH[\"dataset\"][\"max_points\"],\n",
    "                             batch_size=self.ARCH[\"train\"][\"batch_size\"],\n",
    "                             workers=self.ARCH[\"train\"][\"workers\"],\n",
    "                             gt=True,\n",
    "                             shuffle_train=True)\n",
    "\n",
    "        # weights for loss (and bias)\n",
    "        # weights for loss (and bias)\n",
    "        epsilon_w = self.ARCH[\"train\"][\"epsilon_w\"]\n",
    "        content = torch.zeros(self.parser.get_n_classes(), dtype=torch.float)\n",
    "        for cl, freq in DATA[\"content\"].items():\n",
    "            x_cl = self.parser.to_xentropy(cl)  # map actual class to xentropy class\n",
    "            content[x_cl] += freq\n",
    "        self.loss_w = 1 / (content + epsilon_w)  # get weights\n",
    "        for x_cl, w in enumerate(self.loss_w):  # ignore the ones necessary to ignore\n",
    "            if DATA[\"learning_ignore\"][x_cl]:\n",
    "                # don't weigh\n",
    "                self.loss_w[x_cl] = 0\n",
    "        print(\"Loss weights from content: \", self.loss_w.data)\n",
    "        # concatenate the encoder and the head\n",
    "        with torch.no_grad():\n",
    "            self.model = SalsaNet(self.ARCH,\n",
    "                                  self.parser.get_n_classes(),\n",
    "                                  self.path)\n",
    "\n",
    "        self.tb_logger = Logger(self.log + \"/tb\", self.model)\n",
    "\n",
    "        # GPU?\n",
    "        self.gpu = False\n",
    "        self.multi_gpu = False\n",
    "        self.n_gpus = 0\n",
    "        self.model_single = self.model\n",
    "        pytorch_total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(\"{}: {:,}\".format(name, param.numel()))\n",
    "        print(\"Total of Trainable Parameters: {:,}\".format(pytorch_total_params))\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"Training in device: \", self.device)\n",
    "        if torch.cuda.is_available() and torch.cuda.device_count() > 0:\n",
    "            cudnn.benchmark = True\n",
    "            cudnn.fastest = True\n",
    "            self.gpu = True\n",
    "            self.n_gpus = 1\n",
    "            self.model.cuda()\n",
    "        if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "            print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "            self.model = nn.DataParallel(self.model)  # spread in gpus\n",
    "            self.model = convert_model(self.model).cuda()  # sync batchnorm\n",
    "            self.model_single = self.model.module  # single model to get weight names\n",
    "            self.multi_gpu = True\n",
    "            self.n_gpus = torch.cuda.device_count()\n",
    "\n",
    "\n",
    "        self.criterion = nn.NLLLoss(weight=self.loss_w).to(self.device)\n",
    "        self.ls = Lovasz_softmax(ignore=0).to(self.device)\n",
    "        # loss as dataparallel too (more images in batch)\n",
    "        if self.n_gpus > 1:\n",
    "            self.criterion = nn.DataParallel(self.criterion).cuda()  # spread in gpus\n",
    "            self.ls = nn.DataParallel(self.ls).cuda()\n",
    "        self.optimizer = optim.SGD([{'params': self.model.parameters()}],\n",
    "                                   lr=self.ARCH[\"train\"][\"lr\"],\n",
    "                                   momentum=self.ARCH[\"train\"][\"momentum\"],\n",
    "                                   weight_decay=self.ARCH[\"train\"][\"w_decay\"])\n",
    "\n",
    "        # Use warmup learning rate\n",
    "        # post decay and step sizes come in epochs and we want it in steps\n",
    "        steps_per_epoch = self.parser.get_train_size()\n",
    "        up_steps = int(self.ARCH[\"train\"][\"wup_epochs\"] * steps_per_epoch)\n",
    "        final_decay = self.ARCH[\"train\"][\"lr_decay\"] ** (1 / steps_per_epoch)\n",
    "        self.scheduler = warmupLR(optimizer=self.optimizer,\n",
    "                                  lr=self.ARCH[\"train\"][\"lr\"],\n",
    "                                  warmup_steps=up_steps,\n",
    "                                  momentum=self.ARCH[\"train\"][\"momentum\"],\n",
    "                                  decay=final_decay)\n",
    "\n",
    "        if self.path is not None:\n",
    "            torch.nn.Module.dump_patches = True\n",
    "            w_dict = torch.load(path + \"/SalsaNet\",\n",
    "                                map_location=lambda storage, loc: storage)\n",
    "            self.model.load_state_dict(w_dict['state_dict'], strict=True)\n",
    "            self.optimizer.load_state_dict(w_dict['optimizer'])\n",
    "            self.epoch = w_dict['epoch'] + 1\n",
    "            self.scheduler.load_state_dict(w_dict['scheduler'])\n",
    "            print(\"dict epoch:\", w_dict['epoch'])\n",
    "            self.info = w_dict['info']\n",
    "            print(\"info\", w_dict['info'])\n",
    "\n",
    "    def calculate_estimate(self, epoch, iter):\n",
    "        estimate = int((self.data_time_t.avg + self.batch_time_t.avg) * \\\n",
    "                       (self.parser.get_train_size() * self.ARCH['train']['max_epochs'] - (\n",
    "                               iter + 1 + epoch * self.parser.get_train_size()))) + \\\n",
    "                   int(self.batch_time_e.avg * self.parser.get_valid_size() * (\n",
    "                           self.ARCH['train']['max_epochs'] - (epoch)))\n",
    "        return str(datetime.timedelta(seconds=estimate))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_mpl_colormap(cmap_name):\n",
    "        cmap = plt.get_cmap(cmap_name)\n",
    "        # Initialize the matplotlib color map\n",
    "        sm = plt.cm.ScalarMappable(cmap=cmap)\n",
    "        # Obtain linear color range\n",
    "        color_range = sm.to_rgba(np.linspace(0, 1, 256), bytes=True)[:, 2::-1]\n",
    "        return color_range.reshape(256, 1, 3)\n",
    "\n",
    "    @staticmethod\n",
    "    def make_log_img(depth, mask, pred, gt, color_fn):\n",
    "        # input should be [depth, pred, gt]\n",
    "        # make range image (normalized to 0,1 for saving)\n",
    "        depth = (cv2.normalize(depth, None, alpha=0, beta=1,\n",
    "                               norm_type=cv2.NORM_MINMAX,\n",
    "                               dtype=cv2.CV_32F) * 255.0).astype(np.uint8)\n",
    "        out_img = cv2.applyColorMap(\n",
    "            depth, Trainer.get_mpl_colormap('viridis')) * mask[..., None]\n",
    "        # make label prediction\n",
    "        pred_color = color_fn((pred * mask).astype(np.int32))\n",
    "        out_img = np.concatenate([out_img, pred_color], axis=0)\n",
    "        # make label gt\n",
    "        gt_color = color_fn(gt)\n",
    "        out_img = np.concatenate([out_img, gt_color], axis=0)\n",
    "        return (out_img).astype(np.uint8)\n",
    "\n",
    "    @staticmethod\n",
    "    def save_to_log(logdir, logger, info, epoch, w_summary=False, model=None, img_summary=False, imgs=[]):\n",
    "        # save scalars\n",
    "        for tag, value in info.items():\n",
    "            logger.scalar_summary(tag, value, epoch)\n",
    "\n",
    "        # save summaries of weights and biases\n",
    "        if w_summary and model:\n",
    "            for tag, value in model.named_parameters():\n",
    "                tag = tag.replace('.', '/')\n",
    "                logger.histo_summary(tag, value.data.cpu().numpy(), epoch)\n",
    "                if value.grad is not None:\n",
    "                    logger.histo_summary(\n",
    "                        tag + '/grad', value.grad.data.cpu().numpy(), epoch)\n",
    "\n",
    "        if img_summary and len(imgs) > 0:\n",
    "            directory = os.path.join(logdir, \"predictions\")\n",
    "            if not os.path.isdir(directory):\n",
    "                os.makedirs(directory)\n",
    "            for i, img in enumerate(imgs):\n",
    "                name = os.path.join(directory, str(i) + \".png\")\n",
    "                cv2.imwrite(name, img)\n",
    "\n",
    "    def train_epoch(self, train_loader, model, criterion, optimizer, epoch,\n",
    "                    evaluator, scheduler, color_fn, report=10, show_scans=False):\n",
    "        losses = AverageMeter()\n",
    "        acc = AverageMeter()\n",
    "        iou = AverageMeter()\n",
    "        update_ratio_meter = AverageMeter()\n",
    "\n",
    "        # empty the cache to train now\n",
    "        if self.gpu:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # switch to train mode\n",
    "        model.train()\n",
    "\n",
    "        end = time.time()\n",
    "        for i, (in_vol, proj_mask, proj_labels, _, path_seq, path_name, _, _, _, _, _, _, _, _, _) in enumerate(train_loader):\n",
    "            # measure data loading time\n",
    "            self.data_time_t.update(time.time() - end)\n",
    "            if not self.multi_gpu and self.gpu:\n",
    "                in_vol = in_vol.cuda()\n",
    "                # proj_mask = proj_mask.cuda()\n",
    "            if self.gpu:\n",
    "                proj_labels = proj_labels.cuda().long()\n",
    "\n",
    "            # compute output\n",
    "            output = model(in_vol)\n",
    "            loss = criterion(torch.log(output.clamp(min=1e-8)), proj_labels) + self.ls(output, proj_labels.long())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            if self.n_gpus > 1:\n",
    "                idx = torch.ones(self.n_gpus).cuda()\n",
    "                loss.backward(idx)\n",
    "            else:\n",
    "                loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            loss = loss.mean()\n",
    "            with torch.no_grad():\n",
    "                evaluator.reset()\n",
    "                argmax = output.argmax(dim=1)\n",
    "                evaluator.addBatch(argmax, proj_labels)\n",
    "                accuracy = evaluator.getacc()\n",
    "                jaccard, class_jaccard = evaluator.getIoU()\n",
    "\n",
    "            losses.update(loss.item(), in_vol.size(0))\n",
    "            acc.update(accuracy.item(), in_vol.size(0))\n",
    "            iou.update(jaccard.item(), in_vol.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            self.batch_time_t.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            # get gradient updates and weights, so I can print the relationship of\n",
    "            # their norms\n",
    "            update_ratios = []\n",
    "            for g in self.optimizer.param_groups:\n",
    "                lr = g[\"lr\"]\n",
    "                for value in g[\"params\"]:\n",
    "                    if value.grad is not None:\n",
    "                        w = np.linalg.norm(value.data.cpu().numpy().reshape((-1)))\n",
    "                        update = np.linalg.norm(-max(lr, 1e-10) *\n",
    "                                                value.grad.cpu().numpy().reshape((-1)))\n",
    "                        update_ratios.append(update / max(w, 1e-10))\n",
    "            update_ratios = np.array(update_ratios)\n",
    "            update_mean = update_ratios.mean()\n",
    "            update_std = update_ratios.std()\n",
    "            update_ratio_meter.update(update_mean)  # over the epoch\n",
    "\n",
    "            if show_scans:\n",
    "                # get the first scan in batch and project points\n",
    "                mask_np = proj_mask[0].cpu().numpy()\n",
    "                depth_np = in_vol[0][0].cpu().numpy()\n",
    "                pred_np = argmax[0].cpu().numpy()\n",
    "                gt_np = proj_labels[0].cpu().numpy()\n",
    "                out = Trainer.make_log_img(depth_np, mask_np, pred_np, gt_np, color_fn)\n",
    "\n",
    "                mask_np = proj_mask[1].cpu().numpy()\n",
    "                depth_np = in_vol[1][0].cpu().numpy()\n",
    "                pred_np = argmax[1].cpu().numpy()\n",
    "                gt_np = proj_labels[1].cpu().numpy()\n",
    "                out2 = Trainer.make_log_img(depth_np, mask_np, pred_np, gt_np, color_fn)\n",
    "\n",
    "                out = np.concatenate([out, out2], axis=0)\n",
    "                cv2.imshow(\"sample_training\", out)\n",
    "                cv2.waitKey(1)\n",
    "\n",
    "            if i % self.ARCH[\"train\"][\"report_batch\"] == 0:\n",
    "                print('Lr: {lr:.3e} | '\n",
    "                      'Update: {umean:.3e} mean,{ustd:.3e} std | '\n",
    "                      'Epoch: [{0}][{1}/{2}] | '\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f}) | '\n",
    "                      'Data {data_time.val:.3f} ({data_time.avg:.3f}) | '\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f}) | '\n",
    "                      'acc {acc.val:.3f} ({acc.avg:.3f}) | '\n",
    "                      'IoU {iou.val:.3f} ({iou.avg:.3f}) | [{estim}]'.format(\n",
    "                    epoch, i, len(train_loader), batch_time=self.batch_time_t,\n",
    "                    data_time=self.data_time_t, loss=losses, acc=acc, iou=iou, lr=lr,\n",
    "                    umean=update_mean, ustd=update_std, estim=self.calculate_estimate(epoch, i)))\n",
    "\n",
    "                save_to_log(self.log, 'log.txt', 'Lr: {lr:.3e} | '\n",
    "                                      'Update: {umean:.3e} mean,{ustd:.3e} std | '\n",
    "                                      'Epoch: [{0}][{1}/{2}] | '\n",
    "                                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f}) | '\n",
    "                                      'Data {data_time.val:.3f} ({data_time.avg:.3f}) | '\n",
    "                                      'Loss {loss.val:.4f} ({loss.avg:.4f}) | '\n",
    "                                      'acc {acc.val:.3f} ({acc.avg:.3f}) | '\n",
    "                                      'IoU {iou.val:.3f} ({iou.avg:.3f}) | [{estim}]'.format(\n",
    "                                    epoch, i, len(train_loader), batch_time=self.batch_time_t,\n",
    "                                    data_time=self.data_time_t, loss=losses, acc=acc, iou=iou, lr=lr,\n",
    "                                    umean=update_mean, ustd=update_std, estim=self.calculate_estimate(epoch, i)))\n",
    "\n",
    "            # step scheduler\n",
    "            scheduler.step()\n",
    "\n",
    "        return acc.avg, iou.avg, losses.avg, update_ratio_meter.avg\n",
    "\n",
    "    def validate(self, val_loader, model, criterion, evaluator, class_func, color_fn, save_scans):\n",
    "        losses = AverageMeter()\n",
    "        jaccs = AverageMeter()\n",
    "        wces = AverageMeter()\n",
    "        acc = AverageMeter()\n",
    "        iou = AverageMeter()\n",
    "        rand_imgs = []\n",
    "\n",
    "        # switch to evaluate mode\n",
    "        model.eval()\n",
    "        evaluator.reset()\n",
    "\n",
    "        # empty the cache to infer in high res\n",
    "        if self.gpu:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            end = time.time()\n",
    "            for i, (in_vol, proj_mask, proj_labels, _, path_seq, path_name, _, _, _, _, _, _, _, _, _) in enumerate(val_loader):\n",
    "                if not self.multi_gpu and self.gpu:\n",
    "                    in_vol = in_vol.cuda()\n",
    "                    proj_mask = proj_mask.cuda()\n",
    "                if self.gpu:\n",
    "                    proj_labels = proj_labels.cuda(non_blocking=True).long()\n",
    "\n",
    "                # compute output\n",
    "                output = model(in_vol)\n",
    "                log_out = torch.log(output.clamp(min=1e-8))\n",
    "                jacc = self.ls(output, proj_labels)\n",
    "                wce = criterion(log_out, proj_labels)\n",
    "                loss = wce + jacc\n",
    "\n",
    "                # measure accuracy and record loss\n",
    "                argmax = output.argmax(dim=1)\n",
    "                evaluator.addBatch(argmax, proj_labels)\n",
    "                losses.update(loss.mean().item(), in_vol.size(0))\n",
    "                jaccs.update(jacc.mean().item(),in_vol.size(0))\n",
    "                wces.update(wce.mean().item(),in_vol.size(0))\n",
    "\n",
    "                if save_scans:\n",
    "                    # get the first scan in batch and project points\n",
    "                    mask_np = proj_mask[0].cpu().numpy()\n",
    "                    depth_np = in_vol[0][0].cpu().numpy()\n",
    "                    pred_np = argmax[0].cpu().numpy()\n",
    "                    gt_np = proj_labels[0].cpu().numpy()\n",
    "                    out = Trainer.make_log_img(depth_np,\n",
    "                                               mask_np,\n",
    "                                               pred_np,\n",
    "                                               gt_np,\n",
    "                                               color_fn)\n",
    "                    rand_imgs.append(out)\n",
    "\n",
    "                # measure elapsed time\n",
    "                self.batch_time_e.update(time.time() - end)\n",
    "                end = time.time()\n",
    "\n",
    "            accuracy = evaluator.getacc()\n",
    "            jaccard, class_jaccard = evaluator.getIoU()\n",
    "            acc.update(accuracy.item(), in_vol.size(0))\n",
    "            iou.update(jaccard.item(), in_vol.size(0))\n",
    "\n",
    "            print('Validation set:\\n'\n",
    "                  'Time avg per batch {batch_time.avg:.3f}\\n'\n",
    "                  'Loss avg {loss.avg:.4f}\\n'\n",
    "                  'Jaccard avg {jac.avg:.4f}\\n'\n",
    "                  'WCE avg {wces.avg:.4f}\\n'\n",
    "                  'Acc avg {acc.avg:.3f}\\n'\n",
    "                  'IoU avg {iou.avg:.3f}'.format(batch_time=self.batch_time_e,\n",
    "                                                 loss=losses,\n",
    "                                                 jac=jaccs,\n",
    "                                                 wces=wces,\n",
    "                                                 acc=acc,\n",
    "                                                 iou=iou))\n",
    "\n",
    "            save_to_log(self.log, 'log.txt', 'Validation set:\\n'\n",
    "                                             'Time avg per batch {batch_time.avg:.3f}\\n'\n",
    "                                             'Loss avg {loss.avg:.4f}\\n'\n",
    "                                             'Jaccard avg {jac.avg:.4f}\\n'\n",
    "                                             'WCE avg {wces.avg:.4f}\\n'\n",
    "                                             'Acc avg {acc.avg:.3f}\\n'\n",
    "                                             'IoU avg {iou.avg:.3f}'.format(batch_time=self.batch_time_e,\n",
    "                                                                            loss=losses,\n",
    "                                                                            jac=jaccs,\n",
    "                                                                            wces=wces,\n",
    "                                                                            acc=acc,\n",
    "                                                                            iou=iou))\n",
    "            # print also classwise\n",
    "            for i, jacc in enumerate(class_jaccard):\n",
    "                print('IoU class {i:} [{class_str:}] = {jacc:.3f}'.format(i=i, class_str=class_func(i), jacc=jacc))\n",
    "                save_to_log(self.log, 'log.txt', 'IoU class {i:} [{class_str:}] = {jacc:.3f}'.format(i=i, class_str=class_func(i), jacc=jacc))\n",
    "\n",
    "        return acc.avg, iou.avg, losses.avg, rand_imgs\n",
    "\n",
    "    def train(self):\n",
    "        self.ignore_class = []\n",
    "        for i, w in enumerate(self.loss_w):\n",
    "            if w < 1e-10:\n",
    "                self.ignore_class.append(i)\n",
    "                print(\"Ignoring class \", i, \" in IoU evaluation\")\n",
    "\n",
    "        self.evaluator = iouEval(self.parser.get_n_classes(),\n",
    "                                 self.device, self.ignore_class)\n",
    "\n",
    "        # train for n epochs\n",
    "        for epoch in range(self.epoch, self.ARCH[\"train\"][\"max_epochs\"]):\n",
    "            # get info for learn rate currently\n",
    "            # groups = self.optimizer.param_groups()\n",
    "            # for name, g in zip(self.lr_group_names, groups):\n",
    "            #     self.info[name] = g['lr']\n",
    "\n",
    "            # train for 1 epoch\n",
    "            acc, iou, loss, update_mean = self.train_epoch(train_loader=self.parser.get_train_set(),\n",
    "                                                           model=self.model,\n",
    "                                                           criterion=self.criterion,\n",
    "                                                           optimizer=self.optimizer,\n",
    "                                                           epoch=epoch,\n",
    "                                                           evaluator=self.evaluator,\n",
    "                                                           scheduler=self.scheduler,\n",
    "                                                           color_fn=self.parser.to_color,\n",
    "                                                           report=self.ARCH[\"train\"][\"report_batch\"],\n",
    "                                                           show_scans=self.ARCH[\"train\"][\"show_scans\"])\n",
    "\n",
    "            # update info\n",
    "            self.info[\"train_update\"] = update_mean\n",
    "            self.info[\"train_loss\"] = loss\n",
    "            self.info[\"train_acc\"] = acc\n",
    "            self.info[\"train_iou\"] = iou\n",
    "\n",
    "            # remember best iou and save checkpoint\n",
    "            state = {'epoch': epoch, 'state_dict': self.model.state_dict(),\n",
    "                     'optimizer': self.optimizer.state_dict(),\n",
    "                     'info': self.info,\n",
    "                     'scheduler': self.scheduler.state_dict()\n",
    "                     }\n",
    "            save_checkpoint(state, self.log, suffix=\"\")\n",
    "\n",
    "            if self.info['train_iou'] > self.info['best_train_iou']:\n",
    "                print(\"Best mean iou in training set so far, save model!\")\n",
    "                self.info['best_train_iou'] = self.info['train_iou']\n",
    "                state = {'epoch': epoch, 'state_dict': self.model.state_dict(),\n",
    "                         'optimizer': self.optimizer.state_dict(),\n",
    "                         'info': self.info,\n",
    "                         'scheduler': self.scheduler.state_dict()\n",
    "                         }\n",
    "                save_checkpoint(state, self.log, suffix=\"_train_best\")\n",
    "\n",
    "            if epoch % self.ARCH[\"train\"][\"report_epoch\"] == 0:\n",
    "                # evaluate on validation set\n",
    "                print(\"*\" * 80)\n",
    "                acc, iou, loss, rand_img = self.validate(val_loader=self.parser.get_valid_set(),\n",
    "                                                         model=self.model,\n",
    "                                                         criterion=self.criterion,\n",
    "                                                         evaluator=self.evaluator,\n",
    "                                                         class_func=self.parser.get_xentropy_class_string,\n",
    "                                                         color_fn=self.parser.to_color,\n",
    "                                                         save_scans=self.ARCH[\"train\"][\"save_scans\"])\n",
    "\n",
    "                # update info\n",
    "                self.info[\"valid_loss\"] = loss\n",
    "                self.info[\"valid_acc\"] = acc\n",
    "                self.info[\"valid_iou\"] = iou\n",
    "\n",
    "            # remember best iou and save checkpoint\n",
    "            if self.info['valid_iou'] > self.info['best_val_iou']:\n",
    "                print(\"Best mean iou in validation so far, save model!\")\n",
    "                print(\"*\" * 80)\n",
    "                self.info['best_val_iou'] = self.info['valid_iou']\n",
    "\n",
    "                # save the weights!\n",
    "                state = {'epoch': epoch, 'state_dict': self.model.state_dict(),\n",
    "                         'optimizer': self.optimizer.state_dict(),\n",
    "                         'info': self.info,\n",
    "                         'scheduler': self.scheduler.state_dict()\n",
    "                         }\n",
    "                save_checkpoint(state, self.log, suffix=\"_valid_best\")\n",
    "\n",
    "            print(\"*\" * 80)\n",
    "\n",
    "            # save to log\n",
    "            Trainer.save_to_log(logdir=self.log,\n",
    "                                logger=self.tb_logger,\n",
    "                                info=self.info,\n",
    "                                epoch=epoch,\n",
    "                                w_summary=self.ARCH[\"train\"][\"save_summary\"],\n",
    "                                model=self.model_single,\n",
    "                                img_summary=self.ARCH[\"train\"][\"save_scans\"],\n",
    "                                imgs=rand_img)\n",
    "\n",
    "        print('Finished Training')\n",
    "\n",
    "        return\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cfg = {\n",
    " 'color_map': {0: [0, 0, 0],\n",
    "               1: [0, 0, 255],\n",
    "               10: [245, 150, 100],\n",
    "               11: [245, 230, 100],\n",
    "               13: [250, 80, 100],\n",
    "               15: [150, 60, 30],\n",
    "               16: [255, 0, 0],\n",
    "               18: [180, 30, 80],\n",
    "               20: [255, 0, 0],\n",
    "               30: [30, 30, 255],\n",
    "               31: [200, 40, 255],\n",
    "               32: [90, 30, 150],\n",
    "               40: [255, 0, 255],\n",
    "               44: [255, 150, 255],\n",
    "               48: [75, 0, 75],\n",
    "               49: [75, 0, 175],\n",
    "               50: [0, 200, 255],\n",
    "               51: [50, 120, 255],\n",
    "               52: [0, 150, 255],\n",
    "               60: [170, 255, 150],\n",
    "               70: [0, 175, 0],\n",
    "               71: [0, 60, 135],\n",
    "               72: [80, 240, 150],\n",
    "               80: [150, 240, 255],\n",
    "               81: [0, 0, 255],\n",
    "               99: [255, 255, 50],\n",
    "               252: [245, 150, 100],\n",
    "               253: [200, 40, 255],\n",
    "               254: [30, 30, 255],\n",
    "               255: [90, 30, 150],\n",
    "               256: [255, 0, 0],\n",
    "               257: [250, 80, 100],\n",
    "               258: [180, 30, 80],\n",
    "               259: [255, 0, 0]},\n",
    " 'content': {0: 0.018889854628292943,\n",
    "             1: 0.0002937197336781505,\n",
    "             10: 0.040818519255974316,\n",
    "             11: 0.00016609538710764618,\n",
    "             13: 2.7879693665067774e-05,\n",
    "             15: 0.00039838616015114444,\n",
    "             16: 0.0,\n",
    "             18: 0.0020633612104619787,\n",
    "             20: 0.0016218197275284021,\n",
    "             30: 0.00017698551338515307,\n",
    "             31: 1.1065903904919655e-08,\n",
    "             32: 5.532951952459828e-09,\n",
    "             40: 0.1987493871255525,\n",
    "             44: 0.014717169549888214,\n",
    "             48: 0.14392298360372,\n",
    "             49: 0.0039048553037472045,\n",
    "             50: 0.1326861944777486,\n",
    "             51: 0.0723592229456223,\n",
    "             52: 0.002395131480328884,\n",
    "             60: 4.7084144280367186e-05,\n",
    "             70: 0.26681502148037506,\n",
    "             71: 0.006035012012626033,\n",
    "             72: 0.07814222006271769,\n",
    "             80: 0.002855498193863172,\n",
    "             81: 0.0006155958086189918,\n",
    "             99: 0.009923127583046915,\n",
    "             252: 0.001789309418528068,\n",
    "             253: 0.00012709999297008662,\n",
    "             254: 0.00016059776092534436,\n",
    "             255: 3.745553104802113e-05,\n",
    "             256: 0.0,\n",
    "             257: 0.00011351574470342043,\n",
    "             258: 0.00010157861367183268,\n",
    "             259: 4.3840131989471124e-05},\n",
    " 'labels': {0: 'unlabeled',\n",
    "            1: 'outlier',\n",
    "            10: 'car',\n",
    "            11: 'bicycle',\n",
    "            13: 'bus',\n",
    "            15: 'motorcycle',\n",
    "            16: 'on-rails',\n",
    "            18: 'truck',\n",
    "            20: 'other-vehicle',\n",
    "            30: 'person',\n",
    "            31: 'bicyclist',\n",
    "            32: 'motorcyclist',\n",
    "            40: 'road',\n",
    "            44: 'parking',\n",
    "            48: 'sidewalk',\n",
    "            49: 'other-ground',\n",
    "            50: 'building',\n",
    "            51: 'fence',\n",
    "            52: 'other-structure',\n",
    "            60: 'lane-marking',\n",
    "            70: 'vegetation',\n",
    "            71: 'trunk',\n",
    "            72: 'terrain',\n",
    "            80: 'pole',\n",
    "            81: 'traffic-sign',\n",
    "            99: 'other-object',\n",
    "            252: 'moving-car',\n",
    "            253: 'moving-bicyclist',\n",
    "            254: 'moving-person',\n",
    "            255: 'moving-motorcyclist',\n",
    "            256: 'moving-on-rails',\n",
    "            257: 'moving-bus',\n",
    "            258: 'moving-truck',\n",
    "            259: 'moving-other-vehicle'},\n",
    " 'learning_ignore': {0: True,\n",
    "                     1: False,\n",
    "                     2: False,\n",
    "                     3: False,\n",
    "                     4: False,\n",
    "                     5: False,\n",
    "                     6: False,\n",
    "                     7: False,\n",
    "                     8: False,\n",
    "                     9: False,\n",
    "                     10: False,\n",
    "                     11: False,\n",
    "                     12: False,\n",
    "                     13: False,\n",
    "                     14: False,\n",
    "                     15: False,\n",
    "                     16: False,\n",
    "                     17: False,\n",
    "                     18: False,\n",
    "                     19: False},\n",
    " 'learning_map': {0: 0,\n",
    "                  1: 0,\n",
    "                  10: 1,\n",
    "                  11: 2,\n",
    "                  13: 5,\n",
    "                  15: 3,\n",
    "                  16: 5,\n",
    "                  18: 4,\n",
    "                  20: 5,\n",
    "                  30: 6,\n",
    "                  31: 7,\n",
    "                  32: 8,\n",
    "                  40: 9,\n",
    "                  44: 10,\n",
    "                  48: 11,\n",
    "                  49: 12,\n",
    "                  50: 13,\n",
    "                  51: 14,\n",
    "                  52: 0,\n",
    "                  60: 9,\n",
    "                  70: 15,\n",
    "                  71: 16,\n",
    "                  72: 17,\n",
    "                  80: 18,\n",
    "                  81: 19,\n",
    "                  99: 0,\n",
    "                  252: 1,\n",
    "                  253: 7,\n",
    "                  254: 6,\n",
    "                  255: 8,\n",
    "                  256: 5,\n",
    "                  257: 5,\n",
    "                  258: 4,\n",
    "                  259: 5},\n",
    " 'learning_map_inv': {0: 0,\n",
    "                      1: 10,\n",
    "                      2: 11,\n",
    "                      3: 15,\n",
    "                      4: 18,\n",
    "                      5: 20,\n",
    "                      6: 30,\n",
    "                      7: 31,\n",
    "                      8: 32,\n",
    "                      9: 40,\n",
    "                      10: 44,\n",
    "                      11: 48,\n",
    "                      12: 49,\n",
    "                      13: 50,\n",
    "                      14: 51,\n",
    "                      15: 70,\n",
    "                      16: 71,\n",
    "                      17: 72,\n",
    "                      18: 80,\n",
    "                      19: 81},\n",
    " 'name': 'kitti',\n",
    " 'split': {'test': [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21],\n",
    "           'train': [0, 1, 2, 3, 4, 5, 6, 7, 9, 10],\n",
    "           'valid': [8]}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### arch cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_cfg = {\n",
    " 'dataset': {'labels': 'kitti',\n",
    "             'max_points': 150000,\n",
    "             'scans': 'kitti',\n",
    "             \n",
    "             # KITTI\n",
    "             'sensor': {\n",
    "                 'fov_down': -25,\n",
    "                 'fov_up': 3,\n",
    "                 \n",
    "                 'img_means': [12.12, 10.88, 0.23, -1.04, 0.21], # range, x, y, z, signal\n",
    "                 'img_stds': [12.32, 11.47, 6.91, 0.86, 0.16], # range, x, y, z, signal\n",
    "                 \n",
    "                 'img_prop': {\n",
    "                     'height': 64,\n",
    "                     'width': 2048,\n",
    "                 },\n",
    "                 'name': 'HDL64',\n",
    "                 'type': 'spherical'\n",
    "             },\n",
    "             \n",
    "             # HUSKY\n",
    "#              'sensor': {\n",
    "#                  'fov_down': -30.67,\n",
    "#                  'fov_up': 10.67,\n",
    "                 \n",
    "#                  'img_means': [8.75550024, 0.07549276, -1.13823771, -0.13648431, 0.06386641], # range, x, y, z, signal\n",
    "#                  'img_stds': [10.08941738, 10.40510729, 8.21806914, 1.15425178, 0.07281147], # range, x, y, z, signal\n",
    "                 \n",
    "#                  'img_prop': {\n",
    "#                      'height': 32,\n",
    "#                      'width': 2048,\n",
    "# #                      TODO: scale?\n",
    "# #                      'width': 2169,\n",
    "#                  },\n",
    "#                  'name': 'HDL32',\n",
    "#                  'type': 'spherical'\n",
    "#              },\n",
    "             \n",
    " },\n",
    " 'post': {'CRF': {'params': False, 'train': True, 'use': False},\n",
    "          'KNN': {'params': {'cutoff': 1.0,\n",
    "                             'knn': 5,\n",
    "                             'search': 5,\n",
    "                             'sigma': 1.0},\n",
    "                  'use': True}},\n",
    " 'train': {'batch_size': 30,\n",
    "           'epsilon_w': 0.001,\n",
    "           'loss': 'xentropy',\n",
    "           'lr': 0.05,\n",
    "           'lr_decay': 0.99,\n",
    "           'max_epochs': 40,\n",
    "           'momentum': 0.9,\n",
    "           'report_batch': 10,\n",
    "           'report_epoch': 1,\n",
    "           'save_scans': True,\n",
    "           'save_summary': False,\n",
    "           'show_scans': False,\n",
    "           'w_decay': 0.0001,\n",
    "           'workers': 4,\n",
    "           'wup_epochs': 1}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir_path = '/datasets/KITTI_Odometry/dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'salsanext'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where to create \"logs\" folder to store model weights\n",
    "log_dir_path = '/home/crowbar/2-projects/SalsaNext/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences folder exists! Using sequences from /datasets/KITTI_Odometry/dataset/sequences\n",
      "parsing seq 00\n",
      "parsing seq 01\n",
      "parsing seq 02\n",
      "parsing seq 03\n",
      "parsing seq 04\n",
      "parsing seq 05\n",
      "parsing seq 06\n",
      "parsing seq 07\n",
      "parsing seq 09\n",
      "parsing seq 10\n",
      "Using 19130 scans from sequences [0, 1, 2, 3, 4, 5, 6, 7, 9, 10]\n",
      "Sequences folder exists! Using sequences from /datasets/KITTI_Odometry/dataset/sequences\n",
      "parsing seq 08\n",
      "Using 4071 scans from sequences [8]\n",
      "Loss weights from content:  tensor([  0.0000,  22.9317, 857.5627, 715.1100, 315.9618, 356.2452, 747.6170,\n",
      "        887.2239, 963.8915,   5.0051,  63.6247,   6.9002, 203.8796,   7.4802,\n",
      "         13.6315,   3.7339, 142.1462,  12.6355, 259.3699, 618.9667])\n",
      "downCntx.conv1.weight: 160\n",
      "downCntx.conv1.bias: 32\n",
      "downCntx.conv2.weight: 1,440\n",
      "downCntx.conv2.bias: 32\n",
      "downCntx.bn1.weight: 32\n",
      "downCntx.bn1.bias: 32\n",
      "downCntx.conv3.weight: 9,216\n",
      "downCntx.conv3.bias: 32\n",
      "downCntx.bn2.weight: 32\n",
      "downCntx.bn2.bias: 32\n",
      "resBlock1.conv1.weight: 1,024\n",
      "resBlock1.conv1.bias: 32\n",
      "resBlock1.conv2.weight: 9,216\n",
      "resBlock1.conv2.bias: 32\n",
      "resBlock1.bn1.weight: 32\n",
      "resBlock1.bn1.bias: 32\n",
      "resBlock1.conv3.weight: 9,216\n",
      "resBlock1.conv3.bias: 32\n",
      "resBlock1.bn2.weight: 32\n",
      "resBlock1.bn2.bias: 32\n",
      "resBlock2.conv1.weight: 2,048\n",
      "resBlock2.conv1.bias: 64\n",
      "resBlock2.conv2.weight: 18,432\n",
      "resBlock2.conv2.bias: 64\n",
      "resBlock2.bn1.weight: 64\n",
      "resBlock2.bn1.bias: 64\n",
      "resBlock2.conv3.weight: 36,864\n",
      "resBlock2.conv3.bias: 64\n",
      "resBlock2.bn2.weight: 64\n",
      "resBlock2.bn2.bias: 64\n",
      "resBlock3.conv1.weight: 8,192\n",
      "resBlock3.conv1.bias: 128\n",
      "resBlock3.conv2.weight: 73,728\n",
      "resBlock3.conv2.bias: 128\n",
      "resBlock3.bn1.weight: 128\n",
      "resBlock3.bn1.bias: 128\n",
      "resBlock3.conv3.weight: 147,456\n",
      "resBlock3.conv3.bias: 128\n",
      "resBlock3.bn2.weight: 128\n",
      "resBlock3.bn2.bias: 128\n",
      "resBlock4.conv1.weight: 32,768\n",
      "resBlock4.conv1.bias: 256\n",
      "resBlock4.conv2.weight: 294,912\n",
      "resBlock4.conv2.bias: 256\n",
      "resBlock4.bn1.weight: 256\n",
      "resBlock4.bn1.bias: 256\n",
      "resBlock4.conv3.weight: 589,824\n",
      "resBlock4.conv3.bias: 256\n",
      "resBlock4.bn2.weight: 256\n",
      "resBlock4.bn2.bias: 256\n",
      "resBlock5.conv1.weight: 131,072\n",
      "resBlock5.conv1.bias: 512\n",
      "resBlock5.conv2.weight: 1,179,648\n",
      "resBlock5.conv2.bias: 512\n",
      "resBlock5.bn1.weight: 512\n",
      "resBlock5.bn1.bias: 512\n",
      "resBlock5.conv3.weight: 2,359,296\n",
      "resBlock5.conv3.bias: 512\n",
      "resBlock5.bn2.weight: 512\n",
      "resBlock5.bn2.bias: 512\n",
      "resBlock6.conv1.weight: 262,144\n",
      "resBlock6.conv1.bias: 512\n",
      "resBlock6.conv2.weight: 2,359,296\n",
      "resBlock6.conv2.bias: 512\n",
      "resBlock6.bn1.weight: 512\n",
      "resBlock6.bn1.bias: 512\n",
      "resBlock6.conv3.weight: 2,359,296\n",
      "resBlock6.conv3.bias: 512\n",
      "resBlock6.bn2.weight: 512\n",
      "resBlock6.bn2.bias: 512\n",
      "upBlock1.trans.weight: 2,359,296\n",
      "upBlock1.trans.bias: 512\n",
      "upBlock1.trans_bn.weight: 512\n",
      "upBlock1.trans_bn.bias: 512\n",
      "upBlock1.conv1.weight: 2,359,296\n",
      "upBlock1.conv1.bias: 512\n",
      "upBlock1.bn1.weight: 512\n",
      "upBlock1.bn1.bias: 512\n",
      "upBlock1.conv2.weight: 2,359,296\n",
      "upBlock1.conv2.bias: 512\n",
      "upBlock1.bn2.weight: 512\n",
      "upBlock1.bn2.bias: 512\n",
      "upBlock1.conv3.weight: 2,359,296\n",
      "upBlock1.conv3.bias: 512\n",
      "upBlock1.bn3.weight: 512\n",
      "upBlock1.bn3.bias: 512\n",
      "upBlock2.trans.weight: 1,179,648\n",
      "upBlock2.trans.bias: 256\n",
      "upBlock2.trans_bn.weight: 256\n",
      "upBlock2.trans_bn.bias: 256\n",
      "upBlock2.conv1.weight: 589,824\n",
      "upBlock2.conv1.bias: 256\n",
      "upBlock2.bn1.weight: 256\n",
      "upBlock2.bn1.bias: 256\n",
      "upBlock2.conv2.weight: 589,824\n",
      "upBlock2.conv2.bias: 256\n",
      "upBlock2.bn2.weight: 256\n",
      "upBlock2.bn2.bias: 256\n",
      "upBlock2.conv3.weight: 589,824\n",
      "upBlock2.conv3.bias: 256\n",
      "upBlock2.bn3.weight: 256\n",
      "upBlock2.bn3.bias: 256\n",
      "upBlock3.trans.weight: 294,912\n",
      "upBlock3.trans.bias: 128\n",
      "upBlock3.trans_bn.weight: 128\n",
      "upBlock3.trans_bn.bias: 128\n",
      "upBlock3.conv1.weight: 147,456\n",
      "upBlock3.conv1.bias: 128\n",
      "upBlock3.bn1.weight: 128\n",
      "upBlock3.bn1.bias: 128\n",
      "upBlock3.conv2.weight: 147,456\n",
      "upBlock3.conv2.bias: 128\n",
      "upBlock3.bn2.weight: 128\n",
      "upBlock3.bn2.bias: 128\n",
      "upBlock3.conv3.weight: 147,456\n",
      "upBlock3.conv3.bias: 128\n",
      "upBlock3.bn3.weight: 128\n",
      "upBlock3.bn3.bias: 128\n",
      "upBlock4.trans.weight: 73,728\n",
      "upBlock4.trans.bias: 64\n",
      "upBlock4.trans_bn.weight: 64\n",
      "upBlock4.trans_bn.bias: 64\n",
      "upBlock4.conv1.weight: 36,864\n",
      "upBlock4.conv1.bias: 64\n",
      "upBlock4.bn1.weight: 64\n",
      "upBlock4.bn1.bias: 64\n",
      "upBlock4.conv2.weight: 36,864\n",
      "upBlock4.conv2.bias: 64\n",
      "upBlock4.bn2.weight: 64\n",
      "upBlock4.bn2.bias: 64\n",
      "upBlock4.conv3.weight: 36,864\n",
      "upBlock4.conv3.bias: 64\n",
      "upBlock4.bn3.weight: 64\n",
      "upBlock4.bn3.bias: 64\n",
      "upBlock5.trans.weight: 18,432\n",
      "upBlock5.trans.bias: 32\n",
      "upBlock5.trans_bn.weight: 32\n",
      "upBlock5.trans_bn.bias: 32\n",
      "upBlock5.conv1.weight: 9,216\n",
      "upBlock5.conv1.bias: 32\n",
      "upBlock5.bn1.weight: 32\n",
      "upBlock5.bn1.bias: 32\n",
      "upBlock5.conv2.weight: 9,216\n",
      "upBlock5.conv2.bias: 32\n",
      "upBlock5.bn2.weight: 32\n",
      "upBlock5.bn2.bias: 32\n",
      "upBlock5.conv3.weight: 9,216\n",
      "upBlock5.conv3.bias: 32\n",
      "upBlock5.bn3.weight: 32\n",
      "upBlock5.bn3.bias: 32\n",
      "logits.weight: 640\n",
      "logits.bias: 20\n",
      "Total of Trainable Parameters: 23,262,548\n",
      "Training in device:  cuda\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(arch_cfg, data_cfg, dataset_dir_path, log_dir_path, pretrained, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring class  0  in IoU evaluation\n",
      "[IOU EVAL] IGNORE:  tensor([0])\n",
      "[IOU EVAL] INCLUDE:  tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
      "        19])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Traceback (most recent call last):\n  File \"/home/crowbar/.conda/envs/salsanext/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 99, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/crowbar/.conda/envs/salsanext/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 99, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"<ipython-input-16-3ff460cc3076>\", line 138, in __getitem__\n    proj_time_start = get_sync_time()\n  File \"<ipython-input-16-3ff460cc3076>\", line 6, in get_sync_time\n    torch.cuda.synchronize()\n  File \"/home/crowbar/.conda/envs/salsanext/lib/python3.7/site-packages/torch/cuda/__init__.py\", line 363, in synchronize\n    _lazy_init()\n  File \"/home/crowbar/.conda/envs/salsanext/lib/python3.7/site-packages/torch/cuda/__init__.py\", line 161, in _lazy_init\n    \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-4a0a40f36913>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    438\u001b[0m                                                            \u001b[0mcolor_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_color\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m                                                            \u001b[0mreport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mARCH\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"report_batch\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                                                            show_scans=self.ARCH[\"train\"][\"show_scans\"])\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;31m# update info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-4a0a40f36913>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, train_loader, model, criterion, optimizer, epoch, evaluator, scheduler, color_fn, report, show_scans)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0min_vol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m             \u001b[0;31m# measure data loading time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_time_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/salsanext/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    580\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/salsanext/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"KeyError:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Traceback (most recent call last):\n  File \"/home/crowbar/.conda/envs/salsanext/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 99, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/crowbar/.conda/envs/salsanext/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 99, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"<ipython-input-16-3ff460cc3076>\", line 138, in __getitem__\n    proj_time_start = get_sync_time()\n  File \"<ipython-input-16-3ff460cc3076>\", line 6, in get_sync_time\n    torch.cuda.synchronize()\n  File \"/home/crowbar/.conda/envs/salsanext/lib/python3.7/site-packages/torch/cuda/__init__.py\", line 363, in synchronize\n    _lazy_init()\n  File \"/home/crowbar/.conda/envs/salsanext/lib/python3.7/site-packages/torch/cuda/__init__.py\", line 161, in _lazy_init\n    \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
